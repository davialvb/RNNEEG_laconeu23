{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c36c6fbc",
   "metadata": {},
   "source": [
    "# EEG classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84b63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a45a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e026afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"../dataset/\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(data_path):\n",
    "    for filename in filenames:\n",
    "        if \"train\" in filename:\n",
    "            train_path = os.path.join(dirname, filename)\n",
    "        if \"test\" in filename:\n",
    "            test_path = os.path.join(dirname, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536dd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(target):\n",
    "    if target == -55:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6292ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# remove unnamed columns\n",
    "train_df = train_df.iloc[: , 3:]\n",
    "test_df = test_df.iloc[: , 3:]\n",
    "\n",
    "# change target value\n",
    "train_df.target = train_df.target.apply(encode_target)\n",
    "test_df.target = test_df.target.apply(encode_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce118603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb0489f2430>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_df.iloc[2:3, :-1].to_numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13791eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classes\n",
    "np.unique(train_df.to_numpy()[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51002f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(train_df, test_df, batch_size = 100, val_split=0.3):\n",
    "\n",
    "    # transform dataframe to numpy array\n",
    "    train_data = train_df.to_numpy()\n",
    "    test_data = test_df.to_numpy()\n",
    "    \n",
    "    # create x and y\n",
    "    x_train = train_data[:, :-1]\n",
    "    x_test = test_data[:, :-1]\n",
    "    y_train = train_data[:, -1]\n",
    "    y_test = test_data[:, -1]\n",
    "\n",
    "    # create tensor dataset of x and y\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_train).float(),\n",
    "                                                   torch.from_numpy(y_train).long(),)\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x_test).float(),\n",
    "                                                  torch.from_numpy(y_test).long())\n",
    "    \n",
    "    # split dataset in train, val and test\n",
    "    train_len = train_data.shape[0]\n",
    "    val_len = int(train_len * val_split)\n",
    "    train_len -= val_len\n",
    "\n",
    "    # shuffle train and validade data\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a88f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c223f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loader(train_df, test_df, batch_size = batch_size, val_split=val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44e6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        \n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8efef9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 1000 # number of iterations\n",
    "num_epochs = 10 # number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6244d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 200   # input dimension\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 10     # number of hidden layers\n",
    "output_dim = 2   # output dimension\n",
    "\n",
    "# Initiate RNN model\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04b8b5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1  val_loss: 0.6711229085922241  val_accuracy: 56.875 %\n",
      "iteration: 2  val_loss: 0.7064179182052612  val_accuracy: 56.875 %\n",
      "iteration: 3  val_loss: 0.6707783341407776  val_accuracy: 56.875 %\n",
      "iteration: 4  val_loss: 0.6820658445358276  val_accuracy: 56.875 %\n",
      "iteration: 5  val_loss: 0.681868851184845  val_accuracy: 56.875 %\n",
      "iteration: 6  val_loss: 0.6816737651824951  val_accuracy: 56.875 %\n",
      "iteration: 7  val_loss: 0.6554350256919861  val_accuracy: 56.875 %\n",
      "iteration: 8  val_loss: 0.6808097958564758  val_accuracy: 56.875 %\n",
      "iteration: 9  val_loss: 0.6806248426437378  val_accuracy: 56.875 %\n",
      "iteration: 10  val_loss: 0.7091469168663025  val_accuracy: 56.875 %\n",
      "iteration: 11  val_loss: 0.6807329654693604  val_accuracy: 56.875 %\n",
      "iteration: 12  val_loss: 0.6805486679077148  val_accuracy: 56.875 %\n",
      "iteration: 13  val_loss: 0.6803661584854126  val_accuracy: 56.875 %\n",
      "iteration: 14  val_loss: 0.6948717832565308  val_accuracy: 56.875 %\n",
      "iteration: 15  val_loss: 0.6656261086463928  val_accuracy: 56.875 %\n",
      "iteration: 16  val_loss: 0.679827868938446  val_accuracy: 56.875 %\n",
      "iteration: 17  val_loss: 0.6488758325576782  val_accuracy: 56.875 %\n",
      "iteration: 18  val_loss: 0.695253849029541  val_accuracy: 56.875 %\n",
      "iteration: 19  val_loss: 0.7113867402076721  val_accuracy: 56.875 %\n",
      "iteration: 20  val_loss: 0.7109020948410034  val_accuracy: 56.875 %\n",
      "iteration: 21  val_loss: 0.6642727255821228  val_accuracy: 56.875 %\n",
      "iteration: 22  val_loss: 0.7110950946807861  val_accuracy: 56.875 %\n",
      "iteration: 23  val_loss: 0.6950770020484924  val_accuracy: 56.875 %\n",
      "iteration: 24  val_loss: 0.664137601852417  val_accuracy: 56.875 %\n",
      "iteration: 25  val_loss: 0.6951932907104492  val_accuracy: 56.875 %\n",
      "iteration: 26  val_loss: 0.6951732039451599  val_accuracy: 56.875 %\n",
      "iteration: 27  val_loss: 0.6793118715286255  val_accuracy: 56.875 %\n",
      "iteration: 28  val_loss: 0.6952118873596191  val_accuracy: 56.875 %\n",
      "iteration: 29  val_loss: 0.6791998147964478  val_accuracy: 56.875 %\n",
      "iteration: 30  val_loss: 0.6952504515647888  val_accuracy: 56.875 %\n",
      "iteration: 31  val_loss: 0.6629483103752136  val_accuracy: 56.875 %\n",
      "iteration: 32  val_loss: 0.662018895149231  val_accuracy: 56.875 %\n",
      "iteration: 33  val_loss: 0.6783060431480408  val_accuracy: 56.875 %\n",
      "iteration: 34  val_loss: 0.6607139110565186  val_accuracy: 56.875 %\n",
      "iteration: 35  val_loss: 0.6598049402236938  val_accuracy: 56.875 %\n",
      "iteration: 36  val_loss: 0.6958789825439453  val_accuracy: 56.875 %\n",
      "iteration: 37  val_loss: 0.714248538017273  val_accuracy: 56.875 %\n",
      "iteration: 38  val_loss: 0.6597398519515991  val_accuracy: 56.875 %\n",
      "iteration: 39  val_loss: 0.6403146386146545  val_accuracy: 56.875 %\n",
      "iteration: 40  val_loss: 0.6574264764785767  val_accuracy: 56.875 %\n",
      "iteration: 41  val_loss: 0.7161839604377747  val_accuracy: 56.875 %\n",
      "iteration: 42  val_loss: 0.6572322845458984  val_accuracy: 56.875 %\n",
      "iteration: 43  val_loss: 0.6963416337966919  val_accuracy: 56.875 %\n",
      "iteration: 44  val_loss: 0.6963103413581848  val_accuracy: 56.875 %\n",
      "iteration: 45  val_loss: 0.6764823794364929  val_accuracy: 56.875 %\n",
      "iteration: 46  val_loss: 0.6763384938240051  val_accuracy: 56.875 %\n",
      "iteration: 47  val_loss: 0.6559784412384033  val_accuracy: 56.875 %\n",
      "iteration: 48  val_loss: 0.696582555770874  val_accuracy: 56.875 %\n",
      "iteration: 49  val_loss: 0.6346500515937805  val_accuracy: 56.875 %\n",
      "iteration: 50  val_loss: 0.6539247632026672  val_accuracy: 56.875 %\n",
      "iteration: 51  val_loss: 0.6530870795249939  val_accuracy: 56.875 %\n",
      "iteration: 52  val_loss: 0.6971777081489563  val_accuracy: 56.875 %\n",
      "iteration: 53  val_loss: 0.6524363160133362  val_accuracy: 56.875 %\n",
      "iteration: 54  val_loss: 0.6744655966758728  val_accuracy: 56.875 %\n",
      "iteration: 55  val_loss: 0.6973912715911865  val_accuracy: 56.875 %\n",
      "iteration: 56  val_loss: 0.651477038860321  val_accuracy: 56.875 %\n",
      "iteration: 57  val_loss: 0.6740978360176086  val_accuracy: 56.875 %\n",
      "iteration: 58  val_loss: 0.6503483653068542  val_accuracy: 56.875 %\n",
      "iteration: 59  val_loss: 0.6736693978309631  val_accuracy: 56.875 %\n",
      "iteration: 60  val_loss: 0.7221866846084595  val_accuracy: 56.875 %\n",
      "iteration: 61  val_loss: 0.673815906047821  val_accuracy: 56.875 %\n",
      "iteration: 62  val_loss: 0.6736986041069031  val_accuracy: 56.875 %\n",
      "iteration: 63  val_loss: 0.7221214175224304  val_accuracy: 56.875 %\n",
      "iteration: 64  val_loss: 0.6976884007453918  val_accuracy: 56.875 %\n",
      "iteration: 65  val_loss: 0.6501905918121338  val_accuracy: 56.875 %\n",
      "iteration: 66  val_loss: 0.6978344917297363  val_accuracy: 56.875 %\n",
      "iteration: 67  val_loss: 0.6495771408081055  val_accuracy: 56.875 %\n",
      "iteration: 68  val_loss: 0.7225834727287292  val_accuracy: 56.875 %\n",
      "iteration: 69  val_loss: 0.7219892740249634  val_accuracy: 56.875 %\n",
      "iteration: 70  val_loss: 0.6976525187492371  val_accuracy: 56.875 %\n",
      "iteration: 71  val_loss: 0.7212409973144531  val_accuracy: 56.875 %\n",
      "iteration: 72  val_loss: 0.697450578212738  val_accuracy: 56.875 %\n",
      "iteration: 73  val_loss: 0.6974086761474609  val_accuracy: 56.875 %\n",
      "iteration: 74  val_loss: 0.6973671913146973  val_accuracy: 56.875 %\n",
      "iteration: 75  val_loss: 0.6744550466537476  val_accuracy: 56.875 %\n",
      "iteration: 76  val_loss: 0.7204631567001343  val_accuracy: 56.875 %\n",
      "iteration: 77  val_loss: 0.6745980978012085  val_accuracy: 56.875 %\n",
      "iteration: 78  val_loss: 0.6744735240936279  val_accuracy: 56.875 %\n",
      "iteration: 79  val_loss: 0.6513130068778992  val_accuracy: 56.875 %\n",
      "iteration: 80  val_loss: 0.6504978537559509  val_accuracy: 56.875 %\n",
      "iteration: 81  val_loss: 0.6496900320053101  val_accuracy: 56.875 %\n",
      "iteration: 82  val_loss: 0.6979541182518005  val_accuracy: 56.875 %\n",
      "iteration: 83  val_loss: 0.6490829586982727  val_accuracy: 56.875 %\n",
      "iteration: 84  val_loss: 0.7230089902877808  val_accuracy: 56.875 %\n",
      "iteration: 85  val_loss: 0.6734551787376404  val_accuracy: 56.875 %\n",
      "iteration: 86  val_loss: 0.5993465781211853  val_accuracy: 56.875 %\n",
      "iteration: 87  val_loss: 0.6726782917976379  val_accuracy: 56.875 %\n",
      "iteration: 88  val_loss: 0.6725717186927795  val_accuracy: 56.875 %\n",
      "iteration: 89  val_loss: 0.7508734464645386  val_accuracy: 56.875 %\n",
      "iteration: 90  val_loss: 0.6982961893081665  val_accuracy: 56.875 %\n",
      "iteration: 91  val_loss: 0.6477119326591492  val_accuracy: 56.875 %\n",
      "iteration: 92  val_loss: 0.6469290852546692  val_accuracy: 56.875 %\n",
      "iteration: 93  val_loss: 0.6724008917808533  val_accuracy: 56.875 %\n",
      "iteration: 94  val_loss: 0.6458699703216553  val_accuracy: 56.875 %\n",
      "iteration: 95  val_loss: 0.6989306807518005  val_accuracy: 56.875 %\n",
      "iteration: 96  val_loss: 0.6720937490463257  val_accuracy: 56.875 %\n",
      "iteration: 97  val_loss: 0.7259078025817871  val_accuracy: 56.875 %\n",
      "iteration: 98  val_loss: 0.7517961263656616  val_accuracy: 56.875 %\n",
      "iteration: 99  val_loss: 0.6469097137451172  val_accuracy: 56.875 %\n",
      "iteration: 100  val_loss: 0.6461339592933655  val_accuracy: 56.875 %\n",
      "iteration: 101  val_loss: 0.6988596320152283  val_accuracy: 56.875 %\n",
      "iteration: 102  val_loss: 0.7254195213317871  val_accuracy: 56.875 %\n",
      "iteration: 103  val_loss: 0.69861900806427  val_accuracy: 56.875 %\n",
      "iteration: 104  val_loss: 0.7506618499755859  val_accuracy: 56.875 %\n",
      "iteration: 105  val_loss: 0.7235655188560486  val_accuracy: 56.875 %\n",
      "iteration: 106  val_loss: 0.6483427286148071  val_accuracy: 56.875 %\n",
      "iteration: 107  val_loss: 0.6982858180999756  val_accuracy: 56.875 %\n",
      "iteration: 108  val_loss: 0.6729941964149475  val_accuracy: 56.875 %\n",
      "iteration: 109  val_loss: 0.6474595069885254  val_accuracy: 56.875 %\n",
      "iteration: 110  val_loss: 0.620762825012207  val_accuracy: 56.875 %\n",
      "iteration: 111  val_loss: 0.6187065839767456  val_accuracy: 56.875 %\n",
      "iteration: 112  val_loss: 0.6441749930381775  val_accuracy: 56.875 %\n",
      "iteration: 113  val_loss: 0.6714125275611877  val_accuracy: 56.875 %\n",
      "iteration: 114  val_loss: 0.6431582570075989  val_accuracy: 56.875 %\n",
      "iteration: 115  val_loss: 0.7283338904380798  val_accuracy: 56.875 %\n",
      "iteration: 116  val_loss: 0.6149240732192993  val_accuracy: 56.875 %\n",
      "iteration: 117  val_loss: 0.6418944597244263  val_accuracy: 56.875 %\n",
      "iteration: 118  val_loss: 0.6411634683609009  val_accuracy: 56.875 %\n",
      "iteration: 119  val_loss: 0.7302241325378418  val_accuracy: 56.875 %\n",
      "iteration: 120  val_loss: 0.6706086993217468  val_accuracy: 56.875 %\n",
      "iteration: 121  val_loss: 0.7001566290855408  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 122  val_loss: 0.6705990433692932  val_accuracy: 56.875 %\n",
      "iteration: 123  val_loss: 0.7298182845115662  val_accuracy: 56.875 %\n",
      "iteration: 124  val_loss: 0.7583452463150024  val_accuracy: 56.875 %\n",
      "iteration: 125  val_loss: 0.671167254447937  val_accuracy: 56.875 %\n",
      "iteration: 126  val_loss: 0.6996780037879944  val_accuracy: 56.875 %\n",
      "iteration: 127  val_loss: 0.6996140480041504  val_accuracy: 56.875 %\n",
      "iteration: 128  val_loss: 0.6995506286621094  val_accuracy: 56.875 %\n",
      "iteration: 129  val_loss: 0.6431229710578918  val_accuracy: 56.875 %\n",
      "iteration: 130  val_loss: 0.7283667922019958  val_accuracy: 56.875 %\n",
      "iteration: 131  val_loss: 0.6995007395744324  val_accuracy: 56.875 %\n",
      "iteration: 132  val_loss: 0.6713659763336182  val_accuracy: 56.875 %\n",
      "iteration: 133  val_loss: 0.7560016512870789  val_accuracy: 56.875 %\n",
      "iteration: 134  val_loss: 0.7266650795936584  val_accuracy: 56.875 %\n",
      "iteration: 135  val_loss: 0.6989860534667969  val_accuracy: 56.875 %\n",
      "iteration: 136  val_loss: 0.6989288926124573  val_accuracy: 56.875 %\n",
      "iteration: 137  val_loss: 0.7256494164466858  val_accuracy: 56.875 %\n",
      "iteration: 138  val_loss: 0.7513604164123535  val_accuracy: 56.875 %\n",
      "iteration: 139  val_loss: 0.6983776092529297  val_accuracy: 56.875 %\n",
      "iteration: 140  val_loss: 0.6473941206932068  val_accuracy: 56.875 %\n",
      "iteration: 141  val_loss: 0.7504409551620483  val_accuracy: 56.875 %\n",
      "iteration: 142  val_loss: 0.6730107665061951  val_accuracy: 56.875 %\n",
      "iteration: 143  val_loss: 0.6221058368682861  val_accuracy: 56.875 %\n",
      "iteration: 144  val_loss: 0.7772157192230225  val_accuracy: 56.875 %\n",
      "iteration: 145  val_loss: 0.6227647662162781  val_accuracy: 56.875 %\n",
      "iteration: 146  val_loss: 0.6466352939605713  val_accuracy: 56.875 %\n",
      "iteration: 147  val_loss: 0.7780218720436096  val_accuracy: 56.875 %\n",
      "iteration: 148  val_loss: 0.7236688733100891  val_accuracy: 56.875 %\n",
      "iteration: 149  val_loss: 0.6232792735099792  val_accuracy: 56.875 %\n",
      "iteration: 150  val_loss: 0.6469482779502869  val_accuracy: 56.875 %\n",
      "iteration: 151  val_loss: 0.6986435651779175  val_accuracy: 56.875 %\n",
      "iteration: 152  val_loss: 0.6985898017883301  val_accuracy: 56.875 %\n",
      "iteration: 153  val_loss: 0.6725580096244812  val_accuracy: 56.875 %\n",
      "iteration: 154  val_loss: 0.6462935209274292  val_accuracy: 56.875 %\n",
      "iteration: 155  val_loss: 0.6988167762756348  val_accuracy: 56.875 %\n",
      "iteration: 156  val_loss: 0.7252770662307739  val_accuracy: 56.875 %\n",
      "iteration: 157  val_loss: 0.6464228630065918  val_accuracy: 56.875 %\n",
      "iteration: 158  val_loss: 0.6456515192985535  val_accuracy: 56.875 %\n",
      "iteration: 159  val_loss: 0.6989900469779968  val_accuracy: 56.875 %\n",
      "iteration: 160  val_loss: 0.7796880006790161  val_accuracy: 56.875 %\n",
      "iteration: 161  val_loss: 0.672626793384552  val_accuracy: 56.875 %\n",
      "iteration: 162  val_loss: 0.6985629796981812  val_accuracy: 56.875 %\n",
      "iteration: 163  val_loss: 0.6466811299324036  val_accuracy: 56.875 %\n",
      "iteration: 164  val_loss: 0.6723107099533081  val_accuracy: 56.875 %\n",
      "iteration: 165  val_loss: 0.7253708243370056  val_accuracy: 56.875 %\n",
      "iteration: 166  val_loss: 0.6724618673324585  val_accuracy: 56.875 %\n",
      "iteration: 167  val_loss: 0.6197124719619751  val_accuracy: 56.875 %\n",
      "iteration: 168  val_loss: 0.726136326789856  val_accuracy: 56.875 %\n",
      "iteration: 169  val_loss: 0.6988293528556824  val_accuracy: 56.875 %\n",
      "iteration: 170  val_loss: 0.6722287535667419  val_accuracy: 56.875 %\n",
      "iteration: 171  val_loss: 0.6721264123916626  val_accuracy: 56.875 %\n",
      "iteration: 172  val_loss: 0.7258245944976807  val_accuracy: 56.875 %\n",
      "iteration: 173  val_loss: 0.7251965999603271  val_accuracy: 56.875 %\n",
      "iteration: 174  val_loss: 0.6985542178153992  val_accuracy: 56.875 %\n",
      "iteration: 175  val_loss: 0.698501467704773  val_accuracy: 56.875 %\n",
      "iteration: 176  val_loss: 0.6726827025413513  val_accuracy: 56.875 %\n",
      "iteration: 177  val_loss: 0.6985238194465637  val_accuracy: 56.875 %\n",
      "iteration: 178  val_loss: 0.5951898694038391  val_accuracy: 56.875 %\n",
      "iteration: 179  val_loss: 0.7527866363525391  val_accuracy: 56.875 %\n",
      "iteration: 180  val_loss: 0.6724423766136169  val_accuracy: 56.875 %\n",
      "iteration: 181  val_loss: 0.6459819674491882  val_accuracy: 56.875 %\n",
      "iteration: 182  val_loss: 0.725743293762207  val_accuracy: 56.875 %\n",
      "iteration: 183  val_loss: 0.672311007976532  val_accuracy: 56.875 %\n",
      "iteration: 184  val_loss: 0.7519513964653015  val_accuracy: 56.875 %\n",
      "iteration: 185  val_loss: 0.6209738850593567  val_accuracy: 56.875 %\n",
      "iteration: 186  val_loss: 0.6988102793693542  val_accuracy: 56.875 %\n",
      "iteration: 187  val_loss: 0.6457539200782776  val_accuracy: 56.875 %\n",
      "iteration: 188  val_loss: 0.6719754338264465  val_accuracy: 56.875 %\n",
      "iteration: 189  val_loss: 0.699038028717041  val_accuracy: 56.875 %\n",
      "iteration: 190  val_loss: 0.671951413154602  val_accuracy: 56.875 %\n",
      "iteration: 191  val_loss: 0.6990562081336975  val_accuracy: 56.875 %\n",
      "iteration: 192  val_loss: 0.6989983916282654  val_accuracy: 56.875 %\n",
      "iteration: 193  val_loss: 0.7258788347244263  val_accuracy: 56.875 %\n",
      "iteration: 194  val_loss: 0.6722564101219177  val_accuracy: 56.875 %\n",
      "iteration: 195  val_loss: 0.6721538305282593  val_accuracy: 56.875 %\n",
      "iteration: 196  val_loss: 0.672052264213562  val_accuracy: 56.875 %\n",
      "iteration: 197  val_loss: 0.6719516515731812  val_accuracy: 56.875 %\n",
      "iteration: 198  val_loss: 0.6718519330024719  val_accuracy: 56.875 %\n",
      "iteration: 199  val_loss: 0.6717531681060791  val_accuracy: 56.875 %\n",
      "iteration: 200  val_loss: 0.6441023349761963  val_accuracy: 56.875 %\n",
      "iteration: 201  val_loss: 0.6433519124984741  val_accuracy: 56.875 %\n",
      "iteration: 202  val_loss: 0.6426081657409668  val_accuracy: 56.875 %\n",
      "iteration: 203  val_loss: 0.6128780841827393  val_accuracy: 56.875 %\n",
      "iteration: 204  val_loss: 0.7300033569335938  val_accuracy: 56.875 %\n",
      "iteration: 205  val_loss: 0.787975549697876  val_accuracy: 56.875 %\n",
      "iteration: 206  val_loss: 0.6712687611579895  val_accuracy: 56.875 %\n",
      "iteration: 207  val_loss: 0.6711754202842712  val_accuracy: 56.875 %\n",
      "iteration: 208  val_loss: 0.6710831522941589  val_accuracy: 56.875 %\n",
      "iteration: 209  val_loss: 0.7285045385360718  val_accuracy: 56.875 %\n",
      "iteration: 210  val_loss: 0.6712375283241272  val_accuracy: 56.875 %\n",
      "iteration: 211  val_loss: 0.7565703988075256  val_accuracy: 56.875 %\n",
      "iteration: 212  val_loss: 0.6438466310501099  val_accuracy: 56.875 %\n",
      "iteration: 213  val_loss: 0.7276930212974548  val_accuracy: 56.875 %\n",
      "iteration: 214  val_loss: 0.6437945365905762  val_accuracy: 56.875 %\n",
      "iteration: 215  val_loss: 0.6995097994804382  val_accuracy: 56.875 %\n",
      "iteration: 216  val_loss: 0.7275400161743164  val_accuracy: 56.875 %\n",
      "iteration: 217  val_loss: 0.7545395493507385  val_accuracy: 56.875 %\n",
      "iteration: 218  val_loss: 0.6989215612411499  val_accuracy: 56.875 %\n",
      "iteration: 219  val_loss: 0.672105073928833  val_accuracy: 56.875 %\n",
      "iteration: 220  val_loss: 0.6989406943321228  val_accuracy: 56.875 %\n",
      "iteration: 221  val_loss: 0.6720796227455139  val_accuracy: 56.875 %\n",
      "iteration: 222  val_loss: 0.6719787120819092  val_accuracy: 56.875 %\n",
      "iteration: 223  val_loss: 0.7261922359466553  val_accuracy: 56.875 %\n",
      "iteration: 224  val_loss: 0.7522754669189453  val_accuracy: 56.875 %\n",
      "iteration: 225  val_loss: 0.6725645661354065  val_accuracy: 56.875 %\n",
      "iteration: 226  val_loss: 0.6724591851234436  val_accuracy: 56.875 %\n",
      "iteration: 227  val_loss: 0.6460273265838623  val_accuracy: 56.875 %\n",
      "iteration: 228  val_loss: 0.7257025241851807  val_accuracy: 56.875 %\n",
      "iteration: 229  val_loss: 0.6459531188011169  val_accuracy: 56.875 %\n",
      "iteration: 230  val_loss: 0.7526301741600037  val_accuracy: 56.875 %\n",
      "iteration: 231  val_loss: 0.6724796295166016  val_accuracy: 56.875 %\n",
      "iteration: 232  val_loss: 0.672374963760376  val_accuracy: 56.875 %\n",
      "iteration: 233  val_loss: 0.7252137064933777  val_accuracy: 56.875 %\n",
      "iteration: 234  val_loss: 0.6464927792549133  val_accuracy: 56.875 %\n",
      "iteration: 235  val_loss: 0.6987637877464294  val_accuracy: 56.875 %\n",
      "iteration: 236  val_loss: 0.6987087726593018  val_accuracy: 56.875 %\n",
      "iteration: 237  val_loss: 0.6986543536186218  val_accuracy: 56.875 %\n",
      "iteration: 238  val_loss: 0.6986005306243896  val_accuracy: 56.875 %\n",
      "iteration: 239  val_loss: 0.6725429892539978  val_accuracy: 56.875 %\n",
      "iteration: 240  val_loss: 0.7509911060333252  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 241  val_loss: 0.7237561941146851  val_accuracy: 56.875 %\n",
      "iteration: 242  val_loss: 0.7731658816337585  val_accuracy: 56.875 %\n",
      "iteration: 243  val_loss: 0.6498069167137146  val_accuracy: 56.875 %\n",
      "iteration: 244  val_loss: 0.6979259848594666  val_accuracy: 56.875 %\n",
      "iteration: 245  val_loss: 0.6491985321044922  val_accuracy: 56.875 %\n",
      "iteration: 246  val_loss: 0.6732380986213684  val_accuracy: 56.875 %\n",
      "iteration: 247  val_loss: 0.7732101678848267  val_accuracy: 56.875 %\n",
      "iteration: 248  val_loss: 0.7217158079147339  val_accuracy: 56.875 %\n",
      "iteration: 249  val_loss: 0.6975783705711365  val_accuracy: 56.875 %\n",
      "iteration: 250  val_loss: 0.6506606340408325  val_accuracy: 56.875 %\n",
      "iteration: 251  val_loss: 0.6977236270904541  val_accuracy: 56.875 %\n",
      "iteration: 252  val_loss: 0.6738601922988892  val_accuracy: 56.875 %\n",
      "iteration: 253  val_loss: 0.6737425327301025  val_accuracy: 56.875 %\n",
      "iteration: 254  val_loss: 0.649427592754364  val_accuracy: 56.875 %\n",
      "iteration: 255  val_loss: 0.7227115035057068  val_accuracy: 56.875 %\n",
      "iteration: 256  val_loss: 0.6493187546730042  val_accuracy: 56.875 %\n",
      "iteration: 257  val_loss: 0.7228052616119385  val_accuracy: 56.875 %\n",
      "iteration: 258  val_loss: 0.7222089767456055  val_accuracy: 56.875 %\n",
      "iteration: 259  val_loss: 0.7216188907623291  val_accuracy: 56.875 %\n",
      "iteration: 260  val_loss: 0.6740694046020508  val_accuracy: 56.875 %\n",
      "iteration: 261  val_loss: 0.6739497780799866  val_accuracy: 56.875 %\n",
      "iteration: 262  val_loss: 0.6499659419059753  val_accuracy: 56.875 %\n",
      "iteration: 263  val_loss: 0.6491632461547852  val_accuracy: 56.875 %\n",
      "iteration: 264  val_loss: 0.673224925994873  val_accuracy: 56.875 %\n",
      "iteration: 265  val_loss: 0.69815593957901  val_accuracy: 56.875 %\n",
      "iteration: 266  val_loss: 0.6731870770454407  val_accuracy: 56.875 %\n",
      "iteration: 267  val_loss: 0.6981809139251709  val_accuracy: 56.875 %\n",
      "iteration: 268  val_loss: 0.6731497049331665  val_accuracy: 56.875 %\n",
      "iteration: 269  val_loss: 0.67303866147995  val_accuracy: 56.875 %\n",
      "iteration: 270  val_loss: 0.6982799172401428  val_accuracy: 56.875 %\n",
      "iteration: 271  val_loss: 0.6982296705245972  val_accuracy: 56.875 %\n",
      "iteration: 272  val_loss: 0.6730770468711853  val_accuracy: 56.875 %\n",
      "iteration: 273  val_loss: 0.6982541084289551  val_accuracy: 56.875 %\n",
      "iteration: 274  val_loss: 0.7233677506446838  val_accuracy: 56.875 %\n",
      "iteration: 275  val_loss: 0.6485674381256104  val_accuracy: 56.875 %\n",
      "iteration: 276  val_loss: 0.6730033159255981  val_accuracy: 56.875 %\n",
      "iteration: 277  val_loss: 0.7491233348846436  val_accuracy: 56.875 %\n",
      "iteration: 278  val_loss: 0.6733379364013672  val_accuracy: 56.875 %\n",
      "iteration: 279  val_loss: 0.6235116720199585  val_accuracy: 56.875 %\n",
      "iteration: 280  val_loss: 0.7240611910820007  val_accuracy: 56.875 %\n",
      "iteration: 281  val_loss: 0.6982284188270569  val_accuracy: 56.875 %\n",
      "iteration: 282  val_loss: 0.6730788946151733  val_accuracy: 56.875 %\n",
      "iteration: 283  val_loss: 0.6982528567314148  val_accuracy: 56.875 %\n",
      "iteration: 284  val_loss: 0.6227219700813293  val_accuracy: 56.875 %\n",
      "iteration: 285  val_loss: 0.6466097235679626  val_accuracy: 56.875 %\n",
      "iteration: 286  val_loss: 0.6987327337265015  val_accuracy: 56.875 %\n",
      "iteration: 287  val_loss: 0.6460420489311218  val_accuracy: 56.875 %\n",
      "iteration: 288  val_loss: 0.7792988419532776  val_accuracy: 56.875 %\n",
      "iteration: 289  val_loss: 0.6469441652297974  val_accuracy: 56.875 %\n",
      "iteration: 290  val_loss: 0.6199303865432739  val_accuracy: 56.875 %\n",
      "iteration: 291  val_loss: 0.6989822387695312  val_accuracy: 56.875 %\n",
      "iteration: 292  val_loss: 0.6720245480537415  val_accuracy: 56.875 %\n",
      "iteration: 293  val_loss: 0.6719242334365845  val_accuracy: 56.875 %\n",
      "iteration: 294  val_loss: 0.6718248128890991  val_accuracy: 56.875 %\n",
      "iteration: 295  val_loss: 0.6168734431266785  val_accuracy: 56.875 %\n",
      "iteration: 296  val_loss: 0.6995035409927368  val_accuracy: 56.875 %\n",
      "iteration: 297  val_loss: 0.6432839632034302  val_accuracy: 56.875 %\n",
      "iteration: 298  val_loss: 0.6710994243621826  val_accuracy: 56.875 %\n",
      "iteration: 299  val_loss: 0.6422807574272156  val_accuracy: 56.875 %\n",
      "iteration: 300  val_loss: 0.6999548077583313  val_accuracy: 56.875 %\n",
      "iteration: 301  val_loss: 0.6998878717422485  val_accuracy: 56.875 %\n",
      "iteration: 302  val_loss: 0.6709048748016357  val_accuracy: 56.875 %\n",
      "iteration: 303  val_loss: 0.6708151698112488  val_accuracy: 56.875 %\n",
      "iteration: 304  val_loss: 0.6707261800765991  val_accuracy: 56.875 %\n",
      "iteration: 305  val_loss: 0.7000532746315002  val_accuracy: 56.875 %\n",
      "iteration: 306  val_loss: 0.7292555570602417  val_accuracy: 56.875 %\n",
      "iteration: 307  val_loss: 0.6709593534469604  val_accuracy: 56.875 %\n",
      "iteration: 308  val_loss: 0.6418853402137756  val_accuracy: 56.875 %\n",
      "iteration: 309  val_loss: 0.7000741958618164  val_accuracy: 56.875 %\n",
      "iteration: 310  val_loss: 0.670691728591919  val_accuracy: 56.875 %\n",
      "iteration: 311  val_loss: 0.6706039905548096  val_accuracy: 56.875 %\n",
      "iteration: 312  val_loss: 0.7298045754432678  val_accuracy: 56.875 %\n",
      "iteration: 313  val_loss: 0.6415714025497437  val_accuracy: 56.875 %\n",
      "iteration: 314  val_loss: 0.6705068945884705  val_accuracy: 56.875 %\n",
      "iteration: 315  val_loss: 0.7300739288330078  val_accuracy: 56.875 %\n",
      "iteration: 316  val_loss: 0.7000319957733154  val_accuracy: 56.875 %\n",
      "iteration: 317  val_loss: 0.641514778137207  val_accuracy: 56.875 %\n",
      "iteration: 318  val_loss: 0.6110877990722656  val_accuracy: 56.875 %\n",
      "iteration: 319  val_loss: 0.6395965218544006  val_accuracy: 56.875 %\n",
      "iteration: 320  val_loss: 0.7626984715461731  val_accuracy: 56.875 %\n",
      "iteration: 321  val_loss: 0.7909537553787231  val_accuracy: 56.875 %\n",
      "iteration: 322  val_loss: 0.6708061099052429  val_accuracy: 56.875 %\n",
      "iteration: 323  val_loss: 0.7585172057151794  val_accuracy: 56.875 %\n",
      "iteration: 324  val_loss: 0.642625629901886  val_accuracy: 56.875 %\n",
      "iteration: 325  val_loss: 0.6708700656890869  val_accuracy: 56.875 %\n",
      "iteration: 326  val_loss: 0.6999287605285645  val_accuracy: 56.875 %\n",
      "iteration: 327  val_loss: 0.7288665771484375  val_accuracy: 56.875 %\n",
      "iteration: 328  val_loss: 0.6425504684448242  val_accuracy: 56.875 %\n",
      "iteration: 329  val_loss: 0.7289039492607117  val_accuracy: 56.875 %\n",
      "iteration: 330  val_loss: 0.6710888743400574  val_accuracy: 56.875 %\n",
      "iteration: 331  val_loss: 0.7572353482246399  val_accuracy: 56.875 %\n",
      "iteration: 332  val_loss: 0.7273861169815063  val_accuracy: 56.875 %\n",
      "iteration: 333  val_loss: 0.6441230773925781  val_accuracy: 56.875 %\n",
      "iteration: 334  val_loss: 0.7274372577667236  val_accuracy: 56.875 %\n",
      "iteration: 335  val_loss: 0.7267927527427673  val_accuracy: 56.875 %\n",
      "iteration: 336  val_loss: 0.7261551022529602  val_accuracy: 56.875 %\n",
      "iteration: 337  val_loss: 0.6454565525054932  val_accuracy: 56.875 %\n",
      "iteration: 338  val_loss: 0.6175198554992676  val_accuracy: 56.875 %\n",
      "iteration: 339  val_loss: 0.7273574471473694  val_accuracy: 56.875 %\n",
      "iteration: 340  val_loss: 0.7267137765884399  val_accuracy: 56.875 %\n",
      "iteration: 341  val_loss: 0.6719245910644531  val_accuracy: 56.875 %\n",
      "iteration: 342  val_loss: 0.6718251705169678  val_accuracy: 56.875 %\n",
      "iteration: 343  val_loss: 0.7540045976638794  val_accuracy: 56.875 %\n",
      "iteration: 344  val_loss: 0.6988285183906555  val_accuracy: 56.875 %\n",
      "iteration: 345  val_loss: 0.6456863284111023  val_accuracy: 56.875 %\n",
      "iteration: 346  val_loss: 0.6719512343406677  val_accuracy: 56.875 %\n",
      "iteration: 347  val_loss: 0.6718516945838928  val_accuracy: 56.875 %\n",
      "iteration: 348  val_loss: 0.7265117168426514  val_accuracy: 56.875 %\n",
      "iteration: 349  val_loss: 0.698940634727478  val_accuracy: 56.875 %\n",
      "iteration: 350  val_loss: 0.7524919509887695  val_accuracy: 56.875 %\n",
      "iteration: 351  val_loss: 0.6725125312805176  val_accuracy: 56.875 %\n",
      "iteration: 352  val_loss: 0.6461713910102844  val_accuracy: 56.875 %\n",
      "iteration: 353  val_loss: 0.6721261143684387  val_accuracy: 56.875 %\n",
      "iteration: 354  val_loss: 0.6989250183105469  val_accuracy: 56.875 %\n",
      "iteration: 355  val_loss: 0.7524045705795288  val_accuracy: 56.875 %\n",
      "iteration: 356  val_loss: 0.6985539197921753  val_accuracy: 56.875 %\n",
      "iteration: 357  val_loss: 0.6726083159446716  val_accuracy: 56.875 %\n",
      "iteration: 358  val_loss: 0.6985760927200317  val_accuracy: 56.875 %\n",
      "iteration: 359  val_loss: 0.6985229253768921  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 360  val_loss: 0.7242885828018188  val_accuracy: 56.875 %\n",
      "iteration: 361  val_loss: 0.6982930898666382  val_accuracy: 56.875 %\n",
      "iteration: 362  val_loss: 0.6224642395973206  val_accuracy: 56.875 %\n",
      "iteration: 363  val_loss: 0.6725113391876221  val_accuracy: 56.875 %\n",
      "iteration: 364  val_loss: 0.7248827815055847  val_accuracy: 56.875 %\n",
      "iteration: 365  val_loss: 0.7242650985717773  val_accuracy: 56.875 %\n",
      "iteration: 366  val_loss: 0.6221842765808105  val_accuracy: 56.875 %\n",
      "iteration: 367  val_loss: 0.6986143589019775  val_accuracy: 56.875 %\n",
      "iteration: 368  val_loss: 0.7506353855133057  val_accuracy: 56.875 %\n",
      "iteration: 369  val_loss: 0.6729626655578613  val_accuracy: 56.875 %\n",
      "iteration: 370  val_loss: 0.621898353099823  val_accuracy: 56.875 %\n",
      "iteration: 371  val_loss: 0.6723847985267639  val_accuracy: 56.875 %\n",
      "iteration: 372  val_loss: 0.7251898050308228  val_accuracy: 56.875 %\n",
      "iteration: 373  val_loss: 0.7505854368209839  val_accuracy: 56.875 %\n",
      "iteration: 374  val_loss: 0.6729749441146851  val_accuracy: 56.875 %\n",
      "iteration: 375  val_loss: 0.672865629196167  val_accuracy: 56.875 %\n",
      "iteration: 376  val_loss: 0.698397159576416  val_accuracy: 56.875 %\n",
      "iteration: 377  val_loss: 0.7238600254058838  val_accuracy: 56.875 %\n",
      "iteration: 378  val_loss: 0.6981713771820068  val_accuracy: 56.875 %\n",
      "iteration: 379  val_loss: 0.6482051610946655  val_accuracy: 56.875 %\n",
      "iteration: 380  val_loss: 0.6728693246841431  val_accuracy: 56.875 %\n",
      "iteration: 381  val_loss: 0.6471273303031921  val_accuracy: 56.875 %\n",
      "iteration: 382  val_loss: 0.7247198820114136  val_accuracy: 56.875 %\n",
      "iteration: 383  val_loss: 0.6727293133735657  val_accuracy: 56.875 %\n",
      "iteration: 384  val_loss: 0.6467533707618713  val_accuracy: 56.875 %\n",
      "iteration: 385  val_loss: 0.69869464635849  val_accuracy: 56.875 %\n",
      "iteration: 386  val_loss: 0.6461842060089111  val_accuracy: 56.875 %\n",
      "iteration: 387  val_loss: 0.6988460421562195  val_accuracy: 56.875 %\n",
      "iteration: 388  val_loss: 0.6987901926040649  val_accuracy: 56.875 %\n",
      "iteration: 389  val_loss: 0.7251886129379272  val_accuracy: 56.875 %\n",
      "iteration: 390  val_loss: 0.6985519528388977  val_accuracy: 56.875 %\n",
      "iteration: 391  val_loss: 0.6984991431236267  val_accuracy: 56.875 %\n",
      "iteration: 392  val_loss: 0.6984469294548035  val_accuracy: 56.875 %\n",
      "iteration: 393  val_loss: 0.6727604269981384  val_accuracy: 56.875 %\n",
      "iteration: 394  val_loss: 0.6726530194282532  val_accuracy: 56.875 %\n",
      "iteration: 395  val_loss: 0.6465492248535156  val_accuracy: 56.875 %\n",
      "iteration: 396  val_loss: 0.72523432970047  val_accuracy: 56.875 %\n",
      "iteration: 397  val_loss: 0.6985652446746826  val_accuracy: 56.875 %\n",
      "iteration: 398  val_loss: 0.6466724276542664  val_accuracy: 56.875 %\n",
      "iteration: 399  val_loss: 0.6987161040306091  val_accuracy: 56.875 %\n",
      "iteration: 400  val_loss: 0.6461042165756226  val_accuracy: 56.875 %\n",
      "iteration: 401  val_loss: 0.6988673806190491  val_accuracy: 56.875 %\n",
      "iteration: 402  val_loss: 0.6988114714622498  val_accuracy: 56.875 %\n",
      "iteration: 403  val_loss: 0.6722527742385864  val_accuracy: 56.875 %\n",
      "iteration: 404  val_loss: 0.6988315582275391  val_accuracy: 56.875 %\n",
      "iteration: 405  val_loss: 0.6987759470939636  val_accuracy: 56.875 %\n",
      "iteration: 406  val_loss: 0.645881175994873  val_accuracy: 56.875 %\n",
      "iteration: 407  val_loss: 0.6989275813102722  val_accuracy: 56.875 %\n",
      "iteration: 408  val_loss: 0.7256450653076172  val_accuracy: 56.875 %\n",
      "iteration: 409  val_loss: 0.6196816563606262  val_accuracy: 56.875 %\n",
      "iteration: 410  val_loss: 0.5905067324638367  val_accuracy: 56.875 %\n",
      "iteration: 411  val_loss: 0.6712793111801147  val_accuracy: 56.875 %\n",
      "iteration: 412  val_loss: 0.6711859703063965  val_accuracy: 56.875 %\n",
      "iteration: 413  val_loss: 0.6139557361602783  val_accuracy: 56.875 %\n",
      "iteration: 414  val_loss: 0.6706697940826416  val_accuracy: 56.875 %\n",
      "iteration: 415  val_loss: 0.7001028060913086  val_accuracy: 56.875 %\n",
      "iteration: 416  val_loss: 0.7000345587730408  val_accuracy: 56.875 %\n",
      "iteration: 417  val_loss: 0.6415063738822937  val_accuracy: 56.875 %\n",
      "iteration: 418  val_loss: 0.6110742092132568  val_accuracy: 56.875 %\n",
      "iteration: 419  val_loss: 0.7310529947280884  val_accuracy: 56.875 %\n",
      "iteration: 420  val_loss: 0.6102606058120728  val_accuracy: 56.875 %\n",
      "iteration: 421  val_loss: 0.6391031742095947  val_accuracy: 56.875 %\n",
      "iteration: 422  val_loss: 0.6696754097938538  val_accuracy: 56.875 %\n",
      "iteration: 423  val_loss: 0.6067298650741577  val_accuracy: 56.875 %\n",
      "iteration: 424  val_loss: 0.7336349487304688  val_accuracy: 56.875 %\n",
      "iteration: 425  val_loss: 0.7011849880218506  val_accuracy: 56.875 %\n",
      "iteration: 426  val_loss: 0.6695229411125183  val_accuracy: 56.875 %\n",
      "iteration: 427  val_loss: 0.7329214811325073  val_accuracy: 56.875 %\n",
      "iteration: 428  val_loss: 0.6696794033050537  val_accuracy: 56.875 %\n",
      "iteration: 429  val_loss: 0.6067494750022888  val_accuracy: 56.875 %\n",
      "iteration: 430  val_loss: 0.7014204859733582  val_accuracy: 56.875 %\n",
      "iteration: 431  val_loss: 0.6372523903846741  val_accuracy: 56.875 %\n",
      "iteration: 432  val_loss: 0.6365638375282288  val_accuracy: 56.875 %\n",
      "iteration: 433  val_loss: 0.6688492894172668  val_accuracy: 56.875 %\n",
      "iteration: 434  val_loss: 0.6356630325317383  val_accuracy: 56.875 %\n",
      "iteration: 435  val_loss: 0.7021384835243225  val_accuracy: 56.875 %\n",
      "iteration: 436  val_loss: 0.7688670754432678  val_accuracy: 56.875 %\n",
      "iteration: 437  val_loss: 0.6690158843994141  val_accuracy: 56.875 %\n",
      "iteration: 438  val_loss: 0.6689439415931702  val_accuracy: 56.875 %\n",
      "iteration: 439  val_loss: 0.6030358076095581  val_accuracy: 56.875 %\n",
      "iteration: 440  val_loss: 0.6685099601745605  val_accuracy: 56.875 %\n",
      "iteration: 441  val_loss: 0.6346080303192139  val_accuracy: 56.875 %\n",
      "iteration: 442  val_loss: 0.7025246024131775  val_accuracy: 56.875 %\n",
      "iteration: 443  val_loss: 0.6341911554336548  val_accuracy: 56.875 %\n",
      "iteration: 444  val_loss: 0.6681052446365356  val_accuracy: 56.875 %\n",
      "iteration: 445  val_loss: 0.598611056804657  val_accuracy: 56.875 %\n",
      "iteration: 446  val_loss: 0.7386643886566162  val_accuracy: 56.875 %\n",
      "iteration: 447  val_loss: 0.6679191589355469  val_accuracy: 56.875 %\n",
      "iteration: 448  val_loss: 0.6327279210090637  val_accuracy: 56.875 %\n",
      "iteration: 449  val_loss: 0.632080078125  val_accuracy: 56.875 %\n",
      "iteration: 450  val_loss: 0.6674670577049255  val_accuracy: 56.875 %\n",
      "iteration: 451  val_loss: 0.6674099564552307  val_accuracy: 56.875 %\n",
      "iteration: 452  val_loss: 0.7399458885192871  val_accuracy: 56.875 %\n",
      "iteration: 453  val_loss: 0.6317618489265442  val_accuracy: 56.875 %\n",
      "iteration: 454  val_loss: 0.6673728823661804  val_accuracy: 56.875 %\n",
      "iteration: 455  val_loss: 0.6673166751861572  val_accuracy: 56.875 %\n",
      "iteration: 456  val_loss: 0.6307458877563477  val_accuracy: 56.875 %\n",
      "iteration: 457  val_loss: 0.6301162838935852  val_accuracy: 56.875 %\n",
      "iteration: 458  val_loss: 0.6668946146965027  val_accuracy: 56.875 %\n",
      "iteration: 459  val_loss: 0.7043733596801758  val_accuracy: 56.875 %\n",
      "iteration: 460  val_loss: 0.7789489030838013  val_accuracy: 56.875 %\n",
      "iteration: 461  val_loss: 0.6672550439834595  val_accuracy: 56.875 %\n",
      "iteration: 462  val_loss: 0.6672000885009766  val_accuracy: 56.875 %\n",
      "iteration: 463  val_loss: 0.6671454906463623  val_accuracy: 56.875 %\n",
      "iteration: 464  val_loss: 0.7409363389015198  val_accuracy: 56.875 %\n",
      "iteration: 465  val_loss: 0.7037240862846375  val_accuracy: 56.875 %\n",
      "iteration: 466  val_loss: 0.6673757433891296  val_accuracy: 56.875 %\n",
      "iteration: 467  val_loss: 0.6309430003166199  val_accuracy: 56.875 %\n",
      "iteration: 468  val_loss: 0.667133092880249  val_accuracy: 56.875 %\n",
      "iteration: 469  val_loss: 0.6670791506767273  val_accuracy: 56.875 %\n",
      "iteration: 470  val_loss: 0.6299439668655396  val_accuracy: 56.875 %\n",
      "iteration: 471  val_loss: 0.6293206810951233  val_accuracy: 56.875 %\n",
      "iteration: 472  val_loss: 0.6287029385566711  val_accuracy: 56.875 %\n",
      "iteration: 473  val_loss: 0.666495680809021  val_accuracy: 56.875 %\n",
      "iteration: 474  val_loss: 0.6664478182792664  val_accuracy: 56.875 %\n",
      "iteration: 475  val_loss: 0.6664004325866699  val_accuracy: 56.875 %\n",
      "iteration: 476  val_loss: 0.6663535237312317  val_accuracy: 56.875 %\n",
      "iteration: 477  val_loss: 0.6663070917129517  val_accuracy: 56.875 %\n",
      "iteration: 478  val_loss: 0.6662610769271851  val_accuracy: 56.875 %\n",
      "iteration: 479  val_loss: 0.6270820498466492  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 480  val_loss: 0.6264841556549072  val_accuracy: 56.875 %\n",
      "iteration: 481  val_loss: 0.8658978343009949  val_accuracy: 56.875 %\n",
      "iteration: 482  val_loss: 0.743520975112915  val_accuracy: 56.875 %\n",
      "iteration: 483  val_loss: 0.666642427444458  val_accuracy: 56.875 %\n",
      "iteration: 484  val_loss: 0.7429054975509644  val_accuracy: 56.875 %\n",
      "iteration: 485  val_loss: 0.7044455409049988  val_accuracy: 56.875 %\n",
      "iteration: 486  val_loss: 0.6294065713882446  val_accuracy: 56.875 %\n",
      "iteration: 487  val_loss: 0.666692852973938  val_accuracy: 56.875 %\n",
      "iteration: 488  val_loss: 0.6286132335662842  val_accuracy: 56.875 %\n",
      "iteration: 489  val_loss: 0.5895327925682068  val_accuracy: 56.875 %\n",
      "iteration: 490  val_loss: 0.7054033279418945  val_accuracy: 56.875 %\n",
      "iteration: 491  val_loss: 0.5882067680358887  val_accuracy: 56.875 %\n",
      "iteration: 492  val_loss: 0.6659753918647766  val_accuracy: 56.875 %\n",
      "iteration: 493  val_loss: 0.7058243155479431  val_accuracy: 56.875 %\n",
      "iteration: 494  val_loss: 0.7056982517242432  val_accuracy: 56.875 %\n",
      "iteration: 495  val_loss: 0.6660800576210022  val_accuracy: 56.875 %\n",
      "iteration: 496  val_loss: 0.7056472301483154  val_accuracy: 56.875 %\n",
      "iteration: 497  val_loss: 0.7843484878540039  val_accuracy: 56.875 %\n",
      "iteration: 498  val_loss: 0.6278402209281921  val_accuracy: 56.875 %\n",
      "iteration: 499  val_loss: 0.6662577390670776  val_accuracy: 56.875 %\n",
      "iteration: 500  val_loss: 0.7836382389068604  val_accuracy: 56.875 %\n",
      "iteration: 501  val_loss: 0.6282139420509338  val_accuracy: 56.875 %\n",
      "iteration: 502  val_loss: 0.6663601994514465  val_accuracy: 56.875 %\n",
      "iteration: 503  val_loss: 0.7440645694732666  val_accuracy: 56.875 %\n",
      "iteration: 504  val_loss: 0.6281445622444153  val_accuracy: 56.875 %\n",
      "iteration: 505  val_loss: 0.6275370717048645  val_accuracy: 56.875 %\n",
      "iteration: 506  val_loss: 0.6269351840019226  val_accuracy: 56.875 %\n",
      "iteration: 507  val_loss: 0.7056870460510254  val_accuracy: 56.875 %\n",
      "iteration: 508  val_loss: 0.6660866737365723  val_accuracy: 56.875 %\n",
      "iteration: 509  val_loss: 0.7056361436843872  val_accuracy: 56.875 %\n",
      "iteration: 510  val_loss: 0.7843024134635925  val_accuracy: 56.875 %\n",
      "iteration: 511  val_loss: 0.7435680031776428  val_accuracy: 56.875 %\n",
      "iteration: 512  val_loss: 0.8188731074333191  val_accuracy: 56.875 %\n",
      "iteration: 513  val_loss: 0.5932328701019287  val_accuracy: 56.875 %\n",
      "iteration: 514  val_loss: 0.7044658660888672  val_accuracy: 56.875 %\n",
      "iteration: 515  val_loss: 0.6293590068817139  val_accuracy: 56.875 %\n",
      "iteration: 516  val_loss: 0.6287409067153931  val_accuracy: 56.875 %\n",
      "iteration: 517  val_loss: 0.6665062308311462  val_accuracy: 56.875 %\n",
      "iteration: 518  val_loss: 0.7434598207473755  val_accuracy: 56.875 %\n",
      "iteration: 519  val_loss: 0.6666574478149414  val_accuracy: 56.875 %\n",
      "iteration: 520  val_loss: 0.70472651720047  val_accuracy: 56.875 %\n",
      "iteration: 521  val_loss: 0.6666834354400635  val_accuracy: 56.875 %\n",
      "iteration: 522  val_loss: 0.7046871185302734  val_accuracy: 56.875 %\n",
      "iteration: 523  val_loss: 0.6667092442512512  val_accuracy: 56.875 %\n",
      "iteration: 524  val_loss: 0.6666594743728638  val_accuracy: 56.875 %\n",
      "iteration: 525  val_loss: 0.5903832912445068  val_accuracy: 56.875 %\n",
      "iteration: 526  val_loss: 0.7051838040351868  val_accuracy: 56.875 %\n",
      "iteration: 527  val_loss: 0.6663918495178223  val_accuracy: 56.875 %\n",
      "iteration: 528  val_loss: 0.6275509595870972  val_accuracy: 56.875 %\n",
      "iteration: 529  val_loss: 0.6269490122795105  val_accuracy: 56.875 %\n",
      "iteration: 530  val_loss: 0.7850093245506287  val_accuracy: 56.875 %\n",
      "iteration: 531  val_loss: 0.6663292050361633  val_accuracy: 56.875 %\n",
      "iteration: 532  val_loss: 0.6662828922271729  val_accuracy: 56.875 %\n",
      "iteration: 533  val_loss: 0.7053133249282837  val_accuracy: 56.875 %\n",
      "iteration: 534  val_loss: 0.7440731525421143  val_accuracy: 56.875 %\n",
      "iteration: 535  val_loss: 0.6665087342262268  val_accuracy: 56.875 %\n",
      "iteration: 536  val_loss: 0.6664607524871826  val_accuracy: 56.875 %\n",
      "iteration: 537  val_loss: 0.6664133071899414  val_accuracy: 56.875 %\n",
      "iteration: 538  val_loss: 0.627627432346344  val_accuracy: 56.875 %\n",
      "iteration: 539  val_loss: 0.7053746581077576  val_accuracy: 56.875 %\n",
      "iteration: 540  val_loss: 0.744232177734375  val_accuracy: 56.875 %\n",
      "iteration: 541  val_loss: 0.7049397230148315  val_accuracy: 56.875 %\n",
      "iteration: 542  val_loss: 0.7048227787017822  val_accuracy: 56.875 %\n",
      "iteration: 543  val_loss: 0.6666209101676941  val_accuracy: 56.875 %\n",
      "iteration: 544  val_loss: 0.7047821879386902  val_accuracy: 56.875 %\n",
      "iteration: 545  val_loss: 0.5906080603599548  val_accuracy: 56.875 %\n",
      "iteration: 546  val_loss: 0.6663529872894287  val_accuracy: 56.875 %\n",
      "iteration: 547  val_loss: 0.6274123191833496  val_accuracy: 56.875 %\n",
      "iteration: 548  val_loss: 0.6268115639686584  val_accuracy: 56.875 %\n",
      "iteration: 549  val_loss: 0.6262161135673523  val_accuracy: 56.875 %\n",
      "iteration: 550  val_loss: 0.6658216118812561  val_accuracy: 56.875 %\n",
      "iteration: 551  val_loss: 0.6254702210426331  val_accuracy: 56.875 %\n",
      "iteration: 552  val_loss: 0.7063665390014648  val_accuracy: 56.875 %\n",
      "iteration: 553  val_loss: 0.6656995415687561  val_accuracy: 56.875 %\n",
      "iteration: 554  val_loss: 0.6656594276428223  val_accuracy: 56.875 %\n",
      "iteration: 555  val_loss: 0.7063801288604736  val_accuracy: 56.875 %\n",
      "iteration: 556  val_loss: 0.6656922101974487  val_accuracy: 56.875 %\n",
      "iteration: 557  val_loss: 0.6656519770622253  val_accuracy: 56.875 %\n",
      "iteration: 558  val_loss: 0.7471744418144226  val_accuracy: 56.875 %\n",
      "iteration: 559  val_loss: 0.6657983660697937  val_accuracy: 56.875 %\n",
      "iteration: 560  val_loss: 0.7465062737464905  val_accuracy: 56.875 %\n",
      "iteration: 561  val_loss: 0.6659455895423889  val_accuracy: 56.875 %\n",
      "iteration: 562  val_loss: 0.7058754563331604  val_accuracy: 56.875 %\n",
      "iteration: 563  val_loss: 0.7057487964630127  val_accuracy: 56.875 %\n",
      "iteration: 564  val_loss: 0.7056233882904053  val_accuracy: 56.875 %\n",
      "iteration: 565  val_loss: 0.7054993510246277  val_accuracy: 56.875 %\n",
      "iteration: 566  val_loss: 0.6661984920501709  val_accuracy: 56.875 %\n",
      "iteration: 567  val_loss: 0.7054508328437805  val_accuracy: 56.875 %\n",
      "iteration: 568  val_loss: 0.7053285837173462  val_accuracy: 56.875 %\n",
      "iteration: 569  val_loss: 0.7052075862884521  val_accuracy: 56.875 %\n",
      "iteration: 570  val_loss: 0.6276662945747375  val_accuracy: 56.875 %\n",
      "iteration: 571  val_loss: 0.7053574323654175  val_accuracy: 56.875 %\n",
      "iteration: 572  val_loss: 0.5883821845054626  val_accuracy: 56.875 %\n",
      "iteration: 573  val_loss: 0.824809730052948  val_accuracy: 56.875 %\n",
      "iteration: 574  val_loss: 0.6278790831565857  val_accuracy: 56.875 %\n",
      "iteration: 575  val_loss: 0.6662683486938477  val_accuracy: 56.875 %\n",
      "iteration: 576  val_loss: 0.6662227511405945  val_accuracy: 56.875 %\n",
      "iteration: 577  val_loss: 0.7054112553596497  val_accuracy: 56.875 %\n",
      "iteration: 578  val_loss: 0.6272143721580505  val_accuracy: 56.875 %\n",
      "iteration: 579  val_loss: 0.6266153454780579  val_accuracy: 56.875 %\n",
      "iteration: 580  val_loss: 0.7457389235496521  val_accuracy: 56.875 %\n",
      "iteration: 581  val_loss: 0.744899332523346  val_accuracy: 56.875 %\n",
      "iteration: 582  val_loss: 0.6663127541542053  val_accuracy: 56.875 %\n",
      "iteration: 583  val_loss: 0.8222616910934448  val_accuracy: 56.875 %\n",
      "iteration: 584  val_loss: 0.7045704126358032  val_accuracy: 56.875 %\n",
      "iteration: 585  val_loss: 0.5914450883865356  val_accuracy: 56.875 %\n",
      "iteration: 586  val_loss: 0.7049130797386169  val_accuracy: 56.875 %\n",
      "iteration: 587  val_loss: 0.6283293962478638  val_accuracy: 56.875 %\n",
      "iteration: 588  val_loss: 0.782406747341156  val_accuracy: 56.875 %\n",
      "iteration: 589  val_loss: 0.6667149662971497  val_accuracy: 56.875 %\n",
      "iteration: 590  val_loss: 0.7046396136283875  val_accuracy: 56.875 %\n",
      "iteration: 591  val_loss: 0.6289554834365845  val_accuracy: 56.875 %\n",
      "iteration: 592  val_loss: 0.6283410787582397  val_accuracy: 56.875 %\n",
      "iteration: 593  val_loss: 0.6663954257965088  val_accuracy: 56.875 %\n",
      "iteration: 594  val_loss: 0.6663485765457153  val_accuracy: 56.875 %\n",
      "iteration: 595  val_loss: 0.705207884311676  val_accuracy: 56.875 %\n",
      "iteration: 596  val_loss: 0.6663768887519836  val_accuracy: 56.875 %\n",
      "iteration: 597  val_loss: 0.7051628232002258  val_accuracy: 56.875 %\n",
      "iteration: 598  val_loss: 0.8595981001853943  val_accuracy: 56.875 %\n",
      "iteration: 599  val_loss: 0.7413579225540161  val_accuracy: 56.875 %\n",
      "iteration: 600  val_loss: 0.7772552967071533  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 601  val_loss: 0.7034109234809875  val_accuracy: 56.875 %\n",
      "iteration: 602  val_loss: 0.7033095359802246  val_accuracy: 56.875 %\n",
      "iteration: 603  val_loss: 0.7387331128120422  val_accuracy: 56.875 %\n",
      "iteration: 604  val_loss: 0.6328645348548889  val_accuracy: 56.875 %\n",
      "iteration: 605  val_loss: 0.7741594910621643  val_accuracy: 56.875 %\n",
      "iteration: 606  val_loss: 0.7027407884597778  val_accuracy: 56.875 %\n",
      "iteration: 607  val_loss: 0.6681333184242249  val_accuracy: 56.875 %\n",
      "iteration: 608  val_loss: 0.702723503112793  val_accuracy: 56.875 %\n",
      "iteration: 609  val_loss: 0.7026290893554688  val_accuracy: 56.875 %\n",
      "iteration: 610  val_loss: 0.6682248711585999  val_accuracy: 56.875 %\n",
      "iteration: 611  val_loss: 0.7026130557060242  val_accuracy: 56.875 %\n",
      "iteration: 612  val_loss: 0.66823810338974  val_accuracy: 56.875 %\n",
      "iteration: 613  val_loss: 0.599327027797699  val_accuracy: 56.875 %\n",
      "iteration: 614  val_loss: 0.6326432824134827  val_accuracy: 56.875 %\n",
      "iteration: 615  val_loss: 0.7389127016067505  val_accuracy: 56.875 %\n",
      "iteration: 616  val_loss: 0.6327002048492432  val_accuracy: 56.875 %\n",
      "iteration: 617  val_loss: 0.7032516002655029  val_accuracy: 56.875 %\n",
      "iteration: 618  val_loss: 0.7385743856430054  val_accuracy: 56.875 %\n",
      "iteration: 619  val_loss: 0.7028787732124329  val_accuracy: 56.875 %\n",
      "iteration: 620  val_loss: 0.6680218577384949  val_accuracy: 56.875 %\n",
      "iteration: 621  val_loss: 0.6679596304893494  val_accuracy: 56.875 %\n",
      "iteration: 622  val_loss: 0.632858395576477  val_accuracy: 56.875 %\n",
      "iteration: 623  val_loss: 0.6676997542381287  val_accuracy: 56.875 %\n",
      "iteration: 624  val_loss: 0.7032669186592102  val_accuracy: 56.875 %\n",
      "iteration: 625  val_loss: 0.7386163473129272  val_accuracy: 56.875 %\n",
      "iteration: 626  val_loss: 0.6329717040061951  val_accuracy: 56.875 %\n",
      "iteration: 627  val_loss: 0.7385578751564026  val_accuracy: 56.875 %\n",
      "iteration: 628  val_loss: 0.6679493188858032  val_accuracy: 56.875 %\n",
      "iteration: 629  val_loss: 0.6328250169754028  val_accuracy: 56.875 %\n",
      "iteration: 630  val_loss: 0.6676896810531616  val_accuracy: 56.875 %\n",
      "iteration: 631  val_loss: 0.667630672454834  val_accuracy: 56.875 %\n",
      "iteration: 632  val_loss: 0.7391417622566223  val_accuracy: 56.875 %\n",
      "iteration: 633  val_loss: 0.6324909329414368  val_accuracy: 56.875 %\n",
      "iteration: 634  val_loss: 0.667589545249939  val_accuracy: 56.875 %\n",
      "iteration: 635  val_loss: 0.7751690149307251  val_accuracy: 56.875 %\n",
      "iteration: 636  val_loss: 0.7380329370498657  val_accuracy: 56.875 %\n",
      "iteration: 637  val_loss: 0.6680988073348999  val_accuracy: 56.875 %\n",
      "iteration: 638  val_loss: 0.633306086063385  val_accuracy: 56.875 %\n",
      "iteration: 639  val_loss: 0.7030166387557983  val_accuracy: 56.875 %\n",
      "iteration: 640  val_loss: 0.6329052448272705  val_accuracy: 56.875 %\n",
      "iteration: 641  val_loss: 0.632256031036377  val_accuracy: 56.875 %\n",
      "iteration: 642  val_loss: 0.6675194501876831  val_accuracy: 56.875 %\n",
      "iteration: 643  val_loss: 0.7035031318664551  val_accuracy: 56.875 %\n",
      "iteration: 644  val_loss: 0.7392628192901611  val_accuracy: 56.875 %\n",
      "iteration: 645  val_loss: 0.7031228542327881  val_accuracy: 56.875 %\n",
      "iteration: 646  val_loss: 0.738219678401947  val_accuracy: 56.875 %\n",
      "iteration: 647  val_loss: 0.7027537822723389  val_accuracy: 56.875 %\n",
      "iteration: 648  val_loss: 0.6335864663124084  val_accuracy: 56.875 %\n",
      "iteration: 649  val_loss: 0.6329312920570374  val_accuracy: 56.875 %\n",
      "iteration: 650  val_loss: 0.5614022612571716  val_accuracy: 56.875 %\n",
      "iteration: 651  val_loss: 0.7768049240112305  val_accuracy: 56.875 %\n",
      "iteration: 652  val_loss: 0.6318991184234619  val_accuracy: 56.875 %\n",
      "iteration: 653  val_loss: 0.7397230863571167  val_accuracy: 56.875 %\n",
      "iteration: 654  val_loss: 0.5963008999824524  val_accuracy: 56.875 %\n",
      "iteration: 655  val_loss: 0.7037230730056763  val_accuracy: 56.875 %\n",
      "iteration: 656  val_loss: 0.7398602962493896  val_accuracy: 56.875 %\n",
      "iteration: 657  val_loss: 0.5960900783538818  val_accuracy: 56.875 %\n",
      "iteration: 658  val_loss: 0.6672635078430176  val_accuracy: 56.875 %\n",
      "iteration: 659  val_loss: 0.6672083735466003  val_accuracy: 56.875 %\n",
      "iteration: 660  val_loss: 0.7039258480072021  val_accuracy: 56.875 %\n",
      "iteration: 661  val_loss: 0.7038192749023438  val_accuracy: 56.875 %\n",
      "iteration: 662  val_loss: 0.7037136554718018  val_accuracy: 56.875 %\n",
      "iteration: 663  val_loss: 0.7036091685295105  val_accuracy: 56.875 %\n",
      "iteration: 664  val_loss: 0.7035057544708252  val_accuracy: 56.875 %\n",
      "iteration: 665  val_loss: 0.6316704154014587  val_accuracy: 56.875 %\n",
      "iteration: 666  val_loss: 0.6673460006713867  val_accuracy: 56.875 %\n",
      "iteration: 667  val_loss: 0.6308437585830688  val_accuracy: 56.875 %\n",
      "iteration: 668  val_loss: 0.5933215618133545  val_accuracy: 56.875 %\n",
      "iteration: 669  val_loss: 0.6667954325675964  val_accuracy: 56.875 %\n",
      "iteration: 670  val_loss: 0.6667447090148926  val_accuracy: 56.875 %\n",
      "iteration: 671  val_loss: 0.7045949101448059  val_accuracy: 56.875 %\n",
      "iteration: 672  val_loss: 0.6667701601982117  val_accuracy: 56.875 %\n",
      "iteration: 673  val_loss: 0.7045570611953735  val_accuracy: 56.875 %\n",
      "iteration: 674  val_loss: 0.7044438123703003  val_accuracy: 56.875 %\n",
      "iteration: 675  val_loss: 0.7417928576469421  val_accuracy: 56.875 %\n",
      "iteration: 676  val_loss: 0.7409967184066772  val_accuracy: 56.875 %\n",
      "iteration: 677  val_loss: 0.7037461400032043  val_accuracy: 56.875 %\n",
      "iteration: 678  val_loss: 0.5947964191436768  val_accuracy: 56.875 %\n",
      "iteration: 679  val_loss: 0.704083263874054  val_accuracy: 56.875 %\n",
      "iteration: 680  val_loss: 0.6302626729011536  val_accuracy: 56.875 %\n",
      "iteration: 681  val_loss: 0.5550375580787659  val_accuracy: 56.875 %\n",
      "iteration: 682  val_loss: 0.6281400918960571  val_accuracy: 56.875 %\n",
      "iteration: 683  val_loss: 0.7051469087600708  val_accuracy: 56.875 %\n",
      "iteration: 684  val_loss: 0.7822538018226624  val_accuracy: 56.875 %\n",
      "iteration: 685  val_loss: 0.704529345035553  val_accuracy: 56.875 %\n",
      "iteration: 686  val_loss: 0.7044166326522827  val_accuracy: 56.875 %\n",
      "iteration: 687  val_loss: 0.6668895483016968  val_accuracy: 56.875 %\n",
      "iteration: 688  val_loss: 0.7794660329818726  val_accuracy: 56.875 %\n",
      "iteration: 689  val_loss: 0.6671720743179321  val_accuracy: 56.875 %\n",
      "iteration: 690  val_loss: 0.7408350706100464  val_accuracy: 56.875 %\n",
      "iteration: 691  val_loss: 0.6673257946968079  val_accuracy: 56.875 %\n",
      "iteration: 692  val_loss: 0.6307759881019592  val_accuracy: 56.875 %\n",
      "iteration: 693  val_loss: 0.6301456689834595  val_accuracy: 56.875 %\n",
      "iteration: 694  val_loss: 0.592138946056366  val_accuracy: 56.875 %\n",
      "iteration: 695  val_loss: 0.6666006445884705  val_accuracy: 56.875 %\n",
      "iteration: 696  val_loss: 0.7048132419586182  val_accuracy: 56.875 %\n",
      "iteration: 697  val_loss: 0.5904859900474548  val_accuracy: 56.875 %\n",
      "iteration: 698  val_loss: 0.5886864066123962  val_accuracy: 56.875 %\n",
      "iteration: 699  val_loss: 0.7056242227554321  val_accuracy: 56.875 %\n",
      "iteration: 700  val_loss: 0.6267475485801697  val_accuracy: 56.875 %\n",
      "iteration: 701  val_loss: 0.586342990398407  val_accuracy: 56.875 %\n",
      "iteration: 702  val_loss: 0.6251344084739685  val_accuracy: 56.875 %\n",
      "iteration: 703  val_loss: 0.624553918838501  val_accuracy: 56.875 %\n",
      "iteration: 704  val_loss: 0.6653920412063599  val_accuracy: 56.875 %\n",
      "iteration: 705  val_loss: 0.6238327026367188  val_accuracy: 56.875 %\n",
      "iteration: 706  val_loss: 0.6652109622955322  val_accuracy: 56.875 %\n",
      "iteration: 707  val_loss: 0.6231220960617065  val_accuracy: 56.875 %\n",
      "iteration: 708  val_loss: 0.7499881982803345  val_accuracy: 56.875 %\n",
      "iteration: 709  val_loss: 0.6232662200927734  val_accuracy: 56.875 %\n",
      "iteration: 710  val_loss: 0.6650711297988892  val_accuracy: 56.875 %\n",
      "iteration: 711  val_loss: 0.7075098156929016  val_accuracy: 56.875 %\n",
      "iteration: 712  val_loss: 0.5805884599685669  val_accuracy: 56.875 %\n",
      "iteration: 713  val_loss: 0.7508646845817566  val_accuracy: 56.875 %\n",
      "iteration: 714  val_loss: 0.707504391670227  val_accuracy: 56.875 %\n",
      "iteration: 715  val_loss: 0.7918633222579956  val_accuracy: 56.875 %\n",
      "iteration: 716  val_loss: 0.7067998051643372  val_accuracy: 56.875 %\n",
      "iteration: 717  val_loss: 0.8714517951011658  val_accuracy: 56.875 %\n",
      "iteration: 718  val_loss: 0.626267671585083  val_accuracy: 56.875 %\n",
      "iteration: 719  val_loss: 0.7059934139251709  val_accuracy: 56.875 %\n",
      "iteration: 720  val_loss: 0.6659086346626282  val_accuracy: 56.875 %\n",
      "iteration: 721  val_loss: 0.7860839366912842  val_accuracy: 56.875 %\n",
      "iteration: 722  val_loss: 0.7054159045219421  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 723  val_loss: 0.6662489175796509  val_accuracy: 56.875 %\n",
      "iteration: 724  val_loss: 0.6662035584449768  val_accuracy: 56.875 %\n",
      "iteration: 725  val_loss: 0.7447264790534973  val_accuracy: 56.875 %\n",
      "iteration: 726  val_loss: 0.6275804042816162  val_accuracy: 56.875 %\n",
      "iteration: 727  val_loss: 0.6269786357879639  val_accuracy: 56.875 %\n",
      "iteration: 728  val_loss: 0.7056670784950256  val_accuracy: 56.875 %\n",
      "iteration: 729  val_loss: 0.666098415851593  val_accuracy: 56.875 %\n",
      "iteration: 730  val_loss: 0.7451785802841187  val_accuracy: 56.875 %\n",
      "iteration: 731  val_loss: 0.7052965760231018  val_accuracy: 56.875 %\n",
      "iteration: 732  val_loss: 0.588613748550415  val_accuracy: 56.875 %\n",
      "iteration: 733  val_loss: 0.7848517894744873  val_accuracy: 56.875 %\n",
      "iteration: 734  val_loss: 0.6275755763053894  val_accuracy: 56.875 %\n",
      "iteration: 735  val_loss: 0.5877617597579956  val_accuracy: 56.875 %\n",
      "iteration: 736  val_loss: 0.625946044921875  val_accuracy: 56.875 %\n",
      "iteration: 737  val_loss: 0.7061431407928467  val_accuracy: 56.875 %\n",
      "iteration: 738  val_loss: 0.6658238768577576  val_accuracy: 56.875 %\n",
      "iteration: 739  val_loss: 0.6254782676696777  val_accuracy: 56.875 %\n",
      "iteration: 740  val_loss: 0.7063629031181335  val_accuracy: 56.875 %\n",
      "iteration: 741  val_loss: 0.7467610836029053  val_accuracy: 56.875 %\n",
      "iteration: 742  val_loss: 0.6658889651298523  val_accuracy: 56.875 %\n",
      "iteration: 743  val_loss: 0.6658469438552856  val_accuracy: 56.875 %\n",
      "iteration: 744  val_loss: 0.625564455986023  val_accuracy: 56.875 %\n",
      "iteration: 745  val_loss: 0.6249804496765137  val_accuracy: 56.875 %\n",
      "iteration: 746  val_loss: 0.7476990222930908  val_accuracy: 56.875 %\n",
      "iteration: 747  val_loss: 0.6251079440116882  val_accuracy: 56.875 %\n",
      "iteration: 748  val_loss: 0.7475442886352539  val_accuracy: 56.875 %\n",
      "iteration: 749  val_loss: 0.6657180190086365  val_accuracy: 56.875 %\n",
      "iteration: 750  val_loss: 0.7062743902206421  val_accuracy: 56.875 %\n",
      "iteration: 751  val_loss: 0.6253570318222046  val_accuracy: 56.875 %\n",
      "iteration: 752  val_loss: 0.6247748732566833  val_accuracy: 56.875 %\n",
      "iteration: 753  val_loss: 0.6654481887817383  val_accuracy: 56.875 %\n",
      "iteration: 754  val_loss: 0.7067703008651733  val_accuracy: 56.875 %\n",
      "iteration: 755  val_loss: 0.7477868795394897  val_accuracy: 56.875 %\n",
      "iteration: 756  val_loss: 0.7062954902648926  val_accuracy: 56.875 %\n",
      "iteration: 757  val_loss: 0.7061647772789001  val_accuracy: 56.875 %\n",
      "iteration: 758  val_loss: 0.7462589740753174  val_accuracy: 56.875 %\n",
      "iteration: 759  val_loss: 0.6262940168380737  val_accuracy: 56.875 %\n",
      "iteration: 760  val_loss: 0.7059811353683472  val_accuracy: 56.875 %\n",
      "iteration: 761  val_loss: 0.5860399603843689  val_accuracy: 56.875 %\n",
      "iteration: 762  val_loss: 0.6249617338180542  val_accuracy: 56.875 %\n",
      "iteration: 763  val_loss: 0.6243831515312195  val_accuracy: 56.875 %\n",
      "iteration: 764  val_loss: 0.6653488874435425  val_accuracy: 56.875 %\n",
      "iteration: 765  val_loss: 0.5820173621177673  val_accuracy: 56.875 %\n",
      "iteration: 766  val_loss: 0.5802872180938721  val_accuracy: 56.875 %\n",
      "iteration: 767  val_loss: 0.6648262739181519  val_accuracy: 56.875 %\n",
      "iteration: 768  val_loss: 0.7080227136611938  val_accuracy: 56.875 %\n",
      "iteration: 769  val_loss: 0.707874059677124  val_accuracy: 56.875 %\n",
      "iteration: 770  val_loss: 0.7505209445953369  val_accuracy: 56.875 %\n",
      "iteration: 771  val_loss: 0.6228449940681458  val_accuracy: 56.875 %\n",
      "iteration: 772  val_loss: 0.6222850680351257  val_accuracy: 56.875 %\n",
      "iteration: 773  val_loss: 0.7941449880599976  val_accuracy: 56.875 %\n",
      "iteration: 774  val_loss: 0.7073612213134766  val_accuracy: 56.875 %\n",
      "iteration: 775  val_loss: 0.6651802062988281  val_accuracy: 56.875 %\n",
      "iteration: 776  val_loss: 0.5808550715446472  val_accuracy: 56.875 %\n",
      "iteration: 777  val_loss: 0.6220189332962036  val_accuracy: 56.875 %\n",
      "iteration: 778  val_loss: 0.6214663982391357  val_accuracy: 56.875 %\n",
      "iteration: 779  val_loss: 0.6209187507629395  val_accuracy: 56.875 %\n",
      "iteration: 780  val_loss: 0.7527901530265808  val_accuracy: 56.875 %\n",
      "iteration: 781  val_loss: 0.7082728743553162  val_accuracy: 56.875 %\n",
      "iteration: 782  val_loss: 0.664749801158905  val_accuracy: 56.875 %\n",
      "iteration: 783  val_loss: 0.6647185683250427  val_accuracy: 56.875 %\n",
      "iteration: 784  val_loss: 0.7082582712173462  val_accuracy: 56.875 %\n",
      "iteration: 785  val_loss: 0.751454770565033  val_accuracy: 56.875 %\n",
      "iteration: 786  val_loss: 0.7077363133430481  val_accuracy: 56.875 %\n",
      "iteration: 787  val_loss: 0.7075892686843872  val_accuracy: 56.875 %\n",
      "iteration: 788  val_loss: 0.7921929359436035  val_accuracy: 56.875 %\n",
      "iteration: 789  val_loss: 0.6653560996055603  val_accuracy: 56.875 %\n",
      "iteration: 790  val_loss: 0.7485741376876831  val_accuracy: 56.875 %\n",
      "iteration: 791  val_loss: 0.5422123074531555  val_accuracy: 56.875 %\n",
      "iteration: 792  val_loss: 0.6651374101638794  val_accuracy: 56.875 %\n",
      "iteration: 793  val_loss: 0.6651023626327515  val_accuracy: 56.875 %\n",
      "iteration: 794  val_loss: 0.7074472904205322  val_accuracy: 56.875 %\n",
      "iteration: 795  val_loss: 0.707302987575531  val_accuracy: 56.875 %\n",
      "iteration: 796  val_loss: 0.665209949016571  val_accuracy: 56.875 %\n",
      "iteration: 797  val_loss: 0.7913469076156616  val_accuracy: 56.875 %\n",
      "iteration: 798  val_loss: 0.665464460849762  val_accuracy: 56.875 %\n",
      "iteration: 799  val_loss: 0.5827972888946533  val_accuracy: 56.875 %\n",
      "iteration: 800  val_loss: 0.6651716828346252  val_accuracy: 56.875 %\n",
      "iteration: 801  val_loss: 0.6229645609855652  val_accuracy: 56.875 %\n",
      "iteration: 802  val_loss: 0.6649960875511169  val_accuracy: 56.875 %\n",
      "iteration: 803  val_loss: 0.7076647281646729  val_accuracy: 56.875 %\n",
      "iteration: 804  val_loss: 0.7075182795524597  val_accuracy: 56.875 %\n",
      "iteration: 805  val_loss: 0.7073736190795898  val_accuracy: 56.875 %\n",
      "iteration: 806  val_loss: 0.6651747822761536  val_accuracy: 56.875 %\n",
      "iteration: 807  val_loss: 0.7494637370109558  val_accuracy: 56.875 %\n",
      "iteration: 808  val_loss: 0.7069470882415771  val_accuracy: 56.875 %\n",
      "iteration: 809  val_loss: 0.6653905510902405  val_accuracy: 56.875 %\n",
      "iteration: 810  val_loss: 0.7068803906440735  val_accuracy: 56.875 %\n",
      "iteration: 811  val_loss: 0.7480592727661133  val_accuracy: 56.875 %\n",
      "iteration: 812  val_loss: 0.7063982486724854  val_accuracy: 56.875 %\n",
      "iteration: 813  val_loss: 0.7062650918960571  val_accuracy: 56.875 %\n",
      "iteration: 814  val_loss: 0.6657562255859375  val_accuracy: 56.875 %\n",
      "iteration: 815  val_loss: 0.5847317576408386  val_accuracy: 56.875 %\n",
      "iteration: 816  val_loss: 0.6242070198059082  val_accuracy: 56.875 %\n",
      "iteration: 817  val_loss: 0.5819563865661621  val_accuracy: 56.875 %\n",
      "iteration: 818  val_loss: 0.749896764755249  val_accuracy: 56.875 %\n",
      "iteration: 819  val_loss: 0.7908900380134583  val_accuracy: 56.875 %\n",
      "iteration: 820  val_loss: 0.7065579295158386  val_accuracy: 56.875 %\n",
      "iteration: 821  val_loss: 0.7064231634140015  val_accuracy: 56.875 %\n",
      "iteration: 822  val_loss: 0.7062897086143494  val_accuracy: 56.875 %\n",
      "iteration: 823  val_loss: 0.7061578631401062  val_accuracy: 56.875 %\n",
      "iteration: 824  val_loss: 0.706027090549469  val_accuracy: 56.875 %\n",
      "iteration: 825  val_loss: 0.7459055185317993  val_accuracy: 56.875 %\n",
      "iteration: 826  val_loss: 0.7055691480636597  val_accuracy: 56.875 %\n",
      "iteration: 827  val_loss: 0.5875834822654724  val_accuracy: 56.875 %\n",
      "iteration: 828  val_loss: 0.745964765548706  val_accuracy: 56.875 %\n",
      "iteration: 829  val_loss: 0.7846390008926392  val_accuracy: 56.875 %\n",
      "iteration: 830  val_loss: 0.6663862466812134  val_accuracy: 56.875 %\n",
      "iteration: 831  val_loss: 0.7051485776901245  val_accuracy: 56.875 %\n",
      "iteration: 832  val_loss: 0.6664145588874817  val_accuracy: 56.875 %\n",
      "iteration: 833  val_loss: 0.6663670539855957  val_accuracy: 56.875 %\n",
      "iteration: 834  val_loss: 0.6274608373641968  val_accuracy: 56.875 %\n",
      "iteration: 835  val_loss: 0.7054521441459656  val_accuracy: 56.875 %\n",
      "iteration: 836  val_loss: 0.6662278175354004  val_accuracy: 56.875 %\n",
      "iteration: 837  val_loss: 0.7054036855697632  val_accuracy: 56.875 %\n",
      "iteration: 838  val_loss: 0.7443038821220398  val_accuracy: 56.875 %\n",
      "iteration: 839  val_loss: 0.7049636244773865  val_accuracy: 56.875 %\n",
      "iteration: 840  val_loss: 0.7431589365005493  val_accuracy: 56.875 %\n",
      "iteration: 841  val_loss: 0.5911275148391724  val_accuracy: 56.875 %\n",
      "iteration: 842  val_loss: 0.7435632944107056  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 843  val_loss: 0.7807918190956116  val_accuracy: 56.875 %\n",
      "iteration: 844  val_loss: 0.7414184212684631  val_accuracy: 56.875 %\n",
      "iteration: 845  val_loss: 0.7038963437080383  val_accuracy: 56.875 %\n",
      "iteration: 846  val_loss: 0.6307154297828674  val_accuracy: 56.875 %\n",
      "iteration: 847  val_loss: 0.7040512561798096  val_accuracy: 56.875 %\n",
      "iteration: 848  val_loss: 0.7407423853874207  val_accuracy: 56.875 %\n",
      "iteration: 849  val_loss: 0.6310543417930603  val_accuracy: 56.875 %\n",
      "iteration: 850  val_loss: 0.5936658978462219  val_accuracy: 56.875 %\n",
      "iteration: 851  val_loss: 0.6668494343757629  val_accuracy: 56.875 %\n",
      "iteration: 852  val_loss: 0.6667977571487427  val_accuracy: 56.875 %\n",
      "iteration: 853  val_loss: 0.7045167684555054  val_accuracy: 56.875 %\n",
      "iteration: 854  val_loss: 0.7419829368591309  val_accuracy: 56.875 %\n",
      "iteration: 855  val_loss: 0.6670292019844055  val_accuracy: 56.875 %\n",
      "iteration: 856  val_loss: 0.6669756770133972  val_accuracy: 56.875 %\n",
      "iteration: 857  val_loss: 0.7042561769485474  val_accuracy: 56.875 %\n",
      "iteration: 858  val_loss: 0.704145073890686  val_accuracy: 56.875 %\n",
      "iteration: 859  val_loss: 0.7409936189651489  val_accuracy: 56.875 %\n",
      "iteration: 860  val_loss: 0.6672860383987427  val_accuracy: 56.875 %\n",
      "iteration: 861  val_loss: 0.6672301888465881  val_accuracy: 56.875 %\n",
      "iteration: 862  val_loss: 0.5937315821647644  val_accuracy: 56.875 %\n",
      "iteration: 863  val_loss: 0.6668604612350464  val_accuracy: 56.875 %\n",
      "iteration: 864  val_loss: 0.553961992263794  val_accuracy: 56.875 %\n",
      "iteration: 865  val_loss: 0.6276871562004089  val_accuracy: 56.875 %\n",
      "iteration: 866  val_loss: 0.6662142872810364  val_accuracy: 56.875 %\n",
      "iteration: 867  val_loss: 0.7054257988929749  val_accuracy: 56.875 %\n",
      "iteration: 868  val_loss: 0.627185046672821  val_accuracy: 56.875 %\n",
      "iteration: 869  val_loss: 0.6660782694816589  val_accuracy: 56.875 %\n",
      "iteration: 870  val_loss: 0.7452676892280579  val_accuracy: 56.875 %\n",
      "iteration: 871  val_loss: 0.7053275108337402  val_accuracy: 56.875 %\n",
      "iteration: 872  val_loss: 0.6274018883705139  val_accuracy: 56.875 %\n",
      "iteration: 873  val_loss: 0.7841598987579346  val_accuracy: 56.875 %\n",
      "iteration: 874  val_loss: 0.704962968826294  val_accuracy: 56.875 %\n",
      "iteration: 875  val_loss: 0.5899060964584351  val_accuracy: 56.875 %\n",
      "iteration: 876  val_loss: 0.6271665692329407  val_accuracy: 56.875 %\n",
      "iteration: 877  val_loss: 0.6660735607147217  val_accuracy: 56.875 %\n",
      "iteration: 878  val_loss: 0.6660293936729431  val_accuracy: 56.875 %\n",
      "iteration: 879  val_loss: 0.6262384057044983  val_accuracy: 56.875 %\n",
      "iteration: 880  val_loss: 0.7461925745010376  val_accuracy: 56.875 %\n",
      "iteration: 881  val_loss: 0.705679178237915  val_accuracy: 56.875 %\n",
      "iteration: 882  val_loss: 0.7055535316467285  val_accuracy: 56.875 %\n",
      "iteration: 883  val_loss: 0.6661666035652161  val_accuracy: 56.875 %\n",
      "iteration: 884  val_loss: 0.6267392635345459  val_accuracy: 56.875 %\n",
      "iteration: 885  val_loss: 0.6659590601921082  val_accuracy: 56.875 %\n",
      "iteration: 886  val_loss: 0.7457897067070007  val_accuracy: 56.875 %\n",
      "iteration: 887  val_loss: 0.5872747302055359  val_accuracy: 56.875 %\n",
      "iteration: 888  val_loss: 0.7461746335029602  val_accuracy: 56.875 %\n",
      "iteration: 889  val_loss: 0.666021466255188  val_accuracy: 56.875 %\n",
      "iteration: 890  val_loss: 0.6659777760505676  val_accuracy: 56.875 %\n",
      "iteration: 891  val_loss: 0.7457071542739868  val_accuracy: 56.875 %\n",
      "iteration: 892  val_loss: 0.6661274433135986  val_accuracy: 56.875 %\n",
      "iteration: 893  val_loss: 0.745055079460144  val_accuracy: 56.875 %\n",
      "iteration: 894  val_loss: 0.7442160248756409  val_accuracy: 56.875 %\n",
      "iteration: 895  val_loss: 0.5895658135414124  val_accuracy: 56.875 %\n",
      "iteration: 896  val_loss: 0.6661850810050964  val_accuracy: 56.875 %\n",
      "iteration: 897  val_loss: 0.7448070645332336  val_accuracy: 56.875 %\n",
      "iteration: 898  val_loss: 0.7051533460617065  val_accuracy: 56.875 %\n",
      "iteration: 899  val_loss: 0.7050331830978394  val_accuracy: 56.875 %\n",
      "iteration: 900  val_loss: 0.7049142122268677  val_accuracy: 56.875 %\n",
      "iteration: 901  val_loss: 0.5900952219963074  val_accuracy: 56.875 %\n",
      "iteration: 902  val_loss: 0.7832474708557129  val_accuracy: 56.875 %\n",
      "iteration: 903  val_loss: 0.7047526240348816  val_accuracy: 56.875 %\n",
      "iteration: 904  val_loss: 0.7805752158164978  val_accuracy: 56.875 %\n",
      "iteration: 905  val_loss: 0.666999876499176  val_accuracy: 56.875 %\n",
      "iteration: 906  val_loss: 0.7414959073066711  val_accuracy: 56.875 %\n",
      "iteration: 907  val_loss: 0.5936126112937927  val_accuracy: 56.875 %\n",
      "iteration: 908  val_loss: 0.7794457674026489  val_accuracy: 56.875 %\n",
      "iteration: 909  val_loss: 0.5937517881393433  val_accuracy: 56.875 %\n",
      "iteration: 910  val_loss: 0.7418192028999329  val_accuracy: 56.875 %\n",
      "iteration: 911  val_loss: 0.7410157918930054  val_accuracy: 56.875 %\n",
      "iteration: 912  val_loss: 0.667279839515686  val_accuracy: 56.875 %\n",
      "iteration: 913  val_loss: 0.6306205987930298  val_accuracy: 56.875 %\n",
      "iteration: 914  val_loss: 0.5558822154998779  val_accuracy: 56.875 %\n",
      "iteration: 915  val_loss: 0.5903396010398865  val_accuracy: 56.875 %\n",
      "iteration: 916  val_loss: 0.7051988244056702  val_accuracy: 56.875 %\n",
      "iteration: 917  val_loss: 0.6276879906654358  val_accuracy: 56.875 %\n",
      "iteration: 918  val_loss: 0.7836198210716248  val_accuracy: 56.875 %\n",
      "iteration: 919  val_loss: 0.6282329559326172  val_accuracy: 56.875 %\n",
      "iteration: 920  val_loss: 0.5888757705688477  val_accuracy: 56.875 %\n",
      "iteration: 921  val_loss: 0.6660770773887634  val_accuracy: 56.875 %\n",
      "iteration: 922  val_loss: 0.7848934531211853  val_accuracy: 56.875 %\n",
      "iteration: 923  val_loss: 0.7051331996917725  val_accuracy: 56.875 %\n",
      "iteration: 924  val_loss: 0.6664239168167114  val_accuracy: 56.875 %\n",
      "iteration: 925  val_loss: 0.5889522433280945  val_accuracy: 56.875 %\n",
      "iteration: 926  val_loss: 0.5476799607276917  val_accuracy: 56.875 %\n",
      "iteration: 927  val_loss: 0.6251540780067444  val_accuracy: 56.875 %\n",
      "iteration: 928  val_loss: 0.706519365310669  val_accuracy: 56.875 %\n",
      "iteration: 929  val_loss: 0.7063850164413452  val_accuracy: 56.875 %\n",
      "iteration: 930  val_loss: 0.6656899452209473  val_accuracy: 56.875 %\n",
      "iteration: 931  val_loss: 0.6249738335609436  val_accuracy: 56.875 %\n",
      "iteration: 932  val_loss: 0.6654977798461914  val_accuracy: 56.875 %\n",
      "iteration: 933  val_loss: 0.7066779136657715  val_accuracy: 56.875 %\n",
      "iteration: 934  val_loss: 0.7475520968437195  val_accuracy: 56.875 %\n",
      "iteration: 935  val_loss: 0.5847501158714294  val_accuracy: 56.875 %\n",
      "iteration: 936  val_loss: 0.7066889405250549  val_accuracy: 56.875 %\n",
      "iteration: 937  val_loss: 0.7475795745849609  val_accuracy: 56.875 %\n",
      "iteration: 938  val_loss: 0.7062124609947205  val_accuracy: 56.875 %\n",
      "iteration: 939  val_loss: 0.6657856106758118  val_accuracy: 56.875 %\n",
      "iteration: 940  val_loss: 0.7061548233032227  val_accuracy: 56.875 %\n",
      "iteration: 941  val_loss: 0.7462306618690491  val_accuracy: 56.875 %\n",
      "iteration: 942  val_loss: 0.7056939005851746  val_accuracy: 56.875 %\n",
      "iteration: 943  val_loss: 0.6265984773635864  val_accuracy: 56.875 %\n",
      "iteration: 944  val_loss: 0.7058432102203369  val_accuracy: 56.875 %\n",
      "iteration: 945  val_loss: 0.7057158946990967  val_accuracy: 56.875 %\n",
      "iteration: 946  val_loss: 0.7451094388961792  val_accuracy: 56.875 %\n",
      "iteration: 947  val_loss: 0.7442700266838074  val_accuracy: 56.875 %\n",
      "iteration: 948  val_loss: 0.6664631366729736  val_accuracy: 56.875 %\n",
      "iteration: 949  val_loss: 0.7050269842147827  val_accuracy: 56.875 %\n",
      "iteration: 950  val_loss: 0.7049080729484558  val_accuracy: 56.875 %\n",
      "iteration: 951  val_loss: 0.7430143356323242  val_accuracy: 56.875 %\n",
      "iteration: 952  val_loss: 0.6667688488960266  val_accuracy: 56.875 %\n",
      "iteration: 953  val_loss: 0.6667180061340332  val_accuracy: 56.875 %\n",
      "iteration: 954  val_loss: 0.7426036596298218  val_accuracy: 56.875 %\n",
      "iteration: 955  val_loss: 0.6668713688850403  val_accuracy: 56.875 %\n",
      "iteration: 956  val_loss: 0.6668195724487305  val_accuracy: 56.875 %\n",
      "iteration: 957  val_loss: 0.6290520429611206  val_accuracy: 56.875 %\n",
      "iteration: 958  val_loss: 0.6665917634963989  val_accuracy: 56.875 %\n",
      "iteration: 959  val_loss: 0.743112325668335  val_accuracy: 56.875 %\n",
      "iteration: 960  val_loss: 0.7422948479652405  val_accuracy: 56.875 %\n",
      "iteration: 961  val_loss: 0.6669492125511169  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 962  val_loss: 0.6544308066368103  val_accuracy: 56.875 %\n",
      "iteration: 963  val_loss: 0.6668021082878113  val_accuracy: 56.875 %\n",
      "iteration: 964  val_loss: 0.7045101523399353  val_accuracy: 56.875 %\n",
      "iteration: 965  val_loss: 0.6292579770088196  val_accuracy: 56.875 %\n",
      "iteration: 966  val_loss: 0.666649580001831  val_accuracy: 56.875 %\n",
      "iteration: 967  val_loss: 0.6284604668617249  val_accuracy: 56.875 %\n",
      "iteration: 968  val_loss: 0.666426956653595  val_accuracy: 56.875 %\n",
      "iteration: 969  val_loss: 0.666379451751709  val_accuracy: 56.875 %\n",
      "iteration: 970  val_loss: 0.7439860701560974  val_accuracy: 56.875 %\n",
      "iteration: 971  val_loss: 0.6665312051773071  val_accuracy: 56.875 %\n",
      "iteration: 972  val_loss: 0.7433589696884155  val_accuracy: 56.875 %\n",
      "iteration: 973  val_loss: 0.6287562847137451  val_accuracy: 56.875 %\n",
      "iteration: 974  val_loss: 0.6665090918540955  val_accuracy: 56.875 %\n",
      "iteration: 975  val_loss: 0.7819441556930542  val_accuracy: 56.875 %\n",
      "iteration: 976  val_loss: 0.7044546604156494  val_accuracy: 56.875 %\n",
      "iteration: 977  val_loss: 0.7043415307998657  val_accuracy: 56.875 %\n",
      "iteration: 978  val_loss: 0.741518497467041  val_accuracy: 56.875 %\n",
      "iteration: 979  val_loss: 0.7039334774017334  val_accuracy: 56.875 %\n",
      "iteration: 980  val_loss: 0.7770266532897949  val_accuracy: 56.875 %\n",
      "iteration: 981  val_loss: 0.7749272584915161  val_accuracy: 56.875 %\n",
      "iteration: 982  val_loss: 0.6679263710975647  val_accuracy: 56.875 %\n",
      "iteration: 983  val_loss: 0.7380942702293396  val_accuracy: 56.875 %\n",
      "iteration: 984  val_loss: 0.5988349914550781  val_accuracy: 56.875 %\n",
      "iteration: 985  val_loss: 0.7739272713661194  val_accuracy: 56.875 %\n",
      "iteration: 986  val_loss: 0.633511483669281  val_accuracy: 56.875 %\n",
      "iteration: 987  val_loss: 0.7029399871826172  val_accuracy: 56.875 %\n",
      "iteration: 988  val_loss: 0.6679736375808716  val_accuracy: 56.875 %\n",
      "iteration: 989  val_loss: 0.667911171913147  val_accuracy: 56.875 %\n",
      "iteration: 990  val_loss: 0.6678494215011597  val_accuracy: 56.875 %\n",
      "iteration: 991  val_loss: 0.7383642196655273  val_accuracy: 56.875 %\n",
      "iteration: 992  val_loss: 0.6680061221122742  val_accuracy: 56.875 %\n",
      "iteration: 993  val_loss: 0.6330064535140991  val_accuracy: 56.875 %\n",
      "iteration: 994  val_loss: 0.7739186882972717  val_accuracy: 56.875 %\n",
      "iteration: 995  val_loss: 0.6335163712501526  val_accuracy: 56.875 %\n",
      "iteration: 996  val_loss: 0.6328561902046204  val_accuracy: 56.875 %\n",
      "iteration: 997  val_loss: 0.7386887073516846  val_accuracy: 56.875 %\n",
      "iteration: 998  val_loss: 0.6679142117500305  val_accuracy: 56.875 %\n",
      "iteration: 999  val_loss: 0.7381370663642883  val_accuracy: 56.875 %\n",
      "iteration: 1000  val_loss: 0.6334195137023926  val_accuracy: 56.875 %\n",
      "iteration: 1001  val_loss: 0.6327599883079529  val_accuracy: 56.875 %\n",
      "iteration: 1002  val_loss: 0.6321064233779907  val_accuracy: 56.875 %\n",
      "iteration: 1003  val_loss: 0.6674733757972717  val_accuracy: 56.875 %\n",
      "iteration: 1004  val_loss: 0.5589672923088074  val_accuracy: 56.875 %\n",
      "iteration: 1005  val_loss: 0.7041958570480347  val_accuracy: 56.875 %\n",
      "iteration: 1006  val_loss: 0.6299968957901001  val_accuracy: 56.875 %\n",
      "iteration: 1007  val_loss: 0.7043497562408447  val_accuracy: 56.875 %\n",
      "iteration: 1008  val_loss: 0.7042378187179565  val_accuracy: 56.875 %\n",
      "iteration: 1009  val_loss: 0.7041270732879639  val_accuracy: 56.875 %\n",
      "iteration: 1010  val_loss: 0.6670889854431152  val_accuracy: 56.875 %\n",
      "iteration: 1011  val_loss: 0.7040942311286926  val_accuracy: 56.875 %\n",
      "iteration: 1012  val_loss: 0.667111873626709  val_accuracy: 56.875 %\n",
      "iteration: 1013  val_loss: 0.630053699016571  val_accuracy: 56.875 %\n",
      "iteration: 1014  val_loss: 0.6294245719909668  val_accuracy: 56.875 %\n",
      "iteration: 1015  val_loss: 0.6666964888572693  val_accuracy: 56.875 %\n",
      "iteration: 1016  val_loss: 0.7046680450439453  val_accuracy: 56.875 %\n",
      "iteration: 1017  val_loss: 0.7045528888702393  val_accuracy: 56.875 %\n",
      "iteration: 1018  val_loss: 0.6667987108230591  val_accuracy: 56.875 %\n",
      "iteration: 1019  val_loss: 0.6289799809455872  val_accuracy: 56.875 %\n",
      "iteration: 1020  val_loss: 0.6665714383125305  val_accuracy: 56.875 %\n",
      "iteration: 1021  val_loss: 0.5898509621620178  val_accuracy: 56.875 %\n",
      "iteration: 1022  val_loss: 0.6662302613258362  val_accuracy: 56.875 %\n",
      "iteration: 1023  val_loss: 0.5877546668052673  val_accuracy: 56.875 %\n",
      "iteration: 1024  val_loss: 0.625933051109314  val_accuracy: 56.875 %\n",
      "iteration: 1025  val_loss: 0.6253406405448914  val_accuracy: 56.875 %\n",
      "iteration: 1026  val_loss: 0.6247537136077881  val_accuracy: 56.875 %\n",
      "iteration: 1027  val_loss: 0.665441632270813  val_accuracy: 56.875 %\n",
      "iteration: 1028  val_loss: 0.7067835330963135  val_accuracy: 56.875 %\n",
      "iteration: 1029  val_loss: 0.7478170394897461  val_accuracy: 56.875 %\n",
      "iteration: 1030  val_loss: 0.6656610369682312  val_accuracy: 56.875 %\n",
      "iteration: 1031  val_loss: 0.7063775062561035  val_accuracy: 56.875 %\n",
      "iteration: 1032  val_loss: 0.7873457074165344  val_accuracy: 56.875 %\n",
      "iteration: 1033  val_loss: 0.6660000085830688  val_accuracy: 56.875 %\n",
      "iteration: 1034  val_loss: 0.6261301636695862  val_accuracy: 56.875 %\n",
      "iteration: 1035  val_loss: 0.7463213205337524  val_accuracy: 56.875 %\n",
      "iteration: 1036  val_loss: 0.6659883856773376  val_accuracy: 56.875 %\n",
      "iteration: 1037  val_loss: 0.7456604242324829  val_accuracy: 56.875 %\n",
      "iteration: 1038  val_loss: 0.7841535806655884  val_accuracy: 56.875 %\n",
      "iteration: 1039  val_loss: 0.7434666156768799  val_accuracy: 56.875 %\n",
      "iteration: 1040  val_loss: 0.7426453828811646  val_accuracy: 56.875 %\n",
      "iteration: 1041  val_loss: 0.6293746829032898  val_accuracy: 56.875 %\n",
      "iteration: 1042  val_loss: 0.6287515163421631  val_accuracy: 56.875 %\n",
      "iteration: 1043  val_loss: 0.6665077805519104  val_accuracy: 56.875 %\n",
      "iteration: 1044  val_loss: 0.7434547543525696  val_accuracy: 56.875 %\n",
      "iteration: 1045  val_loss: 0.6666601896286011  val_accuracy: 56.875 %\n",
      "iteration: 1046  val_loss: 0.6666104197502136  val_accuracy: 56.875 %\n",
      "iteration: 1047  val_loss: 0.6665610671043396  val_accuracy: 56.875 %\n",
      "iteration: 1048  val_loss: 0.6665123105049133  val_accuracy: 56.875 %\n",
      "iteration: 1049  val_loss: 0.7049502730369568  val_accuracy: 56.875 %\n",
      "iteration: 1050  val_loss: 0.6665396690368652  val_accuracy: 56.875 %\n",
      "iteration: 1051  val_loss: 0.7817413806915283  val_accuracy: 56.875 %\n",
      "iteration: 1052  val_loss: 0.6292296051979065  val_accuracy: 56.875 %\n",
      "iteration: 1053  val_loss: 0.8187767267227173  val_accuracy: 56.875 %\n",
      "iteration: 1054  val_loss: 0.7039963603019714  val_accuracy: 56.875 %\n",
      "iteration: 1055  val_loss: 0.6304734945297241  val_accuracy: 56.875 %\n",
      "iteration: 1056  val_loss: 0.7413058876991272  val_accuracy: 56.875 %\n",
      "iteration: 1057  val_loss: 0.77716064453125  val_accuracy: 56.875 %\n",
      "iteration: 1058  val_loss: 0.7033864855766296  val_accuracy: 56.875 %\n",
      "iteration: 1059  val_loss: 0.6319698691368103  val_accuracy: 56.875 %\n",
      "iteration: 1060  val_loss: 0.6313236355781555  val_accuracy: 56.875 %\n",
      "iteration: 1061  val_loss: 0.7038020491600037  val_accuracy: 56.875 %\n",
      "iteration: 1062  val_loss: 0.6673197746276855  val_accuracy: 56.875 %\n",
      "iteration: 1063  val_loss: 0.7037729620933533  val_accuracy: 56.875 %\n",
      "iteration: 1064  val_loss: 0.8489713072776794  val_accuracy: 56.875 %\n",
      "iteration: 1065  val_loss: 0.807496190071106  val_accuracy: 56.875 %\n",
      "iteration: 1066  val_loss: 0.6684757471084595  val_accuracy: 56.875 %\n",
      "iteration: 1067  val_loss: 0.634499192237854  val_accuracy: 56.875 %\n",
      "iteration: 1068  val_loss: 0.7369353175163269  val_accuracy: 56.875 %\n",
      "iteration: 1069  val_loss: 0.6684210300445557  val_accuracy: 56.875 %\n",
      "iteration: 1070  val_loss: 0.7023814916610718  val_accuracy: 56.875 %\n",
      "iteration: 1071  val_loss: 0.7022897005081177  val_accuracy: 56.875 %\n",
      "iteration: 1072  val_loss: 0.6348230242729187  val_accuracy: 56.875 %\n",
      "iteration: 1073  val_loss: 0.7024468779563904  val_accuracy: 56.875 %\n",
      "iteration: 1074  val_loss: 0.6344001293182373  val_accuracy: 56.875 %\n",
      "iteration: 1075  val_loss: 0.6337321996688843  val_accuracy: 56.875 %\n",
      "iteration: 1076  val_loss: 0.5981773138046265  val_accuracy: 56.875 %\n",
      "iteration: 1077  val_loss: 0.7032876014709473  val_accuracy: 56.875 %\n",
      "iteration: 1078  val_loss: 0.6677023768424988  val_accuracy: 56.875 %\n",
      "iteration: 1079  val_loss: 0.6320209503173828  val_accuracy: 56.875 %\n",
      "iteration: 1080  val_loss: 0.5953006148338318  val_accuracy: 56.875 %\n",
      "iteration: 1081  val_loss: 0.6671257019042969  val_accuracy: 56.875 %\n",
      "iteration: 1082  val_loss: 0.5931298136711121  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1083  val_loss: 0.5912933945655823  val_accuracy: 56.875 %\n",
      "iteration: 1084  val_loss: 0.7049551010131836  val_accuracy: 56.875 %\n",
      "iteration: 1085  val_loss: 0.7048369646072388  val_accuracy: 56.875 %\n",
      "iteration: 1086  val_loss: 0.742827832698822  val_accuracy: 56.875 %\n",
      "iteration: 1087  val_loss: 0.6668151021003723  val_accuracy: 56.875 %\n",
      "iteration: 1088  val_loss: 0.7422175407409668  val_accuracy: 56.875 %\n",
      "iteration: 1089  val_loss: 0.815851628780365  val_accuracy: 56.875 %\n",
      "iteration: 1090  val_loss: 0.7035285830497742  val_accuracy: 56.875 %\n",
      "iteration: 1091  val_loss: 0.5957110524177551  val_accuracy: 56.875 %\n",
      "iteration: 1092  val_loss: 0.7038670778274536  val_accuracy: 56.875 %\n",
      "iteration: 1093  val_loss: 0.6672728657722473  val_accuracy: 56.875 %\n",
      "iteration: 1094  val_loss: 0.6672171950340271  val_accuracy: 56.875 %\n",
      "iteration: 1095  val_loss: 0.6304099559783936  val_accuracy: 56.875 %\n",
      "iteration: 1096  val_loss: 0.7041770219802856  val_accuracy: 56.875 %\n",
      "iteration: 1097  val_loss: 0.7040669322013855  val_accuracy: 56.875 %\n",
      "iteration: 1098  val_loss: 0.6671310067176819  val_accuracy: 56.875 %\n",
      "iteration: 1099  val_loss: 0.7409929037094116  val_accuracy: 56.875 %\n",
      "iteration: 1100  val_loss: 0.6308292746543884  val_accuracy: 56.875 %\n",
      "iteration: 1101  val_loss: 0.7409091591835022  val_accuracy: 56.875 %\n",
      "iteration: 1102  val_loss: 0.7765202522277832  val_accuracy: 56.875 %\n",
      "iteration: 1103  val_loss: 0.560884952545166  val_accuracy: 56.875 %\n",
      "iteration: 1104  val_loss: 0.6671948432922363  val_accuracy: 56.875 %\n",
      "iteration: 1105  val_loss: 0.6303346157073975  val_accuracy: 56.875 %\n",
      "iteration: 1106  val_loss: 0.7414610981941223  val_accuracy: 56.875 %\n",
      "iteration: 1107  val_loss: 0.6304138898849487  val_accuracy: 56.875 %\n",
      "iteration: 1108  val_loss: 0.7041754722595215  val_accuracy: 56.875 %\n",
      "iteration: 1109  val_loss: 0.6670553684234619  val_accuracy: 56.875 %\n",
      "iteration: 1110  val_loss: 0.6670017242431641  val_accuracy: 56.875 %\n",
      "iteration: 1111  val_loss: 0.7414888143539429  val_accuracy: 56.875 %\n",
      "iteration: 1112  val_loss: 0.6671560406684875  val_accuracy: 56.875 %\n",
      "iteration: 1113  val_loss: 0.6302033066749573  val_accuracy: 56.875 %\n",
      "iteration: 1114  val_loss: 0.6295729875564575  val_accuracy: 56.875 %\n",
      "iteration: 1115  val_loss: 0.7423189282417297  val_accuracy: 56.875 %\n",
      "iteration: 1116  val_loss: 0.6669430732727051  val_accuracy: 56.875 %\n",
      "iteration: 1117  val_loss: 0.6294777989387512  val_accuracy: 56.875 %\n",
      "iteration: 1118  val_loss: 0.7045692205429077  val_accuracy: 56.875 %\n",
      "iteration: 1119  val_loss: 0.7421222925186157  val_accuracy: 56.875 %\n",
      "iteration: 1120  val_loss: 0.6298316121101379  val_accuracy: 56.875 %\n",
      "iteration: 1121  val_loss: 0.6668118238449097  val_accuracy: 56.875 %\n",
      "iteration: 1122  val_loss: 0.7044956088066101  val_accuracy: 56.875 %\n",
      "iteration: 1123  val_loss: 0.6668369770050049  val_accuracy: 56.875 %\n",
      "iteration: 1124  val_loss: 0.7421314716339111  val_accuracy: 56.875 %\n",
      "iteration: 1125  val_loss: 0.6669906973838806  val_accuracy: 56.875 %\n",
      "iteration: 1126  val_loss: 0.7042344808578491  val_accuracy: 56.875 %\n",
      "iteration: 1127  val_loss: 0.7041237950325012  val_accuracy: 56.875 %\n",
      "iteration: 1128  val_loss: 0.7409372925758362  val_accuracy: 56.875 %\n",
      "iteration: 1129  val_loss: 0.7765657901763916  val_accuracy: 56.875 %\n",
      "iteration: 1130  val_loss: 0.7388644814491272  val_accuracy: 56.875 %\n",
      "iteration: 1131  val_loss: 0.7029792070388794  val_accuracy: 56.875 %\n",
      "iteration: 1132  val_loss: 0.7378201484680176  val_accuracy: 56.875 %\n",
      "iteration: 1133  val_loss: 0.6337125301361084  val_accuracy: 56.875 %\n",
      "iteration: 1134  val_loss: 0.737769365310669  val_accuracy: 56.875 %\n",
      "iteration: 1135  val_loss: 0.7025935053825378  val_accuracy: 56.875 %\n",
      "iteration: 1136  val_loss: 0.6682547926902771  val_accuracy: 56.875 %\n",
      "iteration: 1137  val_loss: 0.6338017582893372  val_accuracy: 56.875 %\n",
      "iteration: 1138  val_loss: 0.6331391334533691  val_accuracy: 56.875 %\n",
      "iteration: 1139  val_loss: 0.5971823930740356  val_accuracy: 56.875 %\n",
      "iteration: 1140  val_loss: 0.7035188674926758  val_accuracy: 56.875 %\n",
      "iteration: 1141  val_loss: 0.7751913666725159  val_accuracy: 56.875 %\n",
      "iteration: 1142  val_loss: 0.632803201675415  val_accuracy: 56.875 %\n",
      "iteration: 1143  val_loss: 0.7032134532928467  val_accuracy: 56.875 %\n",
      "iteration: 1144  val_loss: 0.7384672164916992  val_accuracy: 56.875 %\n",
      "iteration: 1145  val_loss: 0.6679766178131104  val_accuracy: 56.875 %\n",
      "iteration: 1146  val_loss: 0.5979095697402954  val_accuracy: 56.875 %\n",
      "iteration: 1147  val_loss: 0.7748932242393494  val_accuracy: 56.875 %\n",
      "iteration: 1148  val_loss: 0.702894389629364  val_accuracy: 56.875 %\n",
      "iteration: 1149  val_loss: 0.7027974724769592  val_accuracy: 56.875 %\n",
      "iteration: 1150  val_loss: 0.668087899684906  val_accuracy: 56.875 %\n",
      "iteration: 1151  val_loss: 0.7375345230102539  val_accuracy: 56.875 %\n",
      "iteration: 1152  val_loss: 0.6339782476425171  val_accuracy: 56.875 %\n",
      "iteration: 1153  val_loss: 0.6680382490158081  val_accuracy: 56.875 %\n",
      "iteration: 1154  val_loss: 0.7377054691314697  val_accuracy: 56.875 %\n",
      "iteration: 1155  val_loss: 0.7025712132453918  val_accuracy: 56.875 %\n",
      "iteration: 1156  val_loss: 0.6682732701301575  val_accuracy: 56.875 %\n",
      "iteration: 1157  val_loss: 0.6338604688644409  val_accuracy: 56.875 %\n",
      "iteration: 1158  val_loss: 0.633197546005249  val_accuracy: 56.875 %\n",
      "iteration: 1159  val_loss: 0.6678005456924438  val_accuracy: 56.875 %\n",
      "iteration: 1160  val_loss: 0.632341206073761  val_accuracy: 56.875 %\n",
      "iteration: 1161  val_loss: 0.739246129989624  val_accuracy: 56.875 %\n",
      "iteration: 1162  val_loss: 0.6677581667900085  val_accuracy: 56.875 %\n",
      "iteration: 1163  val_loss: 0.6676978468894958  val_accuracy: 56.875 %\n",
      "iteration: 1164  val_loss: 0.6676381230354309  val_accuracy: 56.875 %\n",
      "iteration: 1165  val_loss: 0.7391162514686584  val_accuracy: 56.875 %\n",
      "iteration: 1166  val_loss: 0.5972455739974976  val_accuracy: 56.875 %\n",
      "iteration: 1167  val_loss: 0.6674611568450928  val_accuracy: 56.875 %\n",
      "iteration: 1168  val_loss: 0.7035814523696899  val_accuracy: 56.875 %\n",
      "iteration: 1169  val_loss: 0.7034773826599121  val_accuracy: 56.875 %\n",
      "iteration: 1170  val_loss: 0.7391904592514038  val_accuracy: 56.875 %\n",
      "iteration: 1171  val_loss: 0.7030948996543884  val_accuracy: 56.875 %\n",
      "iteration: 1172  val_loss: 0.632706880569458  val_accuracy: 56.875 %\n",
      "iteration: 1173  val_loss: 0.6676527261734009  val_accuracy: 56.875 %\n",
      "iteration: 1174  val_loss: 0.6675933599472046  val_accuracy: 56.875 %\n",
      "iteration: 1175  val_loss: 0.7034062147140503  val_accuracy: 56.875 %\n",
      "iteration: 1176  val_loss: 0.6319204568862915  val_accuracy: 56.875 %\n",
      "iteration: 1177  val_loss: 0.7035617828369141  val_accuracy: 56.875 %\n",
      "iteration: 1178  val_loss: 0.667495608329773  val_accuracy: 56.875 %\n",
      "iteration: 1179  val_loss: 0.6313403844833374  val_accuracy: 56.875 %\n",
      "iteration: 1180  val_loss: 0.6672475337982178  val_accuracy: 56.875 %\n",
      "iteration: 1181  val_loss: 0.6671922206878662  val_accuracy: 56.875 %\n",
      "iteration: 1182  val_loss: 0.6671372652053833  val_accuracy: 56.875 %\n",
      "iteration: 1183  val_loss: 0.7040259838104248  val_accuracy: 56.875 %\n",
      "iteration: 1184  val_loss: 0.7406750321388245  val_accuracy: 56.875 %\n",
      "iteration: 1185  val_loss: 0.739884614944458  val_accuracy: 56.875 %\n",
      "iteration: 1186  val_loss: 0.6318225264549255  val_accuracy: 56.875 %\n",
      "iteration: 1187  val_loss: 0.7036009430885315  val_accuracy: 56.875 %\n",
      "iteration: 1188  val_loss: 0.7034968137741089  val_accuracy: 56.875 %\n",
      "iteration: 1189  val_loss: 0.6675442457199097  val_accuracy: 56.875 %\n",
      "iteration: 1190  val_loss: 0.6315007209777832  val_accuracy: 56.875 %\n",
      "iteration: 1191  val_loss: 0.6672945618629456  val_accuracy: 56.875 %\n",
      "iteration: 1192  val_loss: 0.6672387719154358  val_accuracy: 56.875 %\n",
      "iteration: 1193  val_loss: 0.667183518409729  val_accuracy: 56.875 %\n",
      "iteration: 1194  val_loss: 0.6302959322929382  val_accuracy: 56.875 %\n",
      "iteration: 1195  val_loss: 0.666944682598114  val_accuracy: 56.875 %\n",
      "iteration: 1196  val_loss: 0.7417097687721252  val_accuracy: 56.875 %\n",
      "iteration: 1197  val_loss: 0.6301940679550171  val_accuracy: 56.875 %\n",
      "iteration: 1198  val_loss: 0.5548607707023621  val_accuracy: 56.875 %\n",
      "iteration: 1199  val_loss: 0.7433453798294067  val_accuracy: 56.875 %\n",
      "iteration: 1200  val_loss: 0.6287676095962524  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1201  val_loss: 0.6281501054763794  val_accuracy: 56.875 %\n",
      "iteration: 1202  val_loss: 0.6663414835929871  val_accuracy: 56.875 %\n",
      "iteration: 1203  val_loss: 0.6662947535514832  val_accuracy: 56.875 %\n",
      "iteration: 1204  val_loss: 0.7833876013755798  val_accuracy: 56.875 %\n",
      "iteration: 1205  val_loss: 0.704785168170929  val_accuracy: 56.875 %\n",
      "iteration: 1206  val_loss: 0.7046688199043274  val_accuracy: 56.875 %\n",
      "iteration: 1207  val_loss: 0.6288902163505554  val_accuracy: 56.875 %\n",
      "iteration: 1208  val_loss: 0.7048214077949524  val_accuracy: 56.875 %\n",
      "iteration: 1209  val_loss: 0.5904577970504761  val_accuracy: 56.875 %\n",
      "iteration: 1210  val_loss: 0.7051681876182556  val_accuracy: 56.875 %\n",
      "iteration: 1211  val_loss: 0.7050479054450989  val_accuracy: 56.875 %\n",
      "iteration: 1212  val_loss: 0.6280263662338257  val_accuracy: 56.875 %\n",
      "iteration: 1213  val_loss: 0.6274155974388123  val_accuracy: 56.875 %\n",
      "iteration: 1214  val_loss: 0.7448024749755859  val_accuracy: 56.875 %\n",
      "iteration: 1215  val_loss: 0.7051519751548767  val_accuracy: 56.875 %\n",
      "iteration: 1216  val_loss: 0.6664121747016907  val_accuracy: 56.875 %\n",
      "iteration: 1217  val_loss: 0.7051071524620056  val_accuracy: 56.875 %\n",
      "iteration: 1218  val_loss: 0.5893457531929016  val_accuracy: 56.875 %\n",
      "iteration: 1219  val_loss: 0.7054555416107178  val_accuracy: 56.875 %\n",
      "iteration: 1220  val_loss: 0.6271188855171204  val_accuracy: 56.875 %\n",
      "iteration: 1221  val_loss: 0.7056055665016174  val_accuracy: 56.875 %\n",
      "iteration: 1222  val_loss: 0.7054808139801025  val_accuracy: 56.875 %\n",
      "iteration: 1223  val_loss: 0.6662102937698364  val_accuracy: 56.875 %\n",
      "iteration: 1224  val_loss: 0.7446994185447693  val_accuracy: 56.875 %\n",
      "iteration: 1225  val_loss: 0.8213688731193542  val_accuracy: 56.875 %\n",
      "iteration: 1226  val_loss: 0.6292067170143127  val_accuracy: 56.875 %\n",
      "iteration: 1227  val_loss: 0.6666352152824402  val_accuracy: 56.875 %\n",
      "iteration: 1228  val_loss: 0.7047609686851501  val_accuracy: 56.875 %\n",
      "iteration: 1229  val_loss: 0.6286783814430237  val_accuracy: 56.875 %\n",
      "iteration: 1230  val_loss: 0.7433390617370605  val_accuracy: 56.875 %\n",
      "iteration: 1231  val_loss: 0.6666885614395142  val_accuracy: 56.875 %\n",
      "iteration: 1232  val_loss: 0.6666386127471924  val_accuracy: 56.875 %\n",
      "iteration: 1233  val_loss: 0.7047559022903442  val_accuracy: 56.875 %\n",
      "iteration: 1234  val_loss: 0.7046399116516113  val_accuracy: 56.875 %\n",
      "iteration: 1235  val_loss: 0.6289569139480591  val_accuracy: 56.875 %\n",
      "iteration: 1236  val_loss: 0.7047925591468811  val_accuracy: 56.875 %\n",
      "iteration: 1237  val_loss: 0.6286060214042664  val_accuracy: 56.875 %\n",
      "iteration: 1238  val_loss: 0.6279901266098022  val_accuracy: 56.875 %\n",
      "iteration: 1239  val_loss: 0.705215334892273  val_accuracy: 56.875 %\n",
      "iteration: 1240  val_loss: 0.6663727760314941  val_accuracy: 56.875 %\n",
      "iteration: 1241  val_loss: 0.6274815201759338  val_accuracy: 56.875 %\n",
      "iteration: 1242  val_loss: 0.6268756985664368  val_accuracy: 56.875 %\n",
      "iteration: 1243  val_loss: 0.7057161331176758  val_accuracy: 56.875 %\n",
      "iteration: 1244  val_loss: 0.7055901885032654  val_accuracy: 56.875 %\n",
      "iteration: 1245  val_loss: 0.6268237829208374  val_accuracy: 56.875 %\n",
      "iteration: 1246  val_loss: 0.7057397961616516  val_accuracy: 56.875 %\n",
      "iteration: 1247  val_loss: 0.666056215763092  val_accuracy: 56.875 %\n",
      "iteration: 1248  val_loss: 0.6263364553451538  val_accuracy: 56.875 %\n",
      "iteration: 1249  val_loss: 0.6658523082733154  val_accuracy: 56.875 %\n",
      "iteration: 1250  val_loss: 0.6658103466033936  val_accuracy: 56.875 %\n",
      "iteration: 1251  val_loss: 0.6657688021659851  val_accuracy: 56.875 %\n",
      "iteration: 1252  val_loss: 0.7466415166854858  val_accuracy: 56.875 %\n",
      "iteration: 1253  val_loss: 0.6659169793128967  val_accuracy: 56.875 %\n",
      "iteration: 1254  val_loss: 0.625823438167572  val_accuracy: 56.875 %\n",
      "iteration: 1255  val_loss: 0.6252323389053345  val_accuracy: 56.875 %\n",
      "iteration: 1256  val_loss: 0.6655641794204712  val_accuracy: 56.875 %\n",
      "iteration: 1257  val_loss: 0.7065544724464417  val_accuracy: 56.875 %\n",
      "iteration: 1258  val_loss: 0.6655977964401245  val_accuracy: 56.875 %\n",
      "iteration: 1259  val_loss: 0.6655583381652832  val_accuracy: 56.875 %\n",
      "iteration: 1260  val_loss: 0.7065654397010803  val_accuracy: 56.875 %\n",
      "iteration: 1261  val_loss: 0.7064309120178223  val_accuracy: 56.875 %\n",
      "iteration: 1262  val_loss: 0.7062974572181702  val_accuracy: 56.875 %\n",
      "iteration: 1263  val_loss: 0.7465927600860596  val_accuracy: 56.875 %\n",
      "iteration: 1264  val_loss: 0.6659277081489563  val_accuracy: 56.875 %\n",
      "iteration: 1265  val_loss: 0.7459280490875244  val_accuracy: 56.875 %\n",
      "iteration: 1266  val_loss: 0.7055785655975342  val_accuracy: 56.875 %\n",
      "iteration: 1267  val_loss: 0.7054539918899536  val_accuracy: 56.875 %\n",
      "iteration: 1268  val_loss: 0.7053308486938477  val_accuracy: 56.875 %\n",
      "iteration: 1269  val_loss: 0.6663014888763428  val_accuracy: 56.875 %\n",
      "iteration: 1270  val_loss: 0.7443127632141113  val_accuracy: 56.875 %\n",
      "iteration: 1271  val_loss: 0.5509080290794373  val_accuracy: 56.875 %\n",
      "iteration: 1272  val_loss: 0.6660441756248474  val_accuracy: 56.875 %\n",
      "iteration: 1273  val_loss: 0.7057083249092102  val_accuracy: 56.875 %\n",
      "iteration: 1274  val_loss: 0.6660746335983276  val_accuracy: 56.875 %\n",
      "iteration: 1275  val_loss: 0.6264042258262634  val_accuracy: 56.875 %\n",
      "iteration: 1276  val_loss: 0.665870189666748  val_accuracy: 56.875 %\n",
      "iteration: 1277  val_loss: 0.7060064077377319  val_accuracy: 56.875 %\n",
      "iteration: 1278  val_loss: 0.6659018993377686  val_accuracy: 56.875 %\n",
      "iteration: 1279  val_loss: 0.6658594608306885  val_accuracy: 56.875 %\n",
      "iteration: 1280  val_loss: 0.7060249447822571  val_accuracy: 56.875 %\n",
      "iteration: 1281  val_loss: 0.6658912897109985  val_accuracy: 56.875 %\n",
      "iteration: 1282  val_loss: 0.7059696912765503  val_accuracy: 56.875 %\n",
      "iteration: 1283  val_loss: 0.6659228205680847  val_accuracy: 56.875 %\n",
      "iteration: 1284  val_loss: 0.6658802032470703  val_accuracy: 56.875 %\n",
      "iteration: 1285  val_loss: 0.7059889435768127  val_accuracy: 56.875 %\n",
      "iteration: 1286  val_loss: 0.7458086013793945  val_accuracy: 56.875 %\n",
      "iteration: 1287  val_loss: 0.6266751885414124  val_accuracy: 56.875 %\n",
      "iteration: 1288  val_loss: 0.6659421324729919  val_accuracy: 56.875 %\n",
      "iteration: 1289  val_loss: 0.6658993363380432  val_accuracy: 56.875 %\n",
      "iteration: 1290  val_loss: 0.7059557437896729  val_accuracy: 56.875 %\n",
      "iteration: 1291  val_loss: 0.6659308671951294  val_accuracy: 56.875 %\n",
      "iteration: 1292  val_loss: 0.6658880710601807  val_accuracy: 56.875 %\n",
      "iteration: 1293  val_loss: 0.6257164478302002  val_accuracy: 56.875 %\n",
      "iteration: 1294  val_loss: 0.6656896471977234  val_accuracy: 56.875 %\n",
      "iteration: 1295  val_loss: 0.5842957496643066  val_accuracy: 56.875 %\n",
      "iteration: 1296  val_loss: 0.7068145275115967  val_accuracy: 56.875 %\n",
      "iteration: 1297  val_loss: 0.7478948831558228  val_accuracy: 56.875 %\n",
      "iteration: 1298  val_loss: 0.6249539256095886  val_accuracy: 56.875 %\n",
      "iteration: 1299  val_loss: 0.6654927730560303  val_accuracy: 56.875 %\n",
      "iteration: 1300  val_loss: 0.5829882025718689  val_accuracy: 56.875 %\n",
      "iteration: 1301  val_loss: 0.6651995182037354  val_accuracy: 56.875 %\n",
      "iteration: 1302  val_loss: 0.791429340839386  val_accuracy: 56.875 %\n",
      "iteration: 1303  val_loss: 0.6654529571533203  val_accuracy: 56.875 %\n",
      "iteration: 1304  val_loss: 0.6654148101806641  val_accuracy: 56.875 %\n",
      "iteration: 1305  val_loss: 0.7068340182304382  val_accuracy: 56.875 %\n",
      "iteration: 1306  val_loss: 0.7066965103149414  val_accuracy: 56.875 %\n",
      "iteration: 1307  val_loss: 0.7065604329109192  val_accuracy: 56.875 %\n",
      "iteration: 1308  val_loss: 0.6247634887695312  val_accuracy: 56.875 %\n",
      "iteration: 1309  val_loss: 0.6654441356658936  val_accuracy: 56.875 %\n",
      "iteration: 1310  val_loss: 0.5412886142730713  val_accuracy: 56.875 %\n",
      "iteration: 1311  val_loss: 0.6226077079772949  val_accuracy: 56.875 %\n",
      "iteration: 1312  val_loss: 0.664910078048706  val_accuracy: 56.875 %\n",
      "iteration: 1313  val_loss: 0.8367466926574707  val_accuracy: 56.875 %\n",
      "iteration: 1314  val_loss: 0.6652638912200928  val_accuracy: 56.875 %\n",
      "iteration: 1315  val_loss: 0.6652276515960693  val_accuracy: 56.875 %\n",
      "iteration: 1316  val_loss: 0.7912055253982544  val_accuracy: 56.875 %\n",
      "iteration: 1317  val_loss: 0.5831751823425293  val_accuracy: 56.875 %\n",
      "iteration: 1318  val_loss: 0.7909324765205383  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1319  val_loss: 0.6655173897743225  val_accuracy: 56.875 %\n",
      "iteration: 1320  val_loss: 0.7478042840957642  val_accuracy: 56.875 %\n",
      "iteration: 1321  val_loss: 0.7469357252120972  val_accuracy: 56.875 %\n",
      "iteration: 1322  val_loss: 0.6658519506454468  val_accuracy: 56.875 %\n",
      "iteration: 1323  val_loss: 0.6255819201469421  val_accuracy: 56.875 %\n",
      "iteration: 1324  val_loss: 0.6249931454658508  val_accuracy: 56.875 %\n",
      "iteration: 1325  val_loss: 0.6655027866363525  val_accuracy: 56.875 %\n",
      "iteration: 1326  val_loss: 0.6654641032218933  val_accuracy: 56.875 %\n",
      "iteration: 1327  val_loss: 0.7067407965660095  val_accuracy: 56.875 %\n",
      "iteration: 1328  val_loss: 0.7477101683616638  val_accuracy: 56.875 %\n",
      "iteration: 1329  val_loss: 0.6251043081283569  val_accuracy: 56.875 %\n",
      "iteration: 1330  val_loss: 0.7885657548904419  val_accuracy: 56.875 %\n",
      "iteration: 1331  val_loss: 0.7461663484573364  val_accuracy: 56.875 %\n",
      "iteration: 1332  val_loss: 0.7056694626808167  val_accuracy: 56.875 %\n",
      "iteration: 1333  val_loss: 0.7055440545082092  val_accuracy: 56.875 %\n",
      "iteration: 1334  val_loss: 0.6269246339797974  val_accuracy: 56.875 %\n",
      "iteration: 1335  val_loss: 0.6660087704658508  val_accuracy: 56.875 %\n",
      "iteration: 1336  val_loss: 0.7455707788467407  val_accuracy: 56.875 %\n",
      "iteration: 1337  val_loss: 0.78401118516922  val_accuracy: 56.875 %\n",
      "iteration: 1338  val_loss: 0.6664775013923645  val_accuracy: 56.875 %\n",
      "iteration: 1339  val_loss: 0.7821547389030457  val_accuracy: 56.875 %\n",
      "iteration: 1340  val_loss: 0.7045029997825623  val_accuracy: 56.875 %\n",
      "iteration: 1341  val_loss: 0.7043895721435547  val_accuracy: 56.875 %\n",
      "iteration: 1342  val_loss: 0.666908323764801  val_accuracy: 56.875 %\n",
      "iteration: 1343  val_loss: 0.7793487906455994  val_accuracy: 56.875 %\n",
      "iteration: 1344  val_loss: 0.6305165886878967  val_accuracy: 56.875 %\n",
      "iteration: 1345  val_loss: 0.7412573099136353  val_accuracy: 56.875 %\n",
      "iteration: 1346  val_loss: 0.6305940747261047  val_accuracy: 56.875 %\n",
      "iteration: 1347  val_loss: 0.7041007280349731  val_accuracy: 56.875 %\n",
      "iteration: 1348  val_loss: 0.6302233338356018  val_accuracy: 56.875 %\n",
      "iteration: 1349  val_loss: 0.704254686832428  val_accuracy: 56.875 %\n",
      "iteration: 1350  val_loss: 0.7041438817977905  val_accuracy: 56.875 %\n",
      "iteration: 1351  val_loss: 0.704034149646759  val_accuracy: 56.875 %\n",
      "iteration: 1352  val_loss: 0.7406971454620361  val_accuracy: 56.875 %\n",
      "iteration: 1353  val_loss: 0.6673640012741089  val_accuracy: 56.875 %\n",
      "iteration: 1354  val_loss: 0.6309025287628174  val_accuracy: 56.875 %\n",
      "iteration: 1355  val_loss: 0.6671199202537537  val_accuracy: 56.875 %\n",
      "iteration: 1356  val_loss: 0.7040504217147827  val_accuracy: 56.875 %\n",
      "iteration: 1357  val_loss: 0.6671426296234131  val_accuracy: 56.875 %\n",
      "iteration: 1358  val_loss: 0.6301578879356384  val_accuracy: 56.875 %\n",
      "iteration: 1359  val_loss: 0.7790359258651733  val_accuracy: 56.875 %\n",
      "iteration: 1360  val_loss: 0.7769150733947754  val_accuracy: 56.875 %\n",
      "iteration: 1361  val_loss: 0.7033329606056213  val_accuracy: 56.875 %\n",
      "iteration: 1362  val_loss: 0.6676676869392395  val_accuracy: 56.875 %\n",
      "iteration: 1363  val_loss: 0.6676081418991089  val_accuracy: 56.875 %\n",
      "iteration: 1364  val_loss: 0.739223837852478  val_accuracy: 56.875 %\n",
      "iteration: 1365  val_loss: 0.6677642464637756  val_accuracy: 56.875 %\n",
      "iteration: 1366  val_loss: 0.5967427492141724  val_accuracy: 56.875 %\n",
      "iteration: 1367  val_loss: 0.7398701906204224  val_accuracy: 56.875 %\n",
      "iteration: 1368  val_loss: 0.6318353414535522  val_accuracy: 56.875 %\n",
      "iteration: 1369  val_loss: 0.6673932671546936  val_accuracy: 56.875 %\n",
      "iteration: 1370  val_loss: 0.7400094270706177  val_accuracy: 56.875 %\n",
      "iteration: 1371  val_loss: 0.7392265796661377  val_accuracy: 56.875 %\n",
      "iteration: 1372  val_loss: 0.5970747470855713  val_accuracy: 56.875 %\n",
      "iteration: 1373  val_loss: 0.6674314737319946  val_accuracy: 56.875 %\n",
      "iteration: 1374  val_loss: 0.6311275959014893  val_accuracy: 56.875 %\n",
      "iteration: 1375  val_loss: 0.7772738337516785  val_accuracy: 56.875 %\n",
      "iteration: 1376  val_loss: 0.5957688093185425  val_accuracy: 56.875 %\n",
      "iteration: 1377  val_loss: 0.667205810546875  val_accuracy: 56.875 %\n",
      "iteration: 1378  val_loss: 0.7407090067863464  val_accuracy: 56.875 %\n",
      "iteration: 1379  val_loss: 0.7761974930763245  val_accuracy: 56.875 %\n",
      "iteration: 1380  val_loss: 0.7386429309844971  val_accuracy: 56.875 %\n",
      "iteration: 1381  val_loss: 0.5979785919189453  val_accuracy: 56.875 %\n",
      "iteration: 1382  val_loss: 0.6675899028778076  val_accuracy: 56.875 %\n",
      "iteration: 1383  val_loss: 0.6675311326980591  val_accuracy: 56.875 %\n",
      "iteration: 1384  val_loss: 0.6674731373786926  val_accuracy: 56.875 %\n",
      "iteration: 1385  val_loss: 0.7035654783248901  val_accuracy: 56.875 %\n",
      "iteration: 1386  val_loss: 0.6315242052078247  val_accuracy: 56.875 %\n",
      "iteration: 1387  val_loss: 0.6673015356063843  val_accuracy: 56.875 %\n",
      "iteration: 1388  val_loss: 0.667245626449585  val_accuracy: 56.875 %\n",
      "iteration: 1389  val_loss: 0.6671902537345886  val_accuracy: 56.875 %\n",
      "iteration: 1390  val_loss: 0.5935030579566956  val_accuracy: 56.875 %\n",
      "iteration: 1391  val_loss: 0.704402506351471  val_accuracy: 56.875 %\n",
      "iteration: 1392  val_loss: 0.7416806221008301  val_accuracy: 56.875 %\n",
      "iteration: 1393  val_loss: 0.6671061515808105  val_accuracy: 56.875 %\n",
      "iteration: 1394  val_loss: 0.6670522093772888  val_accuracy: 56.875 %\n",
      "iteration: 1395  val_loss: 0.6669987440109253  val_accuracy: 56.875 %\n",
      "iteration: 1396  val_loss: 0.66694575548172  val_accuracy: 56.875 %\n",
      "iteration: 1397  val_loss: 0.7042994499206543  val_accuracy: 56.875 %\n",
      "iteration: 1398  val_loss: 0.7041881084442139  val_accuracy: 56.875 %\n",
      "iteration: 1399  val_loss: 0.8151726722717285  val_accuracy: 56.875 %\n",
      "iteration: 1400  val_loss: 0.7393202185630798  val_accuracy: 56.875 %\n",
      "iteration: 1401  val_loss: 0.738544762134552  val_accuracy: 56.875 %\n",
      "iteration: 1402  val_loss: 0.6330429315567017  val_accuracy: 56.875 %\n",
      "iteration: 1403  val_loss: 0.6677538156509399  val_accuracy: 56.875 %\n",
      "iteration: 1404  val_loss: 0.6676936149597168  val_accuracy: 56.875 %\n",
      "iteration: 1405  val_loss: 0.7032755017280579  val_accuracy: 56.875 %\n",
      "iteration: 1406  val_loss: 0.5967855453491211  val_accuracy: 56.875 %\n",
      "iteration: 1407  val_loss: 0.7036118507385254  val_accuracy: 56.875 %\n",
      "iteration: 1408  val_loss: 0.7395566701889038  val_accuracy: 56.875 %\n",
      "iteration: 1409  val_loss: 0.6676722764968872  val_accuracy: 56.875 %\n",
      "iteration: 1410  val_loss: 0.5605420470237732  val_accuracy: 56.875 %\n",
      "iteration: 1411  val_loss: 0.7039262056350708  val_accuracy: 56.875 %\n",
      "iteration: 1412  val_loss: 0.594054639339447  val_accuracy: 56.875 %\n",
      "iteration: 1413  val_loss: 0.6295636296272278  val_accuracy: 56.875 %\n",
      "iteration: 1414  val_loss: 0.6667359471321106  val_accuracy: 56.875 %\n",
      "iteration: 1415  val_loss: 0.6666854023933411  val_accuracy: 56.875 %\n",
      "iteration: 1416  val_loss: 0.7046845555305481  val_accuracy: 56.875 %\n",
      "iteration: 1417  val_loss: 0.6288537383079529  val_accuracy: 56.875 %\n",
      "iteration: 1418  val_loss: 0.7048370838165283  val_accuracy: 56.875 %\n",
      "iteration: 1419  val_loss: 0.7047202587127686  val_accuracy: 56.875 %\n",
      "iteration: 1420  val_loss: 0.5529388785362244  val_accuracy: 56.875 %\n",
      "iteration: 1421  val_loss: 0.7052615284919739  val_accuracy: 56.875 %\n",
      "iteration: 1422  val_loss: 0.7051403522491455  val_accuracy: 56.875 %\n",
      "iteration: 1423  val_loss: 0.6278184056282043  val_accuracy: 56.875 %\n",
      "iteration: 1424  val_loss: 0.7443322539329529  val_accuracy: 56.875 %\n",
      "iteration: 1425  val_loss: 0.5508667826652527  val_accuracy: 56.875 %\n",
      "iteration: 1426  val_loss: 0.7056413888931274  val_accuracy: 56.875 %\n",
      "iteration: 1427  val_loss: 0.7055162787437439  val_accuracy: 56.875 %\n",
      "iteration: 1428  val_loss: 0.6661888957023621  val_accuracy: 56.875 %\n",
      "iteration: 1429  val_loss: 0.6268202066421509  val_accuracy: 56.875 %\n",
      "iteration: 1430  val_loss: 0.665980875492096  val_accuracy: 56.875 %\n",
      "iteration: 1431  val_loss: 0.6260600090026855  val_accuracy: 56.875 %\n",
      "iteration: 1432  val_loss: 0.7464043498039246  val_accuracy: 56.875 %\n",
      "iteration: 1433  val_loss: 0.705760657787323  val_accuracy: 56.875 %\n",
      "iteration: 1434  val_loss: 0.6660439968109131  val_accuracy: 56.875 %\n",
      "iteration: 1435  val_loss: 0.6660001277923584  val_accuracy: 56.875 %\n",
      "iteration: 1436  val_loss: 0.7456086874008179  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1437  val_loss: 0.6661497354507446  val_accuracy: 56.875 %\n",
      "iteration: 1438  val_loss: 0.7055318355560303  val_accuracy: 56.875 %\n",
      "iteration: 1439  val_loss: 0.6661795973777771  val_accuracy: 56.875 %\n",
      "iteration: 1440  val_loss: 0.6661344766616821  val_accuracy: 56.875 %\n",
      "iteration: 1441  val_loss: 0.7055572271347046  val_accuracy: 56.875 %\n",
      "iteration: 1442  val_loss: 0.6661643981933594  val_accuracy: 56.875 %\n",
      "iteration: 1443  val_loss: 0.6529901027679443  val_accuracy: 56.875 %\n",
      "iteration: 1444  val_loss: 0.6660354733467102  val_accuracy: 56.875 %\n",
      "iteration: 1445  val_loss: 0.6659917831420898  val_accuracy: 56.875 %\n",
      "iteration: 1446  val_loss: 0.6659484505653381  val_accuracy: 56.875 %\n",
      "iteration: 1447  val_loss: 0.7058709263801575  val_accuracy: 56.875 %\n",
      "iteration: 1448  val_loss: 0.7057434916496277  val_accuracy: 56.875 %\n",
      "iteration: 1449  val_loss: 0.7847440242767334  val_accuracy: 56.875 %\n",
      "iteration: 1450  val_loss: 0.7825570106506348  val_accuracy: 56.875 %\n",
      "iteration: 1451  val_loss: 0.7045949101448059  val_accuracy: 56.875 %\n",
      "iteration: 1452  val_loss: 0.7044806480407715  val_accuracy: 56.875 %\n",
      "iteration: 1453  val_loss: 0.6668469309806824  val_accuracy: 56.875 %\n",
      "iteration: 1454  val_loss: 0.5914987921714783  val_accuracy: 56.875 %\n",
      "iteration: 1455  val_loss: 0.628085732460022  val_accuracy: 56.875 %\n",
      "iteration: 1456  val_loss: 0.6663239002227783  val_accuracy: 56.875 %\n",
      "iteration: 1457  val_loss: 0.666277289390564  val_accuracy: 56.875 %\n",
      "iteration: 1458  val_loss: 0.6662312746047974  val_accuracy: 56.875 %\n",
      "iteration: 1459  val_loss: 0.7053977251052856  val_accuracy: 56.875 %\n",
      "iteration: 1460  val_loss: 0.7833045125007629  val_accuracy: 56.875 %\n",
      "iteration: 1461  val_loss: 0.6665822267532349  val_accuracy: 56.875 %\n",
      "iteration: 1462  val_loss: 0.7048420310020447  val_accuracy: 56.875 %\n",
      "iteration: 1463  val_loss: 0.6666090488433838  val_accuracy: 56.875 %\n",
      "iteration: 1464  val_loss: 0.7430421113967896  val_accuracy: 56.875 %\n",
      "iteration: 1465  val_loss: 0.5535648465156555  val_accuracy: 56.875 %\n",
      "iteration: 1466  val_loss: 0.6275305151939392  val_accuracy: 56.875 %\n",
      "iteration: 1467  val_loss: 0.7054200172424316  val_accuracy: 56.875 %\n",
      "iteration: 1468  val_loss: 0.7052971720695496  val_accuracy: 56.875 %\n",
      "iteration: 1469  val_loss: 0.6663221120834351  val_accuracy: 56.875 %\n",
      "iteration: 1470  val_loss: 0.7052507400512695  val_accuracy: 56.875 %\n",
      "iteration: 1471  val_loss: 0.7051296830177307  val_accuracy: 56.875 %\n",
      "iteration: 1472  val_loss: 0.7050097584724426  val_accuracy: 56.875 %\n",
      "iteration: 1473  val_loss: 0.6281120181083679  val_accuracy: 56.875 %\n",
      "iteration: 1474  val_loss: 0.7051612734794617  val_accuracy: 56.875 %\n",
      "iteration: 1475  val_loss: 0.7823108434677124  val_accuracy: 56.875 %\n",
      "iteration: 1476  val_loss: 0.6667318940162659  val_accuracy: 56.875 %\n",
      "iteration: 1477  val_loss: 0.7046147584915161  val_accuracy: 56.875 %\n",
      "iteration: 1478  val_loss: 0.6290148496627808  val_accuracy: 56.875 %\n",
      "iteration: 1479  val_loss: 0.7811392545700073  val_accuracy: 56.875 %\n",
      "iteration: 1480  val_loss: 0.6295506358146667  val_accuracy: 56.875 %\n",
      "iteration: 1481  val_loss: 0.7045380473136902  val_accuracy: 56.875 %\n",
      "iteration: 1482  val_loss: 0.7420401573181152  val_accuracy: 56.875 %\n",
      "iteration: 1483  val_loss: 0.6670138835906982  val_accuracy: 56.875 %\n",
      "iteration: 1484  val_loss: 0.7414416670799255  val_accuracy: 56.875 %\n",
      "iteration: 1485  val_loss: 0.6671680808067322  val_accuracy: 56.875 %\n",
      "iteration: 1486  val_loss: 0.6302443742752075  val_accuracy: 56.875 %\n",
      "iteration: 1487  val_loss: 0.6669300198554993  val_accuracy: 56.875 %\n",
      "iteration: 1488  val_loss: 0.7043222784996033  val_accuracy: 56.875 %\n",
      "iteration: 1489  val_loss: 0.7414673566818237  val_accuracy: 56.875 %\n",
      "iteration: 1490  val_loss: 0.6671614646911621  val_accuracy: 56.875 %\n",
      "iteration: 1491  val_loss: 0.6302219033241272  val_accuracy: 56.875 %\n",
      "iteration: 1492  val_loss: 0.5549283623695374  val_accuracy: 56.875 %\n",
      "iteration: 1493  val_loss: 0.7049031853675842  val_accuracy: 56.875 %\n",
      "iteration: 1494  val_loss: 0.7430018186569214  val_accuracy: 56.875 %\n",
      "iteration: 1495  val_loss: 0.6290642619132996  val_accuracy: 56.875 %\n",
      "iteration: 1496  val_loss: 0.6665953993797302  val_accuracy: 56.875 %\n",
      "iteration: 1497  val_loss: 0.6282706260681152  val_accuracy: 56.875 %\n",
      "iteration: 1498  val_loss: 0.7050913572311401  val_accuracy: 56.875 %\n",
      "iteration: 1499  val_loss: 0.7049719095230103  val_accuracy: 56.875 %\n",
      "iteration: 1500  val_loss: 0.5898694396018982  val_accuracy: 56.875 %\n",
      "iteration: 1501  val_loss: 0.5489756464958191  val_accuracy: 56.875 %\n",
      "iteration: 1502  val_loss: 0.7059938907623291  val_accuracy: 56.875 %\n",
      "iteration: 1503  val_loss: 0.6259526014328003  val_accuracy: 56.875 %\n",
      "iteration: 1504  val_loss: 0.7465326189994812  val_accuracy: 56.875 %\n",
      "iteration: 1505  val_loss: 0.6260722875595093  val_accuracy: 56.875 %\n",
      "iteration: 1506  val_loss: 0.7060861587524414  val_accuracy: 56.875 %\n",
      "iteration: 1507  val_loss: 0.6257564425468445  val_accuracy: 56.875 %\n",
      "iteration: 1508  val_loss: 0.6251662969589233  val_accuracy: 56.875 %\n",
      "iteration: 1509  val_loss: 0.665547251701355  val_accuracy: 56.875 %\n",
      "iteration: 1510  val_loss: 0.7065858244895935  val_accuracy: 56.875 %\n",
      "iteration: 1511  val_loss: 0.6247110366821289  val_accuracy: 56.875 %\n",
      "iteration: 1512  val_loss: 0.7480320930480957  val_accuracy: 56.875 %\n",
      "iteration: 1513  val_loss: 0.6248417496681213  val_accuracy: 56.875 %\n",
      "iteration: 1514  val_loss: 0.6242599487304688  val_accuracy: 56.875 %\n",
      "iteration: 1515  val_loss: 0.6653167605400085  val_accuracy: 56.875 %\n",
      "iteration: 1516  val_loss: 0.7487642168998718  val_accuracy: 56.875 %\n",
      "iteration: 1517  val_loss: 0.7066734433174133  val_accuracy: 56.875 %\n",
      "iteration: 1518  val_loss: 0.542523205280304  val_accuracy: 56.875 %\n",
      "iteration: 1519  val_loss: 0.6651694774627686  val_accuracy: 56.875 %\n",
      "iteration: 1520  val_loss: 0.665134072303772  val_accuracy: 56.875 %\n",
      "iteration: 1521  val_loss: 0.6228152513504028  val_accuracy: 56.875 %\n",
      "iteration: 1522  val_loss: 0.6222513914108276  val_accuracy: 56.875 %\n",
      "iteration: 1523  val_loss: 0.6648247838020325  val_accuracy: 56.875 %\n",
      "iteration: 1524  val_loss: 0.7512600421905518  val_accuracy: 56.875 %\n",
      "iteration: 1525  val_loss: 0.7503534555435181  val_accuracy: 56.875 %\n",
      "iteration: 1526  val_loss: 0.6651406288146973  val_accuracy: 56.875 %\n",
      "iteration: 1527  val_loss: 0.6228413581848145  val_accuracy: 56.875 %\n",
      "iteration: 1528  val_loss: 0.7503451704978943  val_accuracy: 56.875 %\n",
      "iteration: 1529  val_loss: 0.5808358192443848  val_accuracy: 56.875 %\n",
      "iteration: 1530  val_loss: 0.7506961822509766  val_accuracy: 56.875 %\n",
      "iteration: 1531  val_loss: 0.7074346542358398  val_accuracy: 56.875 %\n",
      "iteration: 1532  val_loss: 0.7494376301765442  val_accuracy: 56.875 %\n",
      "iteration: 1533  val_loss: 0.6653234958648682  val_accuracy: 56.875 %\n",
      "iteration: 1534  val_loss: 0.665286660194397  val_accuracy: 56.875 %\n",
      "iteration: 1535  val_loss: 0.7070807218551636  val_accuracy: 56.875 %\n",
      "iteration: 1536  val_loss: 0.6653218269348145  val_accuracy: 56.875 %\n",
      "iteration: 1537  val_loss: 0.7487398386001587  val_accuracy: 56.875 %\n",
      "iteration: 1538  val_loss: 0.7478612661361694  val_accuracy: 56.875 %\n",
      "iteration: 1539  val_loss: 0.7469925284385681  val_accuracy: 56.875 %\n",
      "iteration: 1540  val_loss: 0.5855453014373779  val_accuracy: 56.875 %\n",
      "iteration: 1541  val_loss: 0.747368335723877  val_accuracy: 56.875 %\n",
      "iteration: 1542  val_loss: 0.7061312794685364  val_accuracy: 56.875 %\n",
      "iteration: 1543  val_loss: 0.6658310294151306  val_accuracy: 56.875 %\n",
      "iteration: 1544  val_loss: 0.6657892465591431  val_accuracy: 56.875 %\n",
      "iteration: 1545  val_loss: 0.7061481475830078  val_accuracy: 56.875 %\n",
      "iteration: 1546  val_loss: 0.7060178518295288  val_accuracy: 56.875 %\n",
      "iteration: 1547  val_loss: 0.6658952832221985  val_accuracy: 56.875 %\n",
      "iteration: 1548  val_loss: 0.6658529043197632  val_accuracy: 56.875 %\n",
      "iteration: 1549  val_loss: 0.7060363292694092  val_accuracy: 56.875 %\n",
      "iteration: 1550  val_loss: 0.7459297776222229  val_accuracy: 56.875 %\n",
      "iteration: 1551  val_loss: 0.6660765409469604  val_accuracy: 56.875 %\n",
      "iteration: 1552  val_loss: 0.5867894291877747  val_accuracy: 56.875 %\n",
      "iteration: 1553  val_loss: 0.5850073099136353  val_accuracy: 56.875 %\n",
      "iteration: 1554  val_loss: 0.6654914021492004  val_accuracy: 56.875 %\n",
      "iteration: 1555  val_loss: 0.7479263544082642  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1556  val_loss: 0.7063472270965576  val_accuracy: 56.875 %\n",
      "iteration: 1557  val_loss: 0.5847022533416748  val_accuracy: 56.875 %\n",
      "iteration: 1558  val_loss: 0.6241918802261353  val_accuracy: 56.875 %\n",
      "iteration: 1559  val_loss: 0.6652997732162476  val_accuracy: 56.875 %\n",
      "iteration: 1560  val_loss: 0.6234710216522217  val_accuracy: 56.875 %\n",
      "iteration: 1561  val_loss: 0.622901439666748  val_accuracy: 56.875 %\n",
      "iteration: 1562  val_loss: 0.6223368048667908  val_accuracy: 56.875 %\n",
      "iteration: 1563  val_loss: 0.6648452877998352  val_accuracy: 56.875 %\n",
      "iteration: 1564  val_loss: 0.5353041291236877  val_accuracy: 56.875 %\n",
      "iteration: 1565  val_loss: 0.7087163925170898  val_accuracy: 56.875 %\n",
      "iteration: 1566  val_loss: 0.664553701877594  val_accuracy: 56.875 %\n",
      "iteration: 1567  val_loss: 0.8409379720687866  val_accuracy: 56.875 %\n",
      "iteration: 1568  val_loss: 0.6648898124694824  val_accuracy: 56.875 %\n",
      "iteration: 1569  val_loss: 0.6648573875427246  val_accuracy: 56.875 %\n",
      "iteration: 1570  val_loss: 0.7079558372497559  val_accuracy: 56.875 %\n",
      "iteration: 1571  val_loss: 0.6648944020271301  val_accuracy: 56.875 %\n",
      "iteration: 1572  val_loss: 0.7078772187232971  val_accuracy: 56.875 %\n",
      "iteration: 1573  val_loss: 0.750529408454895  val_accuracy: 56.875 %\n",
      "iteration: 1574  val_loss: 0.6228371262550354  val_accuracy: 56.875 %\n",
      "iteration: 1575  val_loss: 0.664966881275177  val_accuracy: 56.875 %\n",
      "iteration: 1576  val_loss: 0.7077248096466064  val_accuracy: 56.875 %\n",
      "iteration: 1577  val_loss: 0.66500324010849  val_accuracy: 56.875 %\n",
      "iteration: 1578  val_loss: 0.6649699211120605  val_accuracy: 56.875 %\n",
      "iteration: 1579  val_loss: 0.6649367809295654  val_accuracy: 56.875 %\n",
      "iteration: 1580  val_loss: 0.6649040579795837  val_accuracy: 56.875 %\n",
      "iteration: 1581  val_loss: 0.7508424520492554  val_accuracy: 56.875 %\n",
      "iteration: 1582  val_loss: 0.6225910186767578  val_accuracy: 56.875 %\n",
      "iteration: 1583  val_loss: 0.7935264110565186  val_accuracy: 56.875 %\n",
      "iteration: 1584  val_loss: 0.7072093486785889  val_accuracy: 56.875 %\n",
      "iteration: 1585  val_loss: 0.7070692181587219  val_accuracy: 56.875 %\n",
      "iteration: 1586  val_loss: 0.6653269529342651  val_accuracy: 56.875 %\n",
      "iteration: 1587  val_loss: 0.7070016860961914  val_accuracy: 56.875 %\n",
      "iteration: 1588  val_loss: 0.7068638801574707  val_accuracy: 56.875 %\n",
      "iteration: 1589  val_loss: 0.7067274451255798  val_accuracy: 56.875 %\n",
      "iteration: 1590  val_loss: 0.706592321395874  val_accuracy: 56.875 %\n",
      "iteration: 1591  val_loss: 0.665576696395874  val_accuracy: 56.875 %\n",
      "iteration: 1592  val_loss: 0.7065306901931763  val_accuracy: 56.875 %\n",
      "iteration: 1593  val_loss: 0.7471853494644165  val_accuracy: 56.875 %\n",
      "iteration: 1594  val_loss: 0.5852588415145874  val_accuracy: 56.875 %\n",
      "iteration: 1595  val_loss: 0.7065435647964478  val_accuracy: 56.875 %\n",
      "iteration: 1596  val_loss: 0.7064103484153748  val_accuracy: 56.875 %\n",
      "iteration: 1597  val_loss: 0.6656753420829773  val_accuracy: 56.875 %\n",
      "iteration: 1598  val_loss: 0.7470665574073792  val_accuracy: 56.875 %\n",
      "iteration: 1599  val_loss: 0.7864111065864563  val_accuracy: 56.875 %\n",
      "iteration: 1600  val_loss: 0.6661278605461121  val_accuracy: 56.875 %\n",
      "iteration: 1601  val_loss: 0.7055675983428955  val_accuracy: 56.875 %\n",
      "iteration: 1602  val_loss: 0.6268704533576965  val_accuracy: 56.875 %\n",
      "iteration: 1603  val_loss: 0.7454355955123901  val_accuracy: 56.875 %\n",
      "iteration: 1604  val_loss: 0.666187584400177  val_accuracy: 56.875 %\n",
      "iteration: 1605  val_loss: 0.7447943687438965  val_accuracy: 56.875 %\n",
      "iteration: 1606  val_loss: 0.6663368344306946  val_accuracy: 56.875 %\n",
      "iteration: 1607  val_loss: 0.5884190201759338  val_accuracy: 56.875 %\n",
      "iteration: 1608  val_loss: 0.7453778386116028  val_accuracy: 56.875 %\n",
      "iteration: 1609  val_loss: 0.7053726315498352  val_accuracy: 56.875 %\n",
      "iteration: 1610  val_loss: 0.6272987723350525  val_accuracy: 56.875 %\n",
      "iteration: 1611  val_loss: 0.7055215239524841  val_accuracy: 56.875 %\n",
      "iteration: 1612  val_loss: 0.7053988575935364  val_accuracy: 56.875 %\n",
      "iteration: 1613  val_loss: 0.6272408962249756  val_accuracy: 56.875 %\n",
      "iteration: 1614  val_loss: 0.666095495223999  val_accuracy: 56.875 %\n",
      "iteration: 1615  val_loss: 0.6660516262054443  val_accuracy: 56.875 %\n",
      "iteration: 1616  val_loss: 0.745381772518158  val_accuracy: 56.875 %\n",
      "iteration: 1617  val_loss: 0.7053741812705994  val_accuracy: 56.875 %\n",
      "iteration: 1618  val_loss: 0.6662741899490356  val_accuracy: 56.875 %\n",
      "iteration: 1619  val_loss: 0.7835243344306946  val_accuracy: 56.875 %\n",
      "iteration: 1620  val_loss: 0.743097186088562  val_accuracy: 56.875 %\n",
      "iteration: 1621  val_loss: 0.7045174837112427  val_accuracy: 56.875 %\n",
      "iteration: 1622  val_loss: 0.5916544795036316  val_accuracy: 56.875 %\n",
      "iteration: 1623  val_loss: 0.6665223836898804  val_accuracy: 56.875 %\n",
      "iteration: 1624  val_loss: 0.589556097984314  val_accuracy: 56.875 %\n",
      "iteration: 1625  val_loss: 0.54855877161026  val_accuracy: 56.875 %\n",
      "iteration: 1626  val_loss: 0.7060658931732178  val_accuracy: 56.875 %\n",
      "iteration: 1627  val_loss: 0.665867269039154  val_accuracy: 56.875 %\n",
      "iteration: 1628  val_loss: 0.6658256649971008  val_accuracy: 56.875 %\n",
      "iteration: 1629  val_loss: 0.7060835361480713  val_accuracy: 56.875 %\n",
      "iteration: 1630  val_loss: 0.7059552073478699  val_accuracy: 56.875 %\n",
      "iteration: 1631  val_loss: 0.705828070640564  val_accuracy: 56.875 %\n",
      "iteration: 1632  val_loss: 0.7454005479812622  val_accuracy: 56.875 %\n",
      "iteration: 1633  val_loss: 0.7053812742233276  val_accuracy: 56.875 %\n",
      "iteration: 1634  val_loss: 0.666269838809967  val_accuracy: 56.875 %\n",
      "iteration: 1635  val_loss: 0.6271143555641174  val_accuracy: 56.875 %\n",
      "iteration: 1636  val_loss: 0.7056047916412354  val_accuracy: 56.875 %\n",
      "iteration: 1637  val_loss: 0.6661351919174194  val_accuracy: 56.875 %\n",
      "iteration: 1638  val_loss: 0.62662672996521  val_accuracy: 56.875 %\n",
      "iteration: 1639  val_loss: 0.6659308671951294  val_accuracy: 56.875 %\n",
      "iteration: 1640  val_loss: 0.6658885478973389  val_accuracy: 56.875 %\n",
      "iteration: 1641  val_loss: 0.6257197856903076  val_accuracy: 56.875 %\n",
      "iteration: 1642  val_loss: 0.6251355409622192  val_accuracy: 56.875 %\n",
      "iteration: 1643  val_loss: 0.7884936928749084  val_accuracy: 56.875 %\n",
      "iteration: 1644  val_loss: 0.6256904006004333  val_accuracy: 56.875 %\n",
      "iteration: 1645  val_loss: 0.6656844019889832  val_accuracy: 56.875 %\n",
      "iteration: 1646  val_loss: 0.7470249533653259  val_accuracy: 56.875 %\n",
      "iteration: 1647  val_loss: 0.6256589889526367  val_accuracy: 56.875 %\n",
      "iteration: 1648  val_loss: 0.6250753998756409  val_accuracy: 56.875 %\n",
      "iteration: 1649  val_loss: 0.7475823760032654  val_accuracy: 56.875 %\n",
      "iteration: 1650  val_loss: 0.7062170505523682  val_accuracy: 56.875 %\n",
      "iteration: 1651  val_loss: 0.6657821536064148  val_accuracy: 56.875 %\n",
      "iteration: 1652  val_loss: 0.6657412052154541  val_accuracy: 56.875 %\n",
      "iteration: 1653  val_loss: 0.7467646598815918  val_accuracy: 56.875 %\n",
      "iteration: 1654  val_loss: 0.665887713432312  val_accuracy: 56.875 %\n",
      "iteration: 1655  val_loss: 0.6658458709716797  val_accuracy: 56.875 %\n",
      "iteration: 1656  val_loss: 0.5853171348571777  val_accuracy: 56.875 %\n",
      "iteration: 1657  val_loss: 0.665539562702179  val_accuracy: 56.875 %\n",
      "iteration: 1658  val_loss: 0.7065993547439575  val_accuracy: 56.875 %\n",
      "iteration: 1659  val_loss: 0.665573000907898  val_accuracy: 56.875 %\n",
      "iteration: 1660  val_loss: 0.7475413084030151  val_accuracy: 56.875 %\n",
      "iteration: 1661  val_loss: 0.7466841340065002  val_accuracy: 56.875 %\n",
      "iteration: 1662  val_loss: 0.6659054756164551  val_accuracy: 56.875 %\n",
      "iteration: 1663  val_loss: 0.6658635139465332  val_accuracy: 56.875 %\n",
      "iteration: 1664  val_loss: 0.6658218502998352  val_accuracy: 56.875 %\n",
      "iteration: 1665  val_loss: 0.7463997602462769  val_accuracy: 56.875 %\n",
      "iteration: 1666  val_loss: 0.6261757016181946  val_accuracy: 56.875 %\n",
      "iteration: 1667  val_loss: 0.7462594509124756  val_accuracy: 56.875 %\n",
      "iteration: 1668  val_loss: 0.6262921690940857  val_accuracy: 56.875 %\n",
      "iteration: 1669  val_loss: 0.6257029175758362  val_accuracy: 56.875 %\n",
      "iteration: 1670  val_loss: 0.6251187920570374  val_accuracy: 56.875 %\n",
      "iteration: 1671  val_loss: 0.6655364632606506  val_accuracy: 56.875 %\n",
      "iteration: 1672  val_loss: 0.542177140712738  val_accuracy: 56.875 %\n",
      "iteration: 1673  val_loss: 0.6229740381240845  val_accuracy: 56.875 %\n",
      "iteration: 1674  val_loss: 0.6224141120910645  val_accuracy: 56.875 %\n",
      "iteration: 1675  val_loss: 0.793882429599762  val_accuracy: 56.875 %\n",
      "iteration: 1676  val_loss: 0.8337656259536743  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1677  val_loss: 0.7885251045227051  val_accuracy: 56.875 %\n",
      "iteration: 1678  val_loss: 0.625674307346344  val_accuracy: 56.875 %\n",
      "iteration: 1679  val_loss: 0.7062698006629944  val_accuracy: 56.875 %\n",
      "iteration: 1680  val_loss: 0.7869128584861755  val_accuracy: 56.875 %\n",
      "iteration: 1681  val_loss: 0.7451670169830322  val_accuracy: 56.875 %\n",
      "iteration: 1682  val_loss: 0.783379316329956  val_accuracy: 56.875 %\n",
      "iteration: 1683  val_loss: 0.6283472776412964  val_accuracy: 56.875 %\n",
      "iteration: 1684  val_loss: 0.6277399063110352  val_accuracy: 56.875 %\n",
      "iteration: 1685  val_loss: 0.7444164156913757  val_accuracy: 56.875 %\n",
      "iteration: 1686  val_loss: 0.7050098776817322  val_accuracy: 56.875 %\n",
      "iteration: 1687  val_loss: 0.7432842254638672  val_accuracy: 56.875 %\n",
      "iteration: 1688  val_loss: 0.7045868635177612  val_accuracy: 56.875 %\n",
      "iteration: 1689  val_loss: 0.7044737339019775  val_accuracy: 56.875 %\n",
      "iteration: 1690  val_loss: 0.6668508648872375  val_accuracy: 56.875 %\n",
      "iteration: 1691  val_loss: 0.6667997241020203  val_accuracy: 56.875 %\n",
      "iteration: 1692  val_loss: 0.6289855241775513  val_accuracy: 56.875 %\n",
      "iteration: 1693  val_loss: 0.7429797053337097  val_accuracy: 56.875 %\n",
      "iteration: 1694  val_loss: 0.6667751669883728  val_accuracy: 56.875 %\n",
      "iteration: 1695  val_loss: 0.6667248010635376  val_accuracy: 56.875 %\n",
      "iteration: 1696  val_loss: 0.7425742149353027  val_accuracy: 56.875 %\n",
      "iteration: 1697  val_loss: 0.7043240070343018  val_accuracy: 56.875 %\n",
      "iteration: 1698  val_loss: 0.7414749264717102  val_accuracy: 56.875 %\n",
      "iteration: 1699  val_loss: 0.6303939819335938  val_accuracy: 56.875 %\n",
      "iteration: 1700  val_loss: 0.704180896282196  val_accuracy: 56.875 %\n",
      "iteration: 1701  val_loss: 0.5930081009864807  val_accuracy: 56.875 %\n",
      "iteration: 1702  val_loss: 0.6289684176445007  val_accuracy: 56.875 %\n",
      "iteration: 1703  val_loss: 0.6283556818962097  val_accuracy: 56.875 %\n",
      "iteration: 1704  val_loss: 0.6663997769355774  val_accuracy: 56.875 %\n",
      "iteration: 1705  val_loss: 0.5888073444366455  val_accuracy: 56.875 %\n",
      "iteration: 1706  val_loss: 0.6660693883895874  val_accuracy: 56.875 %\n",
      "iteration: 1707  val_loss: 0.6263867616653442  val_accuracy: 56.875 %\n",
      "iteration: 1708  val_loss: 0.6658671498298645  val_accuracy: 56.875 %\n",
      "iteration: 1709  val_loss: 0.6658255457878113  val_accuracy: 56.875 %\n",
      "iteration: 1710  val_loss: 0.6254848837852478  val_accuracy: 56.875 %\n",
      "iteration: 1711  val_loss: 0.6656309366226196  val_accuracy: 56.875 %\n",
      "iteration: 1712  val_loss: 0.5839120149612427  val_accuracy: 56.875 %\n",
      "iteration: 1713  val_loss: 0.6237522959709167  val_accuracy: 56.875 %\n",
      "iteration: 1714  val_loss: 0.6651913523674011  val_accuracy: 56.875 %\n",
      "iteration: 1715  val_loss: 0.6230446696281433  val_accuracy: 56.875 %\n",
      "iteration: 1716  val_loss: 0.665017306804657  val_accuracy: 56.875 %\n",
      "iteration: 1717  val_loss: 0.5797109007835388  val_accuracy: 56.875 %\n",
      "iteration: 1718  val_loss: 0.6213760375976562  val_accuracy: 56.875 %\n",
      "iteration: 1719  val_loss: 0.664620041847229  val_accuracy: 56.875 %\n",
      "iteration: 1720  val_loss: 0.6645901799201965  val_accuracy: 56.875 %\n",
      "iteration: 1721  val_loss: 0.6645607352256775  val_accuracy: 56.875 %\n",
      "iteration: 1722  val_loss: 0.7526923418045044  val_accuracy: 56.875 %\n",
      "iteration: 1723  val_loss: 0.6646966934204102  val_accuracy: 56.875 %\n",
      "iteration: 1724  val_loss: 0.6646661758422852  val_accuracy: 56.875 %\n",
      "iteration: 1725  val_loss: 0.5771612524986267  val_accuracy: 56.875 %\n",
      "iteration: 1726  val_loss: 0.7088854312896729  val_accuracy: 56.875 %\n",
      "iteration: 1727  val_loss: 0.7529745697975159  val_accuracy: 56.875 %\n",
      "iteration: 1728  val_loss: 0.7083516120910645  val_accuracy: 56.875 %\n",
      "iteration: 1729  val_loss: 0.6212276816368103  val_accuracy: 56.875 %\n",
      "iteration: 1730  val_loss: 0.7084878087043762  val_accuracy: 56.875 %\n",
      "iteration: 1731  val_loss: 0.6646532416343689  val_accuracy: 56.875 %\n",
      "iteration: 1732  val_loss: 0.6646230816841125  val_accuracy: 56.875 %\n",
      "iteration: 1733  val_loss: 0.7523468136787415  val_accuracy: 56.875 %\n",
      "iteration: 1734  val_loss: 0.708098292350769  val_accuracy: 56.875 %\n",
      "iteration: 1735  val_loss: 0.6217078566551208  val_accuracy: 56.875 %\n",
      "iteration: 1736  val_loss: 0.664697527885437  val_accuracy: 56.875 %\n",
      "iteration: 1737  val_loss: 0.7519407272338867  val_accuracy: 56.875 %\n",
      "iteration: 1738  val_loss: 0.7510349154472351  val_accuracy: 56.875 %\n",
      "iteration: 1739  val_loss: 0.7075729966163635  val_accuracy: 56.875 %\n",
      "iteration: 1740  val_loss: 0.6650763154029846  val_accuracy: 56.875 %\n",
      "iteration: 1741  val_loss: 0.622585117816925  val_accuracy: 56.875 %\n",
      "iteration: 1742  val_loss: 0.6220287084579468  val_accuracy: 56.875 %\n",
      "iteration: 1743  val_loss: 0.6214772462844849  val_accuracy: 56.875 %\n",
      "iteration: 1744  val_loss: 0.6646435260772705  val_accuracy: 56.875 %\n",
      "iteration: 1745  val_loss: 0.6208029389381409  val_accuracy: 56.875 %\n",
      "iteration: 1746  val_loss: 0.7087135314941406  val_accuracy: 56.875 %\n",
      "iteration: 1747  val_loss: 0.5765489935874939  val_accuracy: 56.875 %\n",
      "iteration: 1748  val_loss: 0.7985429167747498  val_accuracy: 56.875 %\n",
      "iteration: 1749  val_loss: 0.664594292640686  val_accuracy: 56.875 %\n",
      "iteration: 1750  val_loss: 0.6205943822860718  val_accuracy: 56.875 %\n",
      "iteration: 1751  val_loss: 0.5756707191467285  val_accuracy: 56.875 %\n",
      "iteration: 1752  val_loss: 0.7544589638710022  val_accuracy: 56.875 %\n",
      "iteration: 1753  val_loss: 0.8426649570465088  val_accuracy: 56.875 %\n",
      "iteration: 1754  val_loss: 0.6213518381118774  val_accuracy: 56.875 %\n",
      "iteration: 1755  val_loss: 0.6646143198013306  val_accuracy: 56.875 %\n",
      "iteration: 1756  val_loss: 0.5767741203308105  val_accuracy: 56.875 %\n",
      "iteration: 1757  val_loss: 0.6197278499603271  val_accuracy: 56.875 %\n",
      "iteration: 1758  val_loss: 0.6191966533660889  val_accuracy: 56.875 %\n",
      "iteration: 1759  val_loss: 0.6186702251434326  val_accuracy: 56.875 %\n",
      "iteration: 1760  val_loss: 0.7557529211044312  val_accuracy: 56.875 %\n",
      "iteration: 1761  val_loss: 0.6188588738441467  val_accuracy: 56.875 %\n",
      "iteration: 1762  val_loss: 0.8469567894935608  val_accuracy: 56.875 %\n",
      "iteration: 1763  val_loss: 0.6198711395263672  val_accuracy: 56.875 %\n",
      "iteration: 1764  val_loss: 0.7092198133468628  val_accuracy: 56.875 %\n",
      "iteration: 1765  val_loss: 0.61962890625  val_accuracy: 56.875 %\n",
      "iteration: 1766  val_loss: 0.8447412252426147  val_accuracy: 56.875 %\n",
      "iteration: 1767  val_loss: 0.6645753383636475  val_accuracy: 56.875 %\n",
      "iteration: 1768  val_loss: 0.6645456552505493  val_accuracy: 56.875 %\n",
      "iteration: 1769  val_loss: 0.6203858256340027  val_accuracy: 56.875 %\n",
      "iteration: 1770  val_loss: 0.7089400887489319  val_accuracy: 56.875 %\n",
      "iteration: 1771  val_loss: 0.6644591093063354  val_accuracy: 56.875 %\n",
      "iteration: 1772  val_loss: 0.6200133562088013  val_accuracy: 56.875 %\n",
      "iteration: 1773  val_loss: 0.6643085479736328  val_accuracy: 56.875 %\n",
      "iteration: 1774  val_loss: 0.7541353702545166  val_accuracy: 56.875 %\n",
      "iteration: 1775  val_loss: 0.6200655102729797  val_accuracy: 56.875 %\n",
      "iteration: 1776  val_loss: 0.7539075016975403  val_accuracy: 56.875 %\n",
      "iteration: 1777  val_loss: 0.620237410068512  val_accuracy: 56.875 %\n",
      "iteration: 1778  val_loss: 0.6196969151496887  val_accuracy: 56.875 %\n",
      "iteration: 1779  val_loss: 0.6191611886024475  val_accuracy: 56.875 %\n",
      "iteration: 1780  val_loss: 0.7551027536392212  val_accuracy: 56.875 %\n",
      "iteration: 1781  val_loss: 0.7092159986495972  val_accuracy: 56.875 %\n",
      "iteration: 1782  val_loss: 0.7090537548065186  val_accuracy: 56.875 %\n",
      "iteration: 1783  val_loss: 0.7088931798934937  val_accuracy: 56.875 %\n",
      "iteration: 1784  val_loss: 0.6202234029769897  val_accuracy: 56.875 %\n",
      "iteration: 1785  val_loss: 0.7983728647232056  val_accuracy: 56.875 %\n",
      "iteration: 1786  val_loss: 0.6208123564720154  val_accuracy: 56.875 %\n",
      "iteration: 1787  val_loss: 0.6644888520240784  val_accuracy: 56.875 %\n",
      "iteration: 1788  val_loss: 0.7974151968955994  val_accuracy: 56.875 %\n",
      "iteration: 1789  val_loss: 0.621272623538971  val_accuracy: 56.875 %\n",
      "iteration: 1790  val_loss: 0.576850950717926  val_accuracy: 56.875 %\n",
      "iteration: 1791  val_loss: 0.6197622418403625  val_accuracy: 56.875 %\n",
      "iteration: 1792  val_loss: 0.6192259788513184  val_accuracy: 56.875 %\n",
      "iteration: 1793  val_loss: 0.7095757722854614  val_accuracy: 56.875 %\n",
      "iteration: 1794  val_loss: 0.6189912557601929  val_accuracy: 56.875 %\n",
      "iteration: 1795  val_loss: 0.7097068428993225  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1796  val_loss: 0.6641494035720825  val_accuracy: 56.875 %\n",
      "iteration: 1797  val_loss: 0.7096046209335327  val_accuracy: 56.875 %\n",
      "iteration: 1798  val_loss: 0.7094383835792542  val_accuracy: 56.875 %\n",
      "iteration: 1799  val_loss: 0.6642550230026245  val_accuracy: 56.875 %\n",
      "iteration: 1800  val_loss: 0.6642284989356995  val_accuracy: 56.875 %\n",
      "iteration: 1801  val_loss: 0.6189992427825928  val_accuracy: 56.875 %\n",
      "iteration: 1802  val_loss: 0.8465512990951538  val_accuracy: 56.875 %\n",
      "iteration: 1803  val_loss: 0.6644301414489746  val_accuracy: 56.875 %\n",
      "iteration: 1804  val_loss: 0.7089163064956665  val_accuracy: 56.875 %\n",
      "iteration: 1805  val_loss: 0.6201810240745544  val_accuracy: 56.875 %\n",
      "iteration: 1806  val_loss: 0.6643460392951965  val_accuracy: 56.875 %\n",
      "iteration: 1807  val_loss: 0.619519829750061  val_accuracy: 56.875 %\n",
      "iteration: 1808  val_loss: 0.7546265721321106  val_accuracy: 56.875 %\n",
      "iteration: 1809  val_loss: 0.753682553768158  val_accuracy: 56.875 %\n",
      "iteration: 1810  val_loss: 0.7527493834495544  val_accuracy: 56.875 %\n",
      "iteration: 1811  val_loss: 0.6211185455322266  val_accuracy: 56.875 %\n",
      "iteration: 1812  val_loss: 0.6205700635910034  val_accuracy: 56.875 %\n",
      "iteration: 1813  val_loss: 0.6644337773323059  val_accuracy: 56.875 %\n",
      "iteration: 1814  val_loss: 0.6644055247306824  val_accuracy: 56.875 %\n",
      "iteration: 1815  val_loss: 0.5751833319664001  val_accuracy: 56.875 %\n",
      "iteration: 1816  val_loss: 0.6188310980796814  val_accuracy: 56.875 %\n",
      "iteration: 1817  val_loss: 0.6183032393455505  val_accuracy: 56.875 %\n",
      "iteration: 1818  val_loss: 0.6639385223388672  val_accuracy: 56.875 %\n",
      "iteration: 1819  val_loss: 0.6639154553413391  val_accuracy: 56.875 %\n",
      "iteration: 1820  val_loss: 0.7565456032752991  val_accuracy: 56.875 %\n",
      "iteration: 1821  val_loss: 0.572496771812439  val_accuracy: 56.875 %\n",
      "iteration: 1822  val_loss: 0.663846492767334  val_accuracy: 56.875 %\n",
      "iteration: 1823  val_loss: 0.7569923996925354  val_accuracy: 56.875 %\n",
      "iteration: 1824  val_loss: 0.6639726161956787  val_accuracy: 56.875 %\n",
      "iteration: 1825  val_loss: 0.6639490723609924  val_accuracy: 56.875 %\n",
      "iteration: 1826  val_loss: 0.8025320768356323  val_accuracy: 56.875 %\n",
      "iteration: 1827  val_loss: 0.664166271686554  val_accuracy: 56.875 %\n",
      "iteration: 1828  val_loss: 0.8004015684127808  val_accuracy: 56.875 %\n",
      "iteration: 1829  val_loss: 0.7534979581832886  val_accuracy: 56.875 %\n",
      "iteration: 1830  val_loss: 0.6205410957336426  val_accuracy: 56.875 %\n",
      "iteration: 1831  val_loss: 0.6644282937049866  val_accuracy: 56.875 %\n",
      "iteration: 1832  val_loss: 0.7089201211929321  val_accuracy: 56.875 %\n",
      "iteration: 1833  val_loss: 0.8416492342948914  val_accuracy: 56.875 %\n",
      "iteration: 1834  val_loss: 0.7510678768157959  val_accuracy: 56.875 %\n",
      "iteration: 1835  val_loss: 0.7075861692428589  val_accuracy: 56.875 %\n",
      "iteration: 1836  val_loss: 0.6650698781013489  val_accuracy: 56.875 %\n",
      "iteration: 1837  val_loss: 0.834941565990448  val_accuracy: 56.875 %\n",
      "iteration: 1838  val_loss: 0.5414825081825256  val_accuracy: 56.875 %\n",
      "iteration: 1839  val_loss: 0.7498124241828918  val_accuracy: 56.875 %\n",
      "iteration: 1840  val_loss: 0.7907729744911194  val_accuracy: 56.875 %\n",
      "iteration: 1841  val_loss: 0.7065359354019165  val_accuracy: 56.875 %\n",
      "iteration: 1842  val_loss: 0.6248112320899963  val_accuracy: 56.875 %\n",
      "iteration: 1843  val_loss: 0.706680417060852  val_accuracy: 56.875 %\n",
      "iteration: 1844  val_loss: 0.7475621700286865  val_accuracy: 56.875 %\n",
      "iteration: 1845  val_loss: 0.7467052936553955  val_accuracy: 56.875 %\n",
      "iteration: 1846  val_loss: 0.6259222030639648  val_accuracy: 56.875 %\n",
      "iteration: 1847  val_loss: 0.706153392791748  val_accuracy: 56.875 %\n",
      "iteration: 1848  val_loss: 0.6658176779747009  val_accuracy: 56.875 %\n",
      "iteration: 1849  val_loss: 0.6254559755325317  val_accuracy: 56.875 %\n",
      "iteration: 1850  val_loss: 0.7063727378845215  val_accuracy: 56.875 %\n",
      "iteration: 1851  val_loss: 0.6251505017280579  val_accuracy: 56.875 %\n",
      "iteration: 1852  val_loss: 0.70651775598526  val_accuracy: 56.875 %\n",
      "iteration: 1853  val_loss: 0.7471532225608826  val_accuracy: 56.875 %\n",
      "iteration: 1854  val_loss: 0.6658023595809937  val_accuracy: 56.875 %\n",
      "iteration: 1855  val_loss: 0.7464874386787415  val_accuracy: 56.875 %\n",
      "iteration: 1856  val_loss: 0.5862558484077454  val_accuracy: 56.875 %\n",
      "iteration: 1857  val_loss: 0.625087559223175  val_accuracy: 56.875 %\n",
      "iteration: 1858  val_loss: 0.7065479755401611  val_accuracy: 56.875 %\n",
      "iteration: 1859  val_loss: 0.6656006574630737  val_accuracy: 56.875 %\n",
      "iteration: 1860  val_loss: 0.6655615568161011  val_accuracy: 56.875 %\n",
      "iteration: 1861  val_loss: 0.7886307239532471  val_accuracy: 56.875 %\n",
      "iteration: 1862  val_loss: 0.6256197690963745  val_accuracy: 56.875 %\n",
      "iteration: 1863  val_loss: 0.746924638748169  val_accuracy: 56.875 %\n",
      "iteration: 1864  val_loss: 0.6257412433624268  val_accuracy: 56.875 %\n",
      "iteration: 1865  val_loss: 0.6656977534294128  val_accuracy: 56.875 %\n",
      "iteration: 1866  val_loss: 0.665657639503479  val_accuracy: 56.875 %\n",
      "iteration: 1867  val_loss: 0.6656180024147034  val_accuracy: 56.875 %\n",
      "iteration: 1868  val_loss: 0.5838257670402527  val_accuracy: 56.875 %\n",
      "iteration: 1869  val_loss: 0.6653220057487488  val_accuracy: 56.875 %\n",
      "iteration: 1870  val_loss: 0.7070112824440002  val_accuracy: 56.875 %\n",
      "iteration: 1871  val_loss: 0.7899069786071777  val_accuracy: 56.875 %\n",
      "iteration: 1872  val_loss: 0.5842934250831604  val_accuracy: 56.875 %\n",
      "iteration: 1873  val_loss: 0.7068099975585938  val_accuracy: 56.875 %\n",
      "iteration: 1874  val_loss: 0.7066740989685059  val_accuracy: 56.875 %\n",
      "iteration: 1875  val_loss: 0.7885532379150391  val_accuracy: 56.875 %\n",
      "iteration: 1876  val_loss: 0.665830671787262  val_accuracy: 56.875 %\n",
      "iteration: 1877  val_loss: 0.6255038976669312  val_accuracy: 56.875 %\n",
      "iteration: 1878  val_loss: 0.7470641136169434  val_accuracy: 56.875 %\n",
      "iteration: 1879  val_loss: 0.7864083051681519  val_accuracy: 56.875 %\n",
      "iteration: 1880  val_loss: 0.7448589205741882  val_accuracy: 56.875 %\n",
      "iteration: 1881  val_loss: 0.6274663209915161  val_accuracy: 56.875 %\n",
      "iteration: 1882  val_loss: 0.7054459452629089  val_accuracy: 56.875 %\n",
      "iteration: 1883  val_loss: 0.6662305593490601  val_accuracy: 56.875 %\n",
      "iteration: 1884  val_loss: 0.7838236689567566  val_accuracy: 56.875 %\n",
      "iteration: 1885  val_loss: 0.7048911452293396  val_accuracy: 56.875 %\n",
      "iteration: 1886  val_loss: 0.7047749757766724  val_accuracy: 56.875 %\n",
      "iteration: 1887  val_loss: 0.628643274307251  val_accuracy: 56.875 %\n",
      "iteration: 1888  val_loss: 0.8202633857727051  val_accuracy: 56.875 %\n",
      "iteration: 1889  val_loss: 0.741559624671936  val_accuracy: 56.875 %\n",
      "iteration: 1890  val_loss: 0.7407684326171875  val_accuracy: 56.875 %\n",
      "iteration: 1891  val_loss: 0.63102126121521  val_accuracy: 56.875 %\n",
      "iteration: 1892  val_loss: 0.7774535417556763  val_accuracy: 56.875 %\n",
      "iteration: 1893  val_loss: 0.7753735780715942  val_accuracy: 56.875 %\n",
      "iteration: 1894  val_loss: 0.7733173966407776  val_accuracy: 56.875 %\n",
      "iteration: 1895  val_loss: 0.7025628685951233  val_accuracy: 56.875 %\n",
      "iteration: 1896  val_loss: 0.7708525657653809  val_accuracy: 56.875 %\n",
      "iteration: 1897  val_loss: 0.7020457983016968  val_accuracy: 56.875 %\n",
      "iteration: 1898  val_loss: 0.6687222123146057  val_accuracy: 56.875 %\n",
      "iteration: 1899  val_loss: 0.6352710723876953  val_accuracy: 56.875 %\n",
      "iteration: 1900  val_loss: 0.6007647514343262  val_accuracy: 56.875 %\n",
      "iteration: 1901  val_loss: 0.7372985482215881  val_accuracy: 56.875 %\n",
      "iteration: 1902  val_loss: 0.5659484267234802  val_accuracy: 56.875 %\n",
      "iteration: 1903  val_loss: 0.7030244469642639  val_accuracy: 56.875 %\n",
      "iteration: 1904  val_loss: 0.6328843235969543  val_accuracy: 56.875 %\n",
      "iteration: 1905  val_loss: 0.738649845123291  val_accuracy: 56.875 %\n",
      "iteration: 1906  val_loss: 0.7378901243209839  val_accuracy: 56.875 %\n",
      "iteration: 1907  val_loss: 0.7371385097503662  val_accuracy: 56.875 %\n",
      "iteration: 1908  val_loss: 0.6003217101097107  val_accuracy: 56.875 %\n",
      "iteration: 1909  val_loss: 0.7723594307899475  val_accuracy: 56.875 %\n",
      "iteration: 1910  val_loss: 0.7023606300354004  val_accuracy: 56.875 %\n",
      "iteration: 1911  val_loss: 0.7022701501846313  val_accuracy: 56.875 %\n",
      "iteration: 1912  val_loss: 0.7021806240081787  val_accuracy: 56.875 %\n",
      "iteration: 1913  val_loss: 0.6351166367530823  val_accuracy: 56.875 %\n",
      "iteration: 1914  val_loss: 0.6344500184059143  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1915  val_loss: 0.6337893605232239  val_accuracy: 56.875 %\n",
      "iteration: 1916  val_loss: 0.7028311491012573  val_accuracy: 56.875 %\n",
      "iteration: 1917  val_loss: 0.6333839893341064  val_accuracy: 56.875 %\n",
      "iteration: 1918  val_loss: 0.7381125688552856  val_accuracy: 56.875 %\n",
      "iteration: 1919  val_loss: 0.7373585104942322  val_accuracy: 56.875 %\n",
      "iteration: 1920  val_loss: 0.5999746322631836  val_accuracy: 56.875 %\n",
      "iteration: 1921  val_loss: 0.7028725147247314  val_accuracy: 56.875 %\n",
      "iteration: 1922  val_loss: 0.7375272512435913  val_accuracy: 56.875 %\n",
      "iteration: 1923  val_loss: 0.6682444214820862  val_accuracy: 56.875 %\n",
      "iteration: 1924  val_loss: 0.7484679222106934  val_accuracy: 56.875 %\n",
      "iteration: 1925  val_loss: 0.7022722959518433  val_accuracy: 56.875 %\n",
      "iteration: 1926  val_loss: 0.6348671317100525  val_accuracy: 56.875 %\n",
      "iteration: 1927  val_loss: 0.7024276256561279  val_accuracy: 56.875 %\n",
      "iteration: 1928  val_loss: 0.6683925986289978  val_accuracy: 56.875 %\n",
      "iteration: 1929  val_loss: 0.7024139165878296  val_accuracy: 56.875 %\n",
      "iteration: 1930  val_loss: 0.6684043407440186  val_accuracy: 56.875 %\n",
      "iteration: 1931  val_loss: 0.7024000883102417  val_accuracy: 56.875 %\n",
      "iteration: 1932  val_loss: 0.634522557258606  val_accuracy: 56.875 %\n",
      "iteration: 1933  val_loss: 0.6338613629341125  val_accuracy: 56.875 %\n",
      "iteration: 1934  val_loss: 0.6680048704147339  val_accuracy: 56.875 %\n",
      "iteration: 1935  val_loss: 0.6679428815841675  val_accuracy: 56.875 %\n",
      "iteration: 1936  val_loss: 0.7029578685760498  val_accuracy: 56.875 %\n",
      "iteration: 1937  val_loss: 0.7028613686561584  val_accuracy: 56.875 %\n",
      "iteration: 1938  val_loss: 0.668035626411438  val_accuracy: 56.875 %\n",
      "iteration: 1939  val_loss: 0.6679733991622925  val_accuracy: 56.875 %\n",
      "iteration: 1940  val_loss: 0.737928032875061  val_accuracy: 56.875 %\n",
      "iteration: 1941  val_loss: 0.599080502986908  val_accuracy: 56.875 %\n",
      "iteration: 1942  val_loss: 0.7383614778518677  val_accuracy: 56.875 %\n",
      "iteration: 1943  val_loss: 0.6680042147636414  val_accuracy: 56.875 %\n",
      "iteration: 1944  val_loss: 0.6679422855377197  val_accuracy: 56.875 %\n",
      "iteration: 1945  val_loss: 0.6328032612800598  val_accuracy: 56.875 %\n",
      "iteration: 1946  val_loss: 0.7032105922698975  val_accuracy: 56.875 %\n",
      "iteration: 1947  val_loss: 0.7031115293502808  val_accuracy: 56.875 %\n",
      "iteration: 1948  val_loss: 0.6678376197814941  val_accuracy: 56.875 %\n",
      "iteration: 1949  val_loss: 0.6324641704559326  val_accuracy: 56.875 %\n",
      "iteration: 1950  val_loss: 0.6675821542739868  val_accuracy: 56.875 %\n",
      "iteration: 1951  val_loss: 0.7393158078193665  val_accuracy: 56.875 %\n",
      "iteration: 1952  val_loss: 0.738548994064331  val_accuracy: 56.875 %\n",
      "iteration: 1953  val_loss: 0.6330313086509705  val_accuracy: 56.875 %\n",
      "iteration: 1954  val_loss: 0.7031219005584717  val_accuracy: 56.875 %\n",
      "iteration: 1955  val_loss: 0.6326351165771484  val_accuracy: 56.875 %\n",
      "iteration: 1956  val_loss: 0.7032762169837952  val_accuracy: 56.875 %\n",
      "iteration: 1957  val_loss: 0.7386429309844971  val_accuracy: 56.875 %\n",
      "iteration: 1958  val_loss: 0.7029039263725281  val_accuracy: 56.875 %\n",
      "iteration: 1959  val_loss: 0.6680015325546265  val_accuracy: 56.875 %\n",
      "iteration: 1960  val_loss: 0.6679397225379944  val_accuracy: 56.875 %\n",
      "iteration: 1961  val_loss: 0.6327946186065674  val_accuracy: 56.875 %\n",
      "iteration: 1962  val_loss: 0.7032139301300049  val_accuracy: 56.875 %\n",
      "iteration: 1963  val_loss: 0.7384716868400574  val_accuracy: 56.875 %\n",
      "iteration: 1964  val_loss: 0.7028433680534363  val_accuracy: 56.875 %\n",
      "iteration: 1965  val_loss: 0.6680499315261841  val_accuracy: 56.875 %\n",
      "iteration: 1966  val_loss: 0.5983126759529114  val_accuracy: 56.875 %\n",
      "iteration: 1967  val_loss: 0.774448812007904  val_accuracy: 56.875 %\n",
      "iteration: 1968  val_loss: 0.7028042078018188  val_accuracy: 56.875 %\n",
      "iteration: 1969  val_loss: 0.7027091979980469  val_accuracy: 56.875 %\n",
      "iteration: 1970  val_loss: 0.5647895336151123  val_accuracy: 56.875 %\n",
      "iteration: 1971  val_loss: 0.5966222286224365  val_accuracy: 56.875 %\n",
      "iteration: 1972  val_loss: 0.7036454677581787  val_accuracy: 56.875 %\n",
      "iteration: 1973  val_loss: 0.667432963848114  val_accuracy: 56.875 %\n",
      "iteration: 1974  val_loss: 0.7036185264587402  val_accuracy: 56.875 %\n",
      "iteration: 1975  val_loss: 0.6674528121948242  val_accuracy: 56.875 %\n",
      "iteration: 1976  val_loss: 0.6312004923820496  val_accuracy: 56.875 %\n",
      "iteration: 1977  val_loss: 0.7038490176200867  val_accuracy: 56.875 %\n",
      "iteration: 1978  val_loss: 0.594368040561676  val_accuracy: 56.875 %\n",
      "iteration: 1979  val_loss: 0.7414003014564514  val_accuracy: 56.875 %\n",
      "iteration: 1980  val_loss: 0.6304595470428467  val_accuracy: 56.875 %\n",
      "iteration: 1981  val_loss: 0.741313099861145  val_accuracy: 56.875 %\n",
      "iteration: 1982  val_loss: 0.7038621306419373  val_accuracy: 56.875 %\n",
      "iteration: 1983  val_loss: 0.7037563920021057  val_accuracy: 56.875 %\n",
      "iteration: 1984  val_loss: 0.5947520732879639  val_accuracy: 56.875 %\n",
      "iteration: 1985  val_loss: 0.7040925621986389  val_accuracy: 56.875 %\n",
      "iteration: 1986  val_loss: 0.6302398443222046  val_accuracy: 56.875 %\n",
      "iteration: 1987  val_loss: 0.7415592670440674  val_accuracy: 56.875 %\n",
      "iteration: 1988  val_loss: 0.740768313407898  val_accuracy: 56.875 %\n",
      "iteration: 1989  val_loss: 0.7036643028259277  val_accuracy: 56.875 %\n",
      "iteration: 1990  val_loss: 0.7035605907440186  val_accuracy: 56.875 %\n",
      "iteration: 1991  val_loss: 0.5955709218978882  val_accuracy: 56.875 %\n",
      "iteration: 1992  val_loss: 0.7038953900337219  val_accuracy: 56.875 %\n",
      "iteration: 1993  val_loss: 0.7037894129753113  val_accuracy: 56.875 %\n",
      "iteration: 1994  val_loss: 0.7763975858688354  val_accuracy: 56.875 %\n",
      "iteration: 1995  val_loss: 0.6676726341247559  val_accuracy: 56.875 %\n",
      "iteration: 1996  val_loss: 0.7746778726577759  val_accuracy: 56.875 %\n",
      "iteration: 1997  val_loss: 0.702853262424469  val_accuracy: 56.875 %\n",
      "iteration: 1998  val_loss: 0.6333261132240295  val_accuracy: 56.875 %\n",
      "iteration: 1999  val_loss: 0.7030079960823059  val_accuracy: 56.875 %\n",
      "iteration: 2000  val_loss: 0.7029110193252563  val_accuracy: 56.875 %\n",
      "iteration: 2001  val_loss: 0.667995810508728  val_accuracy: 56.875 %\n",
      "iteration: 2002  val_loss: 0.702892005443573  val_accuracy: 56.875 %\n",
      "iteration: 2003  val_loss: 0.668010950088501  val_accuracy: 56.875 %\n",
      "iteration: 2004  val_loss: 0.7028732299804688  val_accuracy: 56.875 %\n",
      "iteration: 2005  val_loss: 0.702777624130249  val_accuracy: 56.875 %\n",
      "iteration: 2006  val_loss: 0.5989434719085693  val_accuracy: 56.875 %\n",
      "iteration: 2007  val_loss: 0.6677646040916443  val_accuracy: 56.875 %\n",
      "iteration: 2008  val_loss: 0.7031832933425903  val_accuracy: 56.875 %\n",
      "iteration: 2009  val_loss: 0.6324790716171265  val_accuracy: 56.875 %\n",
      "iteration: 2010  val_loss: 0.7033374309539795  val_accuracy: 56.875 %\n",
      "iteration: 2011  val_loss: 0.7032372355461121  val_accuracy: 56.875 %\n",
      "iteration: 2012  val_loss: 0.7031378149986267  val_accuracy: 56.875 %\n",
      "iteration: 2013  val_loss: 0.7030394077301025  val_accuracy: 56.875 %\n",
      "iteration: 2014  val_loss: 0.6678939461708069  val_accuracy: 56.875 %\n",
      "iteration: 2015  val_loss: 0.7733911871910095  val_accuracy: 56.875 %\n",
      "iteration: 2016  val_loss: 0.6681889295578003  val_accuracy: 56.875 %\n",
      "iteration: 2017  val_loss: 0.633594810962677  val_accuracy: 56.875 %\n",
      "iteration: 2018  val_loss: 0.737886905670166  val_accuracy: 56.875 %\n",
      "iteration: 2019  val_loss: 0.7371354699134827  val_accuracy: 56.875 %\n",
      "iteration: 2020  val_loss: 0.7023757696151733  val_accuracy: 56.875 %\n",
      "iteration: 2021  val_loss: 0.668436586856842  val_accuracy: 56.875 %\n",
      "iteration: 2022  val_loss: 0.7023623585700989  val_accuracy: 56.875 %\n",
      "iteration: 2023  val_loss: 0.7360958456993103  val_accuracy: 56.875 %\n",
      "iteration: 2024  val_loss: 0.6353234648704529  val_accuracy: 56.875 %\n",
      "iteration: 2025  val_loss: 0.7360630035400391  val_accuracy: 56.875 %\n",
      "iteration: 2026  val_loss: 0.7020056247711182  val_accuracy: 56.875 %\n",
      "iteration: 2027  val_loss: 0.7019187808036804  val_accuracy: 56.875 %\n",
      "iteration: 2028  val_loss: 0.734830379486084  val_accuracy: 56.875 %\n",
      "iteration: 2029  val_loss: 0.636536180973053  val_accuracy: 56.875 %\n",
      "iteration: 2030  val_loss: 0.7348099946975708  val_accuracy: 56.875 %\n",
      "iteration: 2031  val_loss: 0.5715318322181702  val_accuracy: 56.875 %\n",
      "iteration: 2032  val_loss: 0.7021468281745911  val_accuracy: 56.875 %\n",
      "iteration: 2033  val_loss: 0.6017837524414062  val_accuracy: 56.875 %\n",
      "iteration: 2034  val_loss: 0.7708517909049988  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2035  val_loss: 0.6686447262763977  val_accuracy: 56.875 %\n",
      "iteration: 2036  val_loss: 0.5679377913475037  val_accuracy: 56.875 %\n",
      "iteration: 2037  val_loss: 0.6680839657783508  val_accuracy: 56.875 %\n",
      "iteration: 2038  val_loss: 0.6680213212966919  val_accuracy: 56.875 %\n",
      "iteration: 2039  val_loss: 0.702860414981842  val_accuracy: 56.875 %\n",
      "iteration: 2040  val_loss: 0.7374935150146484  val_accuracy: 56.875 %\n",
      "iteration: 2041  val_loss: 0.6682543158531189  val_accuracy: 56.875 %\n",
      "iteration: 2042  val_loss: 0.6338023543357849  val_accuracy: 56.875 %\n",
      "iteration: 2043  val_loss: 0.7376653552055359  val_accuracy: 56.875 %\n",
      "iteration: 2044  val_loss: 0.5994920134544373  val_accuracy: 56.875 %\n",
      "iteration: 2045  val_loss: 0.6327438950538635  val_accuracy: 56.875 %\n",
      "iteration: 2046  val_loss: 0.632098376750946  val_accuracy: 56.875 %\n",
      "iteration: 2047  val_loss: 0.667473316192627  val_accuracy: 56.875 %\n",
      "iteration: 2048  val_loss: 0.6312683820724487  val_accuracy: 56.875 %\n",
      "iteration: 2049  val_loss: 0.6672286987304688  val_accuracy: 56.875 %\n",
      "iteration: 2050  val_loss: 0.6304507255554199  val_accuracy: 56.875 %\n",
      "iteration: 2051  val_loss: 0.7784888744354248  val_accuracy: 56.875 %\n",
      "iteration: 2052  val_loss: 0.6309713125228882  val_accuracy: 56.875 %\n",
      "iteration: 2053  val_loss: 0.6671420335769653  val_accuracy: 56.875 %\n",
      "iteration: 2054  val_loss: 0.7409486174583435  val_accuracy: 56.875 %\n",
      "iteration: 2055  val_loss: 0.7037297487258911  val_accuracy: 56.875 %\n",
      "iteration: 2056  val_loss: 0.6311172246932983  val_accuracy: 56.875 %\n",
      "iteration: 2057  val_loss: 0.6671845316886902  val_accuracy: 56.875 %\n",
      "iteration: 2058  val_loss: 0.6671303510665894  val_accuracy: 56.875 %\n",
      "iteration: 2059  val_loss: 0.6670766472816467  val_accuracy: 56.875 %\n",
      "iteration: 2060  val_loss: 0.7041108012199402  val_accuracy: 56.875 %\n",
      "iteration: 2061  val_loss: 0.5563898682594299  val_accuracy: 56.875 %\n",
      "iteration: 2062  val_loss: 0.6666668057441711  val_accuracy: 56.875 %\n",
      "iteration: 2063  val_loss: 0.6666175127029419  val_accuracy: 56.875 %\n",
      "iteration: 2064  val_loss: 0.7430053353309631  val_accuracy: 56.875 %\n",
      "iteration: 2065  val_loss: 0.5913385152816772  val_accuracy: 56.875 %\n",
      "iteration: 2066  val_loss: 0.5895380973815918  val_accuracy: 56.875 %\n",
      "iteration: 2067  val_loss: 0.7054001688957214  val_accuracy: 56.875 %\n",
      "iteration: 2068  val_loss: 0.6662583351135254  val_accuracy: 56.875 %\n",
      "iteration: 2069  val_loss: 0.7444930672645569  val_accuracy: 56.875 %\n",
      "iteration: 2070  val_loss: 0.820932924747467  val_accuracy: 56.875 %\n",
      "iteration: 2071  val_loss: 0.6668556928634644  val_accuracy: 56.875 %\n",
      "iteration: 2072  val_loss: 0.7796813249588013  val_accuracy: 56.875 %\n",
      "iteration: 2073  val_loss: 0.6671367287635803  val_accuracy: 56.875 %\n",
      "iteration: 2074  val_loss: 0.630139946937561  val_accuracy: 56.875 %\n",
      "iteration: 2075  val_loss: 0.7790557146072388  val_accuracy: 56.875 %\n",
      "iteration: 2076  val_loss: 0.6306626200675964  val_accuracy: 56.875 %\n",
      "iteration: 2077  val_loss: 0.7040693759918213  val_accuracy: 56.875 %\n",
      "iteration: 2078  val_loss: 0.7039616107940674  val_accuracy: 56.875 %\n",
      "iteration: 2079  val_loss: 0.8138065338134766  val_accuracy: 56.875 %\n",
      "iteration: 2080  val_loss: 0.667681097984314  val_accuracy: 56.875 %\n",
      "iteration: 2081  val_loss: 0.6676222681999207  val_accuracy: 56.875 %\n",
      "iteration: 2082  val_loss: 0.7391707897186279  val_accuracy: 56.875 %\n",
      "iteration: 2083  val_loss: 0.6677765250205994  val_accuracy: 56.875 %\n",
      "iteration: 2084  val_loss: 0.6677167415618896  val_accuracy: 56.875 %\n",
      "iteration: 2085  val_loss: 0.7388317584991455  val_accuracy: 56.875 %\n",
      "iteration: 2086  val_loss: 0.6678712368011475  val_accuracy: 56.875 %\n",
      "iteration: 2087  val_loss: 0.7382847666740417  val_accuracy: 56.875 %\n",
      "iteration: 2088  val_loss: 0.7722811102867126  val_accuracy: 56.875 %\n",
      "iteration: 2089  val_loss: 0.804219663143158  val_accuracy: 56.875 %\n",
      "iteration: 2090  val_loss: 0.6689006090164185  val_accuracy: 56.875 %\n",
      "iteration: 2091  val_loss: 0.7678540945053101  val_accuracy: 56.875 %\n",
      "iteration: 2092  val_loss: 0.669206440448761  val_accuracy: 56.875 %\n",
      "iteration: 2093  val_loss: 0.7986398935317993  val_accuracy: 56.875 %\n",
      "iteration: 2094  val_loss: 0.669669508934021  val_accuracy: 56.875 %\n",
      "iteration: 2095  val_loss: 0.7639255523681641  val_accuracy: 56.875 %\n",
      "iteration: 2096  val_loss: 0.7006548643112183  val_accuracy: 56.875 %\n",
      "iteration: 2097  val_loss: 0.7616252899169922  val_accuracy: 56.875 %\n",
      "iteration: 2098  val_loss: 0.7002136707305908  val_accuracy: 56.875 %\n",
      "iteration: 2099  val_loss: 0.7001446485519409  val_accuracy: 56.875 %\n",
      "iteration: 2100  val_loss: 0.7295407652854919  val_accuracy: 56.875 %\n",
      "iteration: 2101  val_loss: 0.6418412327766418  val_accuracy: 56.875 %\n",
      "iteration: 2102  val_loss: 0.6706008315086365  val_accuracy: 56.875 %\n",
      "iteration: 2103  val_loss: 0.7001631259918213  val_accuracy: 56.875 %\n",
      "iteration: 2104  val_loss: 0.7000946402549744  val_accuracy: 56.875 %\n",
      "iteration: 2105  val_loss: 0.6706681251525879  val_accuracy: 56.875 %\n",
      "iteration: 2106  val_loss: 0.6705811023712158  val_accuracy: 56.875 %\n",
      "iteration: 2107  val_loss: 0.7001805305480957  val_accuracy: 56.875 %\n",
      "iteration: 2108  val_loss: 0.6114917397499084  val_accuracy: 56.875 %\n",
      "iteration: 2109  val_loss: 0.6398451328277588  val_accuracy: 56.875 %\n",
      "iteration: 2110  val_loss: 0.6391370892524719  val_accuracy: 56.875 %\n",
      "iteration: 2111  val_loss: 0.7321934700012207  val_accuracy: 56.875 %\n",
      "iteration: 2112  val_loss: 0.7315030097961426  val_accuracy: 56.875 %\n",
      "iteration: 2113  val_loss: 0.6398271918296814  val_accuracy: 56.875 %\n",
      "iteration: 2114  val_loss: 0.7315149903297424  val_accuracy: 56.875 %\n",
      "iteration: 2115  val_loss: 0.6398149728775024  val_accuracy: 56.875 %\n",
      "iteration: 2116  val_loss: 0.6391072273254395  val_accuracy: 56.875 %\n",
      "iteration: 2117  val_loss: 0.6696780920028687  val_accuracy: 56.875 %\n",
      "iteration: 2118  val_loss: 0.6696004867553711  val_accuracy: 56.875 %\n",
      "iteration: 2119  val_loss: 0.6695235371589661  val_accuracy: 56.875 %\n",
      "iteration: 2120  val_loss: 0.6059765219688416  val_accuracy: 56.875 %\n",
      "iteration: 2121  val_loss: 0.7015768885612488  val_accuracy: 56.875 %\n",
      "iteration: 2122  val_loss: 0.733841061592102  val_accuracy: 56.875 %\n",
      "iteration: 2123  val_loss: 0.7968894243240356  val_accuracy: 56.875 %\n",
      "iteration: 2124  val_loss: 0.7007144689559937  val_accuracy: 56.875 %\n",
      "iteration: 2125  val_loss: 0.7925707101821899  val_accuracy: 56.875 %\n",
      "iteration: 2126  val_loss: 0.670556902885437  val_accuracy: 56.875 %\n",
      "iteration: 2127  val_loss: 0.6704709529876709  val_accuracy: 56.875 %\n",
      "iteration: 2128  val_loss: 0.6703858375549316  val_accuracy: 56.875 %\n",
      "iteration: 2129  val_loss: 0.700356125831604  val_accuracy: 56.875 %\n",
      "iteration: 2130  val_loss: 0.6703783869743347  val_accuracy: 56.875 %\n",
      "iteration: 2131  val_loss: 0.6702941656112671  val_accuracy: 56.875 %\n",
      "iteration: 2132  val_loss: 0.6702106595039368  val_accuracy: 56.875 %\n",
      "iteration: 2133  val_loss: 0.700516939163208  val_accuracy: 56.875 %\n",
      "iteration: 2134  val_loss: 0.6702051758766174  val_accuracy: 56.875 %\n",
      "iteration: 2135  val_loss: 0.670122504234314  val_accuracy: 56.875 %\n",
      "iteration: 2136  val_loss: 0.7005993127822876  val_accuracy: 56.875 %\n",
      "iteration: 2137  val_loss: 0.6701178550720215  val_accuracy: 56.875 %\n",
      "iteration: 2138  val_loss: 0.6089006066322327  val_accuracy: 56.875 %\n",
      "iteration: 2139  val_loss: 0.6696428060531616  val_accuracy: 56.875 %\n",
      "iteration: 2140  val_loss: 0.6695653796195984  val_accuracy: 56.875 %\n",
      "iteration: 2141  val_loss: 0.6694889068603516  val_accuracy: 56.875 %\n",
      "iteration: 2142  val_loss: 0.637607991695404  val_accuracy: 56.875 %\n",
      "iteration: 2143  val_loss: 0.6369197964668274  val_accuracy: 56.875 %\n",
      "iteration: 2144  val_loss: 0.7016911506652832  val_accuracy: 56.875 %\n",
      "iteration: 2145  val_loss: 0.6039106845855713  val_accuracy: 56.875 %\n",
      "iteration: 2146  val_loss: 0.7020102143287659  val_accuracy: 56.875 %\n",
      "iteration: 2147  val_loss: 0.7019231915473938  val_accuracy: 56.875 %\n",
      "iteration: 2148  val_loss: 0.6358247995376587  val_accuracy: 56.875 %\n",
      "iteration: 2149  val_loss: 0.7020787596702576  val_accuracy: 56.875 %\n",
      "iteration: 2150  val_loss: 0.7019913196563721  val_accuracy: 56.875 %\n",
      "iteration: 2151  val_loss: 0.7681726217269897  val_accuracy: 56.875 %\n",
      "iteration: 2152  val_loss: 0.6367941498756409  val_accuracy: 56.875 %\n",
      "iteration: 2153  val_loss: 0.6689240336418152  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2154  val_loss: 0.6358946561813354  val_accuracy: 56.875 %\n",
      "iteration: 2155  val_loss: 0.6686376929283142  val_accuracy: 56.875 %\n",
      "iteration: 2156  val_loss: 0.5678856372833252  val_accuracy: 56.875 %\n",
      "iteration: 2157  val_loss: 0.7373512387275696  val_accuracy: 56.875 %\n",
      "iteration: 2158  val_loss: 0.7707610130310059  val_accuracy: 56.875 %\n",
      "iteration: 2159  val_loss: 0.6686612963676453  val_accuracy: 56.875 %\n",
      "iteration: 2160  val_loss: 0.6015705466270447  val_accuracy: 56.875 %\n",
      "iteration: 2161  val_loss: 0.7367933988571167  val_accuracy: 56.875 %\n",
      "iteration: 2162  val_loss: 0.7022573947906494  val_accuracy: 56.875 %\n",
      "iteration: 2163  val_loss: 0.6349076628684998  val_accuracy: 56.875 %\n",
      "iteration: 2164  val_loss: 0.7024126648902893  val_accuracy: 56.875 %\n",
      "iteration: 2165  val_loss: 0.6344888806343079  val_accuracy: 56.875 %\n",
      "iteration: 2166  val_loss: 0.7369375824928284  val_accuracy: 56.875 %\n",
      "iteration: 2167  val_loss: 0.6345281600952148  val_accuracy: 56.875 %\n",
      "iteration: 2168  val_loss: 0.6682102680206299  val_accuracy: 56.875 %\n",
      "iteration: 2169  val_loss: 0.6681464314460754  val_accuracy: 56.875 %\n",
      "iteration: 2170  val_loss: 0.6334587335586548  val_accuracy: 56.875 %\n",
      "iteration: 2171  val_loss: 0.7731069922447205  val_accuracy: 56.875 %\n",
      "iteration: 2172  val_loss: 0.7025187015533447  val_accuracy: 56.875 %\n",
      "iteration: 2173  val_loss: 0.6683161854743958  val_accuracy: 56.875 %\n",
      "iteration: 2174  val_loss: 0.6682512760162354  val_accuracy: 56.875 %\n",
      "iteration: 2175  val_loss: 0.668187141418457  val_accuracy: 56.875 %\n",
      "iteration: 2176  val_loss: 0.7026581168174744  val_accuracy: 56.875 %\n",
      "iteration: 2177  val_loss: 0.7369288206100464  val_accuracy: 56.875 %\n",
      "iteration: 2178  val_loss: 0.6684203147888184  val_accuracy: 56.875 %\n",
      "iteration: 2179  val_loss: 0.7364084720611572  val_accuracy: 56.875 %\n",
      "iteration: 2180  val_loss: 0.6350268125534058  val_accuracy: 56.875 %\n",
      "iteration: 2181  val_loss: 0.7363724112510681  val_accuracy: 56.875 %\n",
      "iteration: 2182  val_loss: 0.6350609064102173  val_accuracy: 56.875 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Iterate through test dataset\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signals, labels \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     37\u001b[0m     signals \u001b[38;5;241m=\u001b[39m signals\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/envs/xlm_classifier/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.13/envs/xlm_classifier/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_dim = 200\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "\n",
    "        train = signals.unsqueeze(1)\n",
    "        labels = labels.long()\n",
    "            \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 16 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for signals, labels in val_loader:\n",
    "                signals = signals.unsqueeze(1)\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(signals)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "            y_true+=labels\n",
    "            y_pred+=predicted\n",
    "            if count % 10 == 0:\n",
    "                # Print validade loss\n",
    "                print('iteration: {}  val_loss: {}  val_accuracy: {} %'.format(count, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "for signals, labels in test_loader:\n",
    "    signals = signals.unsqueeze(1)\n",
    "\n",
    "    # Forward propagation\n",
    "    outputs = model(signals)\n",
    "\n",
    "    # Get predictions from the maximum value\n",
    "    predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "    # Total number of labels\n",
    "    total += labels.size(0)\n",
    "\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "accuracy = 100 * correct / float(total)\n",
    "\n",
    "# store loss and iteration\n",
    "# loss_list.append(loss.data)\n",
    "# accuracy_list.append(accuracy)\n",
    "y_true+=labels\n",
    "y_pred+=predicted\n",
    "# Print validade loss\n",
    "print('test_accuracy: {} %'.format(accuracy.numpy().round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"RNN: Loss vs Number of iteration\")\n",
    "plt.show()\n",
    "\n",
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"RNN: Accuracy vs Number of iteration\")\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a6486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
